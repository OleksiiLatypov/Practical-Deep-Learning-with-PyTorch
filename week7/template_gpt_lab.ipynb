{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/week7/template_gpt_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction in GPT features**\n",
        "\n",
        "**GPU** is recomended for this assignment. `Runtime` -> `Change runtime type` -> `GPU`\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- Main features GPT.\n",
        "- How to build generative QA based on the given data and pre-trained GPT\n",
        "- Prepare own dataset and fine-tune GPT model on them."
      ],
      "metadata": {
        "id": "DggXnOBpK-Aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add OpenAI key to Config\n",
        "\n",
        "Before you, sign up or log in to your OpenAI account and generate an [API key](https://platform.openai.com/account/api-keys).\n",
        "\n",
        ">**Note that while the first queries are free, there is a specific limit after which payment is required. OpenAI grants an initial budget of $18, more than enough to complete the lab, experiment with the pipeline amd fine-tune model.**\n",
        "\n",
        ">**Please keep your API key for use during the lab review. We don't store your key, so after verifying your work and earning points, you can delete it in the API keys.**"
      ],
      "metadata": {
        "id": "6LCjSk22LEj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa1TZ45xKsDt"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] Config\n",
        "\n",
        "class Config:\n",
        "\n",
        "  # # Section 1\n",
        "  gpt3_model_cost = 0.5/1000000\n",
        "  gpt4_model_cost = 5/1000000\n",
        "\n",
        "  # Section 2\n",
        "  wiki_article_path = 'wiki_pages'\n",
        "\n",
        "  # names of Wikipedia's articles\n",
        "  article_titles = ['Kyiv', 'History of Kyiv', 'Kyiv Metro', 'Kyiv culture', 'Kyiv Music Fest', 'FC Dynamo Kyiv', 'Igor Sikorsky Kyiv Polytechnic Institute', 'Paton Bridge', 'Saint Sophia Cathedral', 'Transport in Kyiv', 'Kyiv Zoo', 'Kyiv metropolitan area', 'Taras Shevchenko National University of Kyiv', 'Euromaidan', 'Motherland Monument', 'Podil', 'Kyiv TV Tower']\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # your OpenAI API token key\n",
        "  openai_api_key = <YOUR_OPENAI_KEY>\n",
        "  ### END CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Try GPT"
      ],
      "metadata": {
        "id": "XhsBFifJLUJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Generative Pre-trained Transformer (GPT)](https://arxiv.org/abs/2005.14165) models by OpenAI have taken the natural language processing (NLP) community by introducing compelling language models. These models can perform various NLP tasks like question answering, textual entailment, text summarisation, Etc. Without any supervised training. These language models need very few to no examples to understand the tasks and perform equivalent to or even better than the state-of-the-art models trained in a supervised fashion.\n",
        "\n",
        "With the recent releases of [GPT-4o](https://platform.openai.com/docs/models/gpt-4o) and other models, more powerful new versions of OpenAI’s GPT model may take much time before we can exploit their full potential.\n",
        "\n",
        "We propose considering the possibilities of the OpenAI models they offer and how they can be applied."
      ],
      "metadata": {
        "id": "IDlWut9oLPTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll install the necessary libraries"
      ],
      "metadata": {
        "id": "lP76_jG7x4CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install wget"
      ],
      "metadata": {
        "id": "VNBcbnR3Ll0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247b9c78-7bfe-43d2-c533-14961100b739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.0\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=2477e54d966139e0fca6cc14045eba84cc2b6f85df0d7606b533c10c4f6df480\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "SDABbkiYLO5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **OpenAI API start**\n",
        "\n",
        "Now, using OpenAI API, we can give a prompt to a ChatGPT. Create a [chat completions](https://platform.openai.com/docs/api-reference/chat/create) example from the [official guide](https://platform.openai.com/docs/guides/chat/introduction)."
      ],
      "metadata": {
        "id": "YJKQhb2WL7Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] Client_chat_completion\n",
        "\n",
        "### START CODE HERE ###\n",
        "client = OpenAI(api_key=Config.openai_api_key)\n",
        "\n",
        "def client_chat_completion():\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"This is a test.\"},\n",
        "    ]\n",
        "  )\n",
        "  return response\n",
        "\n",
        "  return response\n",
        "### END CODE HERE\n",
        "\n",
        "test_response = client_chat_completion()\n",
        "test_response"
      ],
      "metadata": {
        "id": "fpgSHT9OL8_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa359bd-b088-45ee-f9e9-f98dd12d1d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9ovKAH0rEHFlNczdQ5ycgpwDEYTR0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm here to assist you. Let me know if you have any questions or need help with anything.\", role='assistant', function_call=None, tool_calls=None))], created=1721923782, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=22, total_tokens=43))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "ChatCompletion(id='chatcmpl-9YqBk4jRNJAKlEBtqgnnjNfrrCEScca',\n",
        "choices=[Choice(finish_reason='stop', index=0, logprobs=None,\n",
        "message=ChatCompletionMessage(content='This is a test.',\n",
        "role='assistant',\n",
        "function_call=None,\n",
        "tool_calls=None))],\n",
        "created=1718090792,\n",
        "model='gpt-3.5-turbo-0125',\n",
        "object='chat.completion',\n",
        "system_fingerprint=None,\n",
        "usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17))\n",
        "```\n",
        "> Note: The model`s answer can be different but have the same meaning\n"
      ],
      "metadata": {
        "id": "PNHDudVwMR7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the result with all the query parameters and data. The assistant’s reply in `choices[0].message.content`.\n",
        "\n",
        "### Response Parameters:\n",
        "\n",
        "`finish_reason` - indicates the end status of the model's output. Possible values are:\n",
        "\n",
        "* `stop`: API returned complete model output.\n",
        "* `length`: Incomplete model output due to max_tokens parameter or token limit.\n",
        "* `content_filter`: Content omitted due to a flag from our content filters.\n",
        "* `null`: API response still in progress or incomplete.\n",
        "usage shows how many tokens the API used. This parameter is important to calculate the cost of your request. We will discuss pricing later.\n",
        "\n",
        "Explanation of the response structure:\n",
        "\n",
        "### ChatCompletion:\n",
        "\n",
        "* `id`: Unique identifier for the chat session.\n",
        "* `choices`: List of possible responses from the model. In this case, it contains one Choice object.\n",
        "* `created`: Timestamp of when the session was created.\n",
        "* `model`: Name of the model used (e.g., gpt-3.5-turbo-0125).\n",
        "* `object`: Type of object (in this case, chat.completion).\n",
        "* `system_fingerprint`: System fingerprint if applicable (in this case, None).\n",
        "* `usage`: Information on token usage, containing:\n",
        "* `completion_tokens`: Number of tokens in the model's response.\n",
        "* `prompt_tokens`: Number of tokens in the initial request.\n",
        "* `total_tokens`: Total number of tokens used.\n",
        "\n",
        "### Choice:\n",
        "\n",
        "* `finish_reason`: Reason for the model's completion.\n",
        "* `index`: Index of the choice (in this case, 0).\n",
        "* `message`: The message object containing:\n",
        "* `content`: The content of the assistant's reply.\n",
        "* `role`: The role of the message (assistant in this case).\n",
        "* `function_call`: Information on any function call made by the assistant (in this case, None).\n",
        "* `tool_calls`: Information on any tools called by the assistant (in this case, None)."
      ],
      "metadata": {
        "id": "knnCwhyfNZNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Models usage examples**\n",
        "\n",
        "We will use the `gpt-3.5-turbo`, one of the current ChatGPT model. This model is accessible on the [web interface](https://chat.openai.com). Your start budget will be enough to end this lab and more."
      ],
      "metadata": {
        "id": "yfwIXjT5PwHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using [Completions](https://platform.openai.com/docs/api-reference/completions), you can try the most available powerful model - `gpt-3.5-turbo`. Let`s ask something about Kyiv.\n",
        "\n",
        "Create a Completion request function with the parameters:\n",
        "- `max_tokens = 100`\n",
        "- `temperature = 0`\n",
        "- `model = \"gpt-3.5-turbo\"`\n",
        "- `prompt is a function argument`"
      ],
      "metadata": {
        "id": "UmUYgHJoP5tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] Gpt_completion\n",
        "\n",
        "### START CODE HERE ###\n",
        "def gpt_completion(prompt, temperature=0, max_tokens=100, model=\"gpt-3.5-turbo\"):\n",
        "    response = ...\n",
        "    return response\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "TcoilUh1Momi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add prompt `\"What is the height of the Kyiv TV tower in metres?\"`"
      ],
      "metadata": {
        "id": "341paOQzQVpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Kyiv_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "kyiv_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "kyiv_prompt_response = gpt_completion(kyiv_prompt)\n",
        "\n",
        "kyiv_prompt_answer = kyiv_prompt_response.choices[0].message.content\n",
        "kyiv_prompt_answer"
      ],
      "metadata": {
        "id": "ze6yHohIQZ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "The height of the Kyiv TV tower is 385 meters.\n",
        "```\n",
        "\n",
        "> Note: The model`s answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "mgQFwzzyQpGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer has likely been obtained. Finally, you can check on the page in [Wikipedia](https://en.wikipedia.org/wiki/Kyiv_TV_Tower).\n",
        "\n",
        "You have tried to ask a simple question that does not require analytical work. It is like a simple Google query. Let us try some math and logical questions.\n",
        "\n",
        "* Create prompt to calculate `8 * 6 + 6`."
      ],
      "metadata": {
        "id": "ivteTrmTQ7BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Math_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "math_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "math_prompt_response = gpt_completion(math_prompt)\n",
        "math_prompt_answer = math_prompt_response.choices[0].message.content\n",
        "\n",
        "math_prompt_answer"
      ],
      "metadata": {
        "id": "QqBanf_ZQvWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "54\n",
        "```\n",
        "\n",
        "> Note: The model`s answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "HMV6KksvRIqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT was able to consider and solve a simple arithmetic example. Let us use the GPT to solve the logic puzzle.\n",
        "\n",
        "* Create prompt to solve this exercise: `Maks have five apples. Mask give two apples to Maria, after what mother give one apple more to Maks. How many apples have Maks now??`"
      ],
      "metadata": {
        "id": "j2IwE5RmRPXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Logic_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "logic_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "logic_prompt_response = gpt_completion(logic_prompt)\n",
        "logic_prompt_answer = logic_prompt_response.choices[0].message.content\n",
        "\n",
        "logic_prompt_answer"
      ],
      "metadata": {
        "id": "_68_ICmuRRJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "Maks has 4 apples now.\n",
        "````\n",
        "\n",
        "> Note: The model`s answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "kEv3vcZJRYky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Models mistakes**\n",
        "\n",
        "GPT 3.5-turbo performed our tasks well. You could notice that the results were not immediately correct, and some prompt changes are necessary before getting the correct answer.\n",
        "\n",
        "GPT is imperfect, so let us look at this issue in more detail.\n",
        "\n",
        "For example, recent events. gpt-3.5-turbo is `up to June 2021` trained model, so it cannot know who is a Monarch of the United Kingdom now.\n",
        "\n",
        "* Create prompt `Who is a monarch of the United Kingdom?`"
      ],
      "metadata": {
        "id": "-T9djBT7RfC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Monarch_UK_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "monarch_UK_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "monarch_UK_prompt_response = gpt_completion(monarch_UK_prompt)\n",
        "monarch_UK_prompt_answer = monarch_UK_prompt_response.choices[0].message.content\n",
        "\n",
        "monarch_UK_prompt_answer"
      ],
      "metadata": {
        "id": "rJAok17_Rl5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "As of September 2021, the monarch of the United Kingdom is Queen Elizabeth II.\n",
        "```\n",
        "\n",
        "> Note: The model`s answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "_JTETo3gRwcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the wrong answer, and you can check it on [Wikipedia page](https://en.wikipedia.org/wiki/Monarchy_of_the_United_Kingdom) (Charles III has been the monarch since 8 September 2022).\n",
        "\n",
        "Let's test the mathematical abilities of the model and try a more complex exercise.\n",
        "\n",
        "* Use your previous math prompt to solve `sqrt(1213*4345)`"
      ],
      "metadata": {
        "id": "llXxsfxhR5l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Harder_math_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "harder_math_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "harder_math_prompt_response = gpt_completion(harder_math_prompt)\n",
        "harder_math_prompt_answer = harder_math_prompt_response.choices[0].message.content\n",
        "\n",
        "harder_math_prompt_answer"
      ],
      "metadata": {
        "id": "tRNZNMWvR6pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "Anything except 2295.75\n",
        "```"
      ],
      "metadata": {
        "id": "vMLzw6HYSCOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT models are a powerful tool. As we have seen, he can also make mistakes and answer incorrectly to complex and simple questions (try to solve the simple exercise `8*6 + 6*8`).\n"
      ],
      "metadata": {
        "id": "CXoZ8CQ8SKHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate haiku**\n",
        "\n",
        "\n",
        "However, models can understand what you ask, act according to the limits of the user's words, and even come up with something of your own.\n",
        "\n",
        "Let us generate a [Haiku](https://www.britannica.com/art/haiku) about ChatGPT using the `Gpt-3.5-turbo` model as the fastest text model.\n",
        "\n",
        "Create a Completion request with next parameters:\n",
        "- `max_tokens = 120`\n",
        "- `temperature = 0.3`\n",
        "- `model = \"gpt-3.5-turbo\"`\n",
        "- `\"Tell me a Haiku about ChatGPT\"`"
      ],
      "metadata": {
        "id": "oY5YbmoDSLQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Gpt3_haiku\n",
        "\n",
        "### START CODE HERE ###\n",
        "gpt3_haiku_prompt = ...\n",
        "haiku_gpt3_response = gpt_completion(...)\n",
        "### END CODE HERE ###\n",
        "\n",
        "haiku_gpt3_answer = haiku_gpt3_response.choices[0].message.content\n",
        "print(haiku_gpt3_answer)"
      ],
      "metadata": {
        "id": "8lmvV0hnSVFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare with `gpt-4-turbo` model as the bigger model.\n",
        "\n",
        "Create a Completion request with next parameters:\n",
        "- `max_tokens = 120`\n",
        "- `temperature = 0.3`\n",
        "- `model = \"gpt-4-turbo\"`\n",
        "- `\"Tell me a Haiku about ChatGPT\"`"
      ],
      "metadata": {
        "id": "DWgpHMo9Sc-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Gpt4_haiku\n",
        "\n",
        "### START CODE HERE ###\n",
        "gpt4_haiku_prompt = ...\n",
        "haiku_gpt4_response = gpt_completion(...)\n",
        "### END CODE HERE ###\n",
        "\n",
        "haiku_gpt4_answer = haiku_gpt4_response.choices[0].message.content\n",
        "print(haiku_gpt4_answer)"
      ],
      "metadata": {
        "id": "OSyRweA-Smdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compare both answers"
      ],
      "metadata": {
        "id": "Z2Ugne2wStb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPT 3.5 turbo model haiku:')\n",
        "print(haiku_gpt3_response.choices[0].message.content)\n",
        "print('------------------------------------')\n",
        "print('GPT 4 turbo model haiku:')\n",
        "print(haiku_gpt4_response.choices[0].message.content)\n",
        "print('------------------------------------')\n",
        "print('GPT 3.5 cost: %.8f $' % (Config.gpt3_model_cost * haiku_gpt3_response.usage.total_tokens))\n",
        "print('GPT 4 cost: %.8f $' % (Config.gpt4_model_cost * haiku_gpt4_response.usage.total_tokens))"
      ],
      "metadata": {
        "id": "2fNmMug5Swdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the GPT-4 model creates more creative and better haiku than GPT-3.5. However, using GPT-4o is more expensive than GPT-3.5, following this table:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>gpt-3.5-turbo:</td>\n",
        "    <td> $0.00001550</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>gpt-4o:</td>\n",
        "    <td> $0.00017400</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "We need to pay more for better results. You can learn more about OpenAI pricing on the [official website](https://openai.com/pricing)."
      ],
      "metadata": {
        "id": "W4DFA1n8T06D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creative solution**\n",
        "\n",
        "GPT is a powerful multitasking model that can solve many different tasks. You can ask any question, how who is the Great Britan monarch or information about a favorite character, and solve math and logic tasks. However, the capabilities of the models are wider than this.\n",
        "\n",
        "Using GPT, we can solve more complex tasks, and let's try to do something."
      ],
      "metadata": {
        "id": "9xV47VWsUyak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GraphGPT**\n",
        "\n",
        "The main idea is to create a prompt that will allow us to convert unstructured natural language into a knowledge graph.\n",
        "\n",
        "To do this, we need to create a prompt from our task and provide the GPT recording formats and an example of execution.\n",
        "\n",
        "\n",
        "You need fill `//-- add your text here --//` in the following prompt:\n",
        "> * Describe the task of finding as many connections as possible and writing them down in a list.\n",
        "* Note that the single format `[ENTITY 1, RELATIONSHIP, ENTITY 2]` should be used, relations are directed, and the order is important.\n",
        "* For testing, use `Maks, Petro and Vlad are colleagues.`"
      ],
      "metadata": {
        "id": "QnH7d0qDU2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Graph_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "graph_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "graph_prompt_response = gpt_completion(graph_prompt)\n",
        "graph_prompt_answer = graph_prompt_response.choices[0].message.content\n",
        "\n",
        "print(graph_prompt_answer)"
      ],
      "metadata": {
        "id": "xCl9NbK1U5iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "```\n",
        "    [[\"Maks\", \"colleague\", \"Petro\"],\n",
        "     [\"Maks\", \"colleague\", \"Vlad\"],\n",
        "     [\"Petro\", \"colleague\", \"Maks\"],\n",
        "     [\"Petro\", \"colleague\", \"Vlad\"],\n",
        "     [\"Vlad\", \"colleague\", \"Maks\"],\n",
        "     [\"Vlad\", \"colleague\", \"Petro\"]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1eZ-ZrrsVAbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create functions using [networkx](https://networkx.org/documentation/stable/reference/introduction.html) for visualizing the obtained connections:"
      ],
      "metadata": {
        "id": "ud-hmWY_VVXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] create_show_graph\n",
        "\n",
        "def create_relationships_dataframe(answer_data):\n",
        "    # Answer data is a string, we need to extract the JSON part\n",
        "    json_start = answer_data.find('[')\n",
        "    json_end = answer_data.rfind(']') + 1\n",
        "    json_data = answer_data[json_start:json_end]\n",
        "\n",
        "    # Close JSON array if it's incomplete\n",
        "    if answer_data.count('[') > answer_data.count(']'):\n",
        "        json_data += ']]'\n",
        "\n",
        "    # Parse the JSON data\n",
        "    relationships = json.loads(json_data)\n",
        "    from_label = [relationship[0] for relationship in relationships]\n",
        "    weight_label = [relationship[1] for relationship in relationships]\n",
        "    to_label = [relationship[2] for relationship in relationships]\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df_relationships = pd.DataFrame({'from': from_label, 'to': to_label, 'weight': weight_label})\n",
        "    return df_relationships\n",
        "\n",
        "def get_connection_relationship_weight(edge, df_relation):\n",
        "    name = df_relation.loc[(df_relation['from'] == edge[0]) & (df_relation['to'] == edge[1])]['weight'].iloc[0]\n",
        "    return name\n",
        "\n",
        "def show_graph(answer_data):\n",
        "    if isinstance(answer_data, list):\n",
        "        relationships = answer_data\n",
        "    else:\n",
        "        print(\"Unsupported data format\")\n",
        "        return\n",
        "\n",
        "    from_label = [relationship[0] for relationship in relationships]\n",
        "    weight_label = [relationship[1] for relationship in relationships]\n",
        "    to_label = [relationship[2] for relationship in relationships]\n",
        "    df_relationships = pd.DataFrame({'from': from_label, 'to': to_label, 'weight': weight_label})\n",
        "\n",
        "    G = nx.from_pandas_edgelist(df_relationships, 'from', 'to', create_using=nx.DiGraph())\n",
        "\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    nx.draw(G, pos, with_labels=True, node_size=800, node_color=\"lightblue\")\n",
        "\n",
        "    labels = {e: df_relationships.loc[(df_relationships['from'] == e[0]) & (df_relationships['to'] == e[1])]['weight'].iloc[0] for e in G.edges}\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
        "\n",
        "    # show graph\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "C-3YqnTPVQuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can visualize your graph."
      ],
      "metadata": {
        "id": "EOPMui17VzqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.loads(graph_prompt_answer)\n",
        "show_graph(data)"
      ],
      "metadata": {
        "id": "M20_bODfV0eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph you obtained show the relationship between Peter, Vlad and Max, as specified in your prompt. All of them are colleagues and related to each other.\n",
        "\n",
        "To make sure that the prompt you created works correctly, let us consider a more complex condition by adding external relations\n",
        "\n",
        "\n",
        "You need fill `//-- add your text here --//` in the following prompt:\n",
        "> * Paste your description and formatting.\n",
        "* Generate next relationships graph: `Markus, Mario, Clara and Mykyta are friends. Maria is Clara teacher and Markus mother and Sam lives with Mario.`"
      ],
      "metadata": {
        "id": "0C69rJOkV6_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Harder_graph_prompt\n",
        "\n",
        "### START CODE HERE ###\n",
        "harder_graph_prompt = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "harder_graph_prompt_response = gpt_completion(harder_graph_prompt, max_tokens=600)\n",
        "harder_graph_prompt_answer = harder_graph_prompt_response.choices[0].message.content\n",
        "\n",
        "print(harder_graph_prompt_answer)"
      ],
      "metadata": {
        "id": "71K3qCNmV-rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = json.loads(harder_graph_prompt_answer)\n",
        "show_graph(result)"
      ],
      "metadata": {
        "id": "A8OkI_PqrUdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "```\n",
        "[['Markus', 'friend', 'Mario'],\n",
        " ['Markus', 'friend', 'Clara'],\n",
        " ['Markus', 'mother', 'Maria'],\n",
        " ['Mario', 'friend', 'Markus'],\n",
        " ['Mario', 'friend', 'Clara'],\n",
        " ['Mario', 'friend', 'Mykyta'],\n",
        " ['Clara', 'friend', 'Markus'],\n",
        " ['Clara', 'friend', 'Mario'],\n",
        " ['Clara', 'teacher', 'Maria'],\n",
        " ['Mykyta', 'friend', 'Mario'],\n",
        " ['Maria', 'teacher', 'Clara'],\n",
        " ['Maria', 'mother', 'Markus'],\n",
        " ['Sam', 'lives with', 'Mario']]\n",
        " ```\n",
        "\n",
        "> Note: The model`s answer can be different but have the same meaning\n"
      ],
      "metadata": {
        "id": "5ORlomy3BvoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 2 - Search Engine with GPT-3**\n",
        "\n",
        "[Haystack](https://haystack.deepset.ai) is an open-source framework for building search systems that work intelligently over large document collections. Besides providing a comfortable entry point to the [OpenAI API](https://openai.com/product), Haystack offers all the other components we need to successfully implement an end-to-end NLP system with GPT: a vector database, a module for retrieval, and the pipeline that combines all those elements into one queryable system.\n",
        "In this lab, we’ll demonstrate how to build a generative question-answering system that uses the `gpt-3.5-turbo` model to present results in convincing natural language.\n"
      ],
      "metadata": {
        "id": "-ye1XcHkWRCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary library\n",
        "!pip install --upgrade farm-haystack[all]"
      ],
      "metadata": {
        "id": "nIVuN1yYWdyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import PreProcessor\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "from haystack.nodes import EmbeddingRetriever\n",
        "from haystack.nodes import PromptNode\n",
        "from haystack import Pipeline"
      ],
      "metadata": {
        "id": "5QwwbnXxU4Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build a Search Engine with GPT-3**\n",
        "\n",
        "If you’ve been online lately, you’ve likely seen the excitement about OpenAI’s newest language model, ChatGPT. ChatGPT is astonishingly good at many things, including debugging code and rewriting text in whatever style you ask it. As an offshoot of GPT-3.5, a large language model (LLM) with billions of parameters, ChatGPT owes its impressive knowledge to the fact that it’s seen a large portion of the internet during training — in the form of the Common Crawl corpus and other data.\n",
        "\n",
        "Chatbots are understandable that people are excited by a language model that can hold a conversation and create a solid semblance of intelligence. But we need to stay critical when it comes to the validity of answers generated by these models. LLMs especially are prone to hallucinations: producing text that sounds sensible at first but doesn’t hold up to closer scrutiny and presenting things as facts that are made up entirely.\n",
        "Semantic search engines — our specialty at deepset — are often powered by extractive question-answering models. These models return verbatim snippets from the knowledge base rather than generating text from scratch the way ChatGPT does.\n",
        "\n",
        "[Haystack](https://docs.haystack.deepset.ai/docs/intro), deepset’s open-source framework for applied natural language processing (NLP), allows you to leverage multiple GPT models in your pipeline. With this approach, you can build a GPT-powered semantic search engine that uses your data as ground truth and bases its natural-language answers on the information it contains. Besides providing a comfortable entry point to the OpenAI API, Haystack offers all the other components you need to successfully implement an end-to-end NLP system with GPT: a vector database, a module for retrieval, and the pipeline that combines all those elements into one queryable system."
      ],
      "metadata": {
        "id": "MvlNl5LUWtsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The advent of large language models**\n",
        "\n",
        "But while the largest BERT model has 336 million parameters, OpenAI’s largest GPT-3.5 model — which ChatGPT is based on — has 520 times as many.\n",
        "\n",
        "From observation, we can say that GPT is exceptionally good at understanding implication and intent. It can remember what’s been discussed earlier in the conversation, including figuring out what you’re referring to with words like “he” or “before that,” It can tell you when your question doesn’t make sense. All of these properties account for the increased importance of actual intelligence. It also has to generate language from scratch, a much more challenging task than returning the correct section from a corpus."
      ],
      "metadata": {
        "id": "MHXPZfZsW2iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Different types of search engines**\n",
        "\n",
        "Semantic search engines come in different varieties and can roughly be distinguished by the type of answer they return. The answers could consist of matching documents (in document search), answer spans (in extractive QA), or newly generated answers (in generative QA)."
      ],
      "metadata": {
        "id": "JKkR9y0cW3gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The GenerativeQAPipeline: Haystack’s component for a generative search engine**\n",
        "\n",
        "In this lab, we use the GenerativeQAPipeline. It consists of a retriever (to find relevant documents) and a generator (to write the text) chained together. The retriever connects to the database. Like the generator, it is often (but not necessarily) based on a Transformer model. Its task is to retrieve the documents from the database that are most likely to contain valuable information based on a user’s input query."
      ],
      "metadata": {
        "id": "SnFE_-pSW8mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download artices from Wikipedia\n",
        "\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/datasets/07_attention_transformers/GPT-3/wikipedia_articles.zip')\n",
        "with zipfile.ZipFile(\"wikipedia_articles.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"\")"
      ],
      "metadata": {
        "id": "jJ4VtzgPXLLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Task**:\n",
        "* Open the file in read mode with UTF-8 encoding and read its content\n",
        "* Create a document dictionary with 'content' and 'meta' keys\n",
        "* Append the document dictionary with 'content' and 'meta' keys\n",
        "\n"
      ],
      "metadata": {
        "id": "adfqyKgI3pTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] Read_documents\n",
        "\n",
        "# init files/articles\n",
        "wiki_files_path = Config.wiki_article_path\n",
        "article_titles = Config.article_titles\n",
        "\n",
        "def read_documents(folder_path, article_titles):\n",
        "    docs = []\n",
        "    for title in article_titles:\n",
        "        file_path = os.path.join(folder_path, f\"{title}.txt\")\n",
        "        ### START CODE HERE ###\n",
        "        ...\n",
        "        ### END CODE HERE ###\n",
        "    return docs\n",
        "\n",
        "docs = read_documents(wiki_files_path, article_titles)"
      ],
      "metadata": {
        "id": "ESEWVYQdbBiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing Documents\n",
        "\n",
        "Use the PreProcessor from the Haystack library to split and clean the documents.\n",
        "\n",
        "> **Task:**\n",
        "> 1. Initialize FAISS Document Store:\n",
        "  - A FAISS Document Store is already initialized with a `Flat index factory string`.\n",
        "2. Initialize PreProcessor with the following parameters:\n",
        "  - `clean_empty_lines`: Removes empty lines from the documents.\n",
        "  - `clean_whitespace`: Cleans up extra whitespace.\n",
        "  - `clean_header_footer`: Removes headers and footers.\n",
        "  - `split_by`: Splits documents by words.\n",
        "  - `split_length`: Each split should have a length of 200 words.\n",
        "  - `split_respect_sentence_boundary`: Ensures splits respect sentence boundaries.\n",
        "3. Process Documents:\n",
        "  - Use the initialized `PreProcessor` to process the docs variable (a list of documents).\n",
        "4. Write Documents to FAISS Document Store:\n",
        "  - Write the processed documents to the FAISS Document Store.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "llVdd_8ibycm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Preprocessing_documents\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Initialize FAISS Document Store\n",
        "document_store = ...\n",
        "\n",
        "# Initialize PreProcessor\n",
        "preprocessor = ...\n",
        "\n",
        "# Process documents\n",
        "processed_docs = ...\n",
        "# Write documents\n",
        "document_store.write_documents(...)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"\\nDocuments have been successfully preprocessed and stored.\")"
      ],
      "metadata": {
        "id": "VgRu64RNcCkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the Document Store\n",
        "\n",
        "> **Task:**\n",
        ">\n",
        "> 1. Set Embedding Dimension:\n",
        "  - Set the embedding_dim to `1536` as per the model requirements.\n",
        "> 2. Remove Existing FAISS Document Store:\n",
        ">  - If a file named `faiss_document_store.db exists, remove it` to ensure a fresh start.\n",
        "3. Initialize FAISS Document Store:\n",
        "  - Initialize the FAISS Document Store with the `Flat index factory string` and `the specified embedding dimension`.\n"
      ],
      "metadata": {
        "id": "vj9sfkG6ewf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str]: Initializing_document_store\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Set embedding dimension according to the model\n",
        "embedding_dim = ...\n",
        "\n",
        "# Remove existing FAISS document store if exists\n",
        "if os.path.exists(\"faiss_document_store.db\"):\n",
        "    ...\n",
        "\n",
        "# Initialize FAISS Document Store with specified embedding dimension\n",
        "document_store = ...\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "5wjqdHMUe0nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deleting Existing Documents and Adding New Ones\n",
        "\n",
        "Delete any existing documents in the database and add the preprocessed documents."
      ],
      "metadata": {
        "id": "9yowOK_qe_MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Rewrite_preprocessed_documents\n",
        "\n",
        "document_store.delete_documents()\n",
        "document_store.write_documents(processed_docs)"
      ],
      "metadata": {
        "id": "Wh0M4cIQfCjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d084dc0-e324-4dd0-83a5-2999a68a6b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing Documents: 10000it [00:01, 9832.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up the Retriever\n",
        "\n",
        ">**Task:**\n",
        ">\n",
        "> Initialize the `EmbeddingRetriever` with the following parameters:\n",
        " - `document_store`: The FAISS Document Store instance.\n",
        " - `embedding_model`: Set to `\"text-embedding-ada-002\"`.\n",
        " - `api_key`: The OpenAI API key from the Config class.\n",
        " - `batch_size`: Set to 32.\n",
        " - `max_seq_len`: Set to 2048."
      ],
      "metadata": {
        "id": "JhSCe4UufGvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Initialize_retriewer\n",
        "\n",
        "# Initialize Retriever with OpenAI's embedding model\n",
        "### START CODE HERE ###\n",
        "retriever = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Update embeddings in the document store\n",
        "document_store.update_embeddings(retriever)\n"
      ],
      "metadata": {
        "id": "2A4aRKdFfQiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up the Generator\n",
        "\n",
        "You are given a partially implemented script that initializes a PromptNode for text generation using OpenAI's GPT-4 model. Your task is to complete the script by initializing the PromptNode with the model name and API key. Follow the instructions below:\n",
        "\n",
        "> **Task:**\n",
        "> - Use the `PromptNode` class from haystack.nodes.\n",
        "> - Set the model name to `\"gpt-4\"`.\n",
        "> - Retrieve the OpenAI API key from the Config class (`Config.openai_api_key)`"
      ],
      "metadata": {
        "id": "BcQzFpdofnQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Initialize_generator\n",
        "\n",
        "# Initialize Generator with GPT-4 model\n",
        "### START CODE HERE ###\n",
        "generator = ...\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "Tp-RPq1e5oFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a Custom Pipeline:\n",
        "\n",
        "> **Task:**\n",
        "> Use the `Pipeline` class from the haystack module to create a new pipeline instance.\n",
        "> 1. Add the retriever node to the pipeline:\n",
        "  - Set the `component` to the `retriever` instance.\n",
        "  - Set the name to `\"Retriever\"`.\n",
        "  - Set the inputs to `[\"Query\"]`.\n",
        "2. Add the generator node to the pipeline:\n",
        "  - Set the `component` to the `generator` instance.\n",
        "  - Set the name to `\"Generator\"`.\n",
        "  - Set the inputs to `[\"Retriever\"]`."
      ],
      "metadata": {
        "id": "UWOLOf8K7_64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Create_pipeline\n",
        "\n",
        "# Create a custom pipeline and add the retriever and generator nodes\n",
        "### START CODE HERE ###\n",
        "pipeline = ...\n",
        "pipeline.add_node(...)\n",
        "pipeline.add_node(...)\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "tZun8Pri8Ab-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.components"
      ],
      "metadata": {
        "id": "2IPpCLvElxTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "{'Generator': <haystack.nodes.prompt.prompt_node.PromptNode at 0x0000>,\n",
        " 'FAISSDocumentStore': <haystack.document_stores.faiss.FAISSDocumentStore at 0x0000>,\n",
        " 'Retriever': <haystack.nodes.retriever.dense.EmbeddingRetriever at 0x0000>}\n",
        "```"
      ],
      "metadata": {
        "id": "V95TusCek6YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass retriever results to generator and set max_tokens for longer completions\n",
        "query = 'Where is located Kyiv zoo?'\n",
        "generator_results = generator.prompt(query, documents=retriever, max_tokens=100)\n",
        "\n",
        "# If generator_results is a list of strings, create dictionaries with empty contexts\n",
        "formatted_results = [{\"answer\": query, \"context\": result} for result in generator_results]\n",
        "print(formatted_results)"
      ],
      "metadata": {
        "id": "xStYrIH-_pMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "\n",
        "```\n",
        "[{'answer': 'Where is located Kyiv zoo?',\n",
        "'context': 'Kyiv Zoo is located in Kyiv, the capital of Ukraine.\n",
        "The exact address is: Academician Bohdan Khmelnytsky Street, 1, Kyiv, Ukraine, 04060.'}]\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning\n"
      ],
      "metadata": {
        "id": "Ka8sabYHmJ_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Task:** Implement a Function to Query the User Pipeline. Follow the instructions below:\n",
        "1. Perform Retrieval:\n",
        "  - Use the `retriever` instance to `retrieve` relevant documents based on the query.\n",
        "  - Set top_k to `the top_k_retriever` parameter (default is 5).\n",
        "2. Pass Retrieval Results to Generator:\n",
        "  - Use the `generator` instance to `prompt` a response based on the query and the retrieved documents.\n",
        "  - Set the max_tokens to the `max_tokens` parameter (default is 1000)."
      ],
      "metadata": {
        "id": "s3aD2oWdnZ0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] Query_pipeline\n",
        "\n",
        "# Function to query the user pipeline and print the answer\n",
        "\n",
        "### START CODE HERE ###\n",
        "def query_pipeline(query, top_k_retriever=5, max_tokens=1000):\n",
        "    # Performing retrieval\n",
        "    retriever_results = ...\n",
        "\n",
        "    # Passing retrieval results to the generator and setting max_tokens for longer completions\n",
        "    generator_results = ...\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "    # Checking if generator_results is empty\n",
        "    if not generator_results:\n",
        "        print(\"No answers found.\")\n",
        "        return\n",
        "\n",
        "    # Print answers\n",
        "    print(generator_results)\n"
      ],
      "metadata": {
        "id": "RDfLG9Ey8dXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Query_pipeline_first\n",
        "\n",
        "query_pipeline_first = query_pipeline(\"What is Kyiv known for?\")\n",
        "print(query_pipeline_first)"
      ],
      "metadata": {
        "id": "rFCk7bXZEI1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "```\n",
        "[\"Kyiv, the capital of Ukraine, is known for its rich history, beautiful architecture ...\"]\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning\n",
        "\n"
      ],
      "metadata": {
        "id": "xBdqGkiHokAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Query_pipeline_second\n",
        "\n",
        "query_pipeline_second = query_pipeline(\"When is the best time to visit Kyiv?\")\n",
        "print(query_pipeline_second)"
      ],
      "metadata": {
        "id": "i3XqmSm7ESb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "```\n",
        "['The best time to visit Kyiv, Ukraine is during late spring (April to June) and early autumn ...']\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "oAGrVJvUpjee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Query_pipeline_third\n",
        "\n",
        "query_pipeline_third = query_pipeline(\"Do people from Kyiv have their own culture?\")\n",
        "print(query_pipeline_third)"
      ],
      "metadata": {
        "id": "rLz1UiMOEUpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "\n",
        "```\n",
        "['Yes, people from Kyiv, the capital city of Ukraine, have their own distinctive culture. ...']\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "uAaXskj7p58l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Query_pipeline_four\n",
        "\n",
        "query_pipeline_four = query_pipeline(\"Tell me about some interesting places in Kyiv\")\n",
        "print(query_pipeline_four)"
      ],
      "metadata": {
        "id": "UW9W_NRxEZP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "\n",
        "```\n",
        "['1. Kyiv Pechersk Lavra: Also known as the Monastery of the Caves ...']\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "oIclAzCGqFXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Query_pipeline_five\n",
        "\n",
        "query_pipeline_five = query_pipeline(\"How was the TV tower built?\")\n",
        "print(query_pipeline_five)"
      ],
      "metadata": {
        "id": "nqnHjwdUEbIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "\n",
        "```\n",
        "['The construction process of a TV tower can vary depending on the specific ...']\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "z5mD765ZqR1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Query_pipeline_six\n",
        "\n",
        "query_pipeline_six = query_pipeline(\"Is Kyiv a good place for clubbing?\")\n",
        "print(query_pipeline_six)"
      ],
      "metadata": {
        "id": "AmA48yhMEfe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result:\n",
        "\n",
        "```\n",
        "['Yes, Kyiv is known for its vibrant nightlife and is indeed a good place for clubbing ...']\n",
        "```\n",
        "> Note: The answer can be different but have the same meaning"
      ],
      "metadata": {
        "id": "NMKMxbwRqboM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 3 - Fine-tune GPT**\n",
        "\n",
        "Fine-tuning is the process of training a [Large Language Model (LLM)](https://en.wikipedia.org/wiki/Large_language_model) to recognize a specific pattern of input and output that can be applied to any custom NLP task.\n",
        "\n",
        "Taken from the [official docs](https://platform.openai.com/docs/guides/fine-tuning), fine-tuning lets you get more out of the GPT-3 models by providing the following:\n",
        "* Higher quality results than prompt design\n",
        "* Ability to train on more examples than can fit in a * prompt\n",
        "* Token savings due to shorter prompts\n",
        "* Lower latency requests\n",
        "\n",
        "Fine-tuning involves the following steps:\n",
        "* Prepare and upload training data\n",
        "* Train a new fine-tuned model\n",
        "* Use your fine-tuned model"
      ],
      "metadata": {
        "id": "03UHzI3ZEnCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/datasets/07_attention_transformers/GPT-3/QA_DataRoot_Labs.csv')"
      ],
      "metadata": {
        "id": "4iiIJsVqOYj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('QA_DataRoot_Labs.csv')\n",
        "\n",
        "print('Prompt:', data.iloc[0]['prompt'])\n",
        "print('Completion:', data.iloc[0]['completion'])"
      ],
      "metadata": {
        "id": "FS8LptbFOeeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset have such view:\n",
        "```\n",
        "{\"prompt\": \"<question>\", \"completion\": \"<ideal answer>\"}\n",
        "{\"prompt\": \"<question>\", \"completion\": \"<ideal answer>\"}\n",
        "{\"prompt\": \"<question>\", \"completion\": \"<ideal answer>\"}\n",
        "```"
      ],
      "metadata": {
        "id": "F1belwM1V-Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the our dataset should be of the following format:\n",
        "\n",
        "```\n",
        "{\"messages\":[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}, {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"}]}\n",
        "{\"messages\":[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What's the weather today?\"}, {\"role\": \"assistant\", \"content\": \"The weather is sunny and warm today.\"}]}\n",
        "```\n",
        "and saved in `.jsonl` format file."
      ],
      "metadata": {
        "id": "oyvJ3ZDBwpFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a **JSONL** file to fine tune it.\n",
        "\n",
        "> Task:\n",
        "- create a jsonl file with the correct format"
      ],
      "metadata": {
        "id": "D7s4TT_FWY3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Create_fine_tune_jsonl\n",
        "\n",
        "csv_file_path = 'QA_DataRoot_Labs.csv'\n",
        "jsonl_file_path = 'QA_DataRoot_Labs_chat.jsonl'\n",
        "\n",
        "with open(csv_file_path, mode='r', encoding='utf-8') as csv_file, open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        ### START CODE HERE ###\n",
        "        json_record = {...}\n",
        "        ### END CODE HERE ###\n",
        "        jsonl_file.write(json.dumps(json_record, ensure_ascii=False) + '\\n')"
      ],
      "metadata": {
        "id": "60Mg6-MFV9dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check data formatting"
      ],
      "metadata": {
        "id": "UoJs40_mzO5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Test_jsonl_file\n",
        "\n",
        "# Load the dataset\n",
        "with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)"
      ],
      "metadata": {
        "id": "DKMwpkNeksPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "Num examples: 36\n",
        "First example:\n",
        "{'role': 'user', 'content': 'What is it DataRoot Labs?'}\n",
        "{'role': 'assistant', 'content': ' DataRoot Labs is a full-service Data Science & Artificial Intelligence R&D company with main focus on Big Data Management & Strategy Consulting, Data Science & Engineering.DataRoot Labs consists of AI, HighLoad and Science teams — geeks, really good at building & assembling AI-Enabled solutions & Infrastructures, complex scientific R&D.AI Lab delivers to our partners and clients the unique value leveraging Deep Learning, Computer Vision, NLP, Advanced Scoring Models'}\n",
        "```"
      ],
      "metadata": {
        "id": "nkUUzj8ZsSfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Upload a training file**\n",
        "\n",
        "Once you have the data validated, the file needs to be uploaded using the [Files API](https://platform.openai.com/docs/api-reference/files/create) in order to be used with a fine-tuning jobs:\n",
        "\n",
        "> **Task:**\n",
        "1. Use the `client.files.create` method to upload the file.\n",
        "2. Set the purpose parameter to `fine-tune`."
      ],
      "metadata": {
        "id": "yta0ZNfr0G5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Upload_training_file\n",
        "\n",
        "### START CODE HERE ###\n",
        "upload_file = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "file_id = upload_file.id"
      ],
      "metadata": {
        "id": "Q686FVbbtVum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create a fine-tuned model**\n",
        "\n",
        "After ensuring you have the right amount and structure for your dataset, and have uploaded the file, the next step is to create a fine-tuning job. We support creating fine-tuning jobs via the [fine-tuning UI](https://platform.openai.com/finetune) or programmatically.\n",
        "\n",
        "\n",
        "> **Task:**\n",
        "1. Use the `client.fine_tuning.jobs.create` method to create a fine-tuning job.\n",
        "2. Set the `training_file` parameter to the file_id obtained from the file upload.\n",
        "3. Set the model parameter to `\"gpt-3.5-turbo\"`."
      ],
      "metadata": {
        "id": "OWSOIayp0qh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Create_job\n",
        "\n",
        "### START CODE HERE ###\n",
        "create_job = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "job_id = create_job.id"
      ],
      "metadata": {
        "id": "hlkwDaHztosr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Use a fine-tuned model**\n",
        "\n",
        "When a job has succeeded, you will see the fine_tuned_model field populated with the name of the model when you retrieve the job details. You may now specify this model as a parameter to in the [Chat Completions](https://platform.openai.com/docs/api-reference/chat).\n",
        "\n",
        "After your job is completed, the model should be available right away for inference use. In some cases, it may take several minutes for your model to become ready to handle requests. If requests to your model time out or the model name cannot be found, it is likely because your model is still being loaded. If this happens, try again in a few minutes.\n",
        "\n",
        ">**Task:**\n",
        "Find your model in [playground ChatGPT](https://platform.openai.com/finetune) and put the name to reference code:"
      ],
      "metadata": {
        "id": "y4S1wnpt5xNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str] Reference_fine_tune_model\n",
        "### START CODE HERE ###\n",
        "fine_tuned_model_name = ...\n",
        "### END CODE HERE ###\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=fine_tuned_model_name,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where is DataRoot Labs headquartered?\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "8593f-82vYZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "```\n",
        "DataRoot Labs is headquarted in Kyiv, Ukraine.\n",
        "```"
      ],
      "metadata": {
        "id": "JqcRfR0m52a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "As we can see, our model fits well the hypothesis function to the data.\n",
        "\n",
        "What's next:\n",
        "- Try experimenting with GPT models and prompt\n",
        "- Using OpenAI API try text to image DALL·E or speech to text Whisper models.\n",
        "- Fine-tune own model using another QA dataset.\n",
        "\n",
        "Make sure that you didn't add or delete any notebook cells. Otherwise your work may not be accepted by the validator!"
      ],
      "metadata": {
        "id": "xTFgt4klB7Km"
      }
    }
  ]
}