{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/week7/template_transformers_pipelines_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvziXoe21z_G_gh"
      },
      "source": [
        "# **Hugging Face Transformers Library**\n",
        "[Hugging Face](https://huggingface.co) is a large open-source community. Their [Hub](https://huggingface.co/models) has a very large number of pre-trained deep learning models, mainly aimed at NLP, using Transformers. In this lab, you will learn how to use pre-trained Hugging Face models to solve various NLP, CV, and Audio tasks.\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- How to use `pipeline()` from the [Models Hub](https://huggingface.co/models) for inference on a variety of NLP, CV, and Audio tasks\n",
        "- How to use [Gradio](https://gradio.app/docs/) library for easy creating user interface and handling the pipeline inference directly on Google Colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQuWgaHU13zo"
      },
      "source": [
        "# **Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kbi2PMjy109L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1ebb56-493a-4bc2-e954-bd8cce4d5c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.1\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=507268 sha256=919588492d7661c5877e98c6b43ff15458d97fcfaa43d98369362fbf22ea98d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio)\n",
            "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.0 (from gradio)\n",
            "  Downloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.5.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.2)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=4fe2918e3da00bbf67e3221f529258a16569f376b7fa773ad6f0e598524812c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, gradio-client, fastapi-cli, altair, fastapi, gradio\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed aiofiles-23.2.1 altair-5.3.0 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.38.1 gradio-client-1.1.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.5.3 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-1.0.7\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "#from transformers import Conversation\n",
        "\n",
        "!pip install sentence_transformers\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "\n",
        "!pip install gradio\n",
        "import gradio as gr\n",
        "\n",
        "!pip install sentencepiece\n",
        "\n",
        "!pip install timm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cpLrb0yup3v4"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] set_seed\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83w7BY3B1-HT"
      },
      "source": [
        "**List of tasks that we will cover:**\n",
        "\n",
        "- **NLP**:\n",
        "    - Masked language modeling\n",
        "    - Question Answering\n",
        "    - Table Question Answering\n",
        "    - Summarization\n",
        "    - Sentence Similarity\n",
        "    - Text Classification\n",
        "    - Token Classification\n",
        "    - Zero-Shot Classification\n",
        "    - Translation\n",
        "    - Text generation\n",
        "    - Text2Text Generation\n",
        "    - Conversational\n",
        "- **Audio**:\n",
        "    - Text-to-Speech\n",
        "    - Automatic Speech Recognition\n",
        "\n",
        "- **Computer Vision**:\n",
        "    - Image Classification\n",
        "    - Image Segmentation\n",
        "\n",
        "The easiest way to use a pre-trained Transformer model on a given task is to use a `pipeline()`.\n",
        "\n",
        "[Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) helps to perform all pre-processing and post-processing steps on your input data:\n",
        "1. **Tokenization:** Split the initial input into multiple sub-entities with properties (tokens).\n",
        "2. **Inference:** Maps every token into a more meaningful representation.\n",
        "3. **Decoding:** Use the above representation to generate and/or extract the final output for the given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhEId9ru5WJ2"
      },
      "source": [
        "# **NLP Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fg2fNqD2FHC"
      },
      "source": [
        "## **Masked language modeling**\n",
        "\n",
        "[Masked language modeling](https://huggingface.co/tasks/fill-mask) (MLM) is the task of masking tokens (e.g., replaced by `[MASK]`) in a sequence and predicting which words should replace this masked word with an appropriate token.\n",
        "\n",
        "### Inference with Fill-Mask Pipeline\n",
        "\n",
        "You can use the Transformers library `fill-mask` pipeline to infer with masked language models, which you can find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads). There are many pretrained models here that can be used to solve the MLM task.\n",
        "\n",
        "**Excercise:**\n",
        ">For the proposed `masked_sequences`, we suggest finding you among the models on the [hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads) a pretrained base BERT (uncased) model, which is already pretrained on a large corpus of English data, and use it in the pipeline for inference. Your model should return `top_k=2` predictions for each sentence in `unmasked_sequences`. All the parameters that the `fill-mask` pipeline uses are described [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.FillMaskPipeline).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BANDUtPF150z"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_mlm\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "masked_sequences = [\"I love animals very much, that's why I work as a [MASK].\",\n",
        "        \"I live several years in Russia, but now I live in [MASK].\",\n",
        "        \"I love to play [MASK], so my bought me a ball.\"\n",
        "        ]\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_mlm =                                 # specify: task and model\n",
        "unmasked_sequences =                        # use the pipeline with parameter top_k=2\n",
        "### END CODE HERE ###\n",
        "\n",
        "for text_i in range(len(masked_sequences)):\n",
        "    print(f\"\\nMasked sentence {text_i + 1}: {masked_sequences[text_i]}\\nSuggested unmasked sentences:\")\n",
        "    for output_i in unmasked_sequences[text_i]:\n",
        "        print(f\"\\t{output_i['sequence']} \\t | Masked word: {output_i['token_str']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpClPQK1qi5j"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Masked sentence 1: I love animals very much, that's why I work as a [MASK].\n",
        "Suggested unmasked sentences:\n",
        "\ti love animals very much, that's why i work as a vet. \t | Masked word: vet\n",
        "\ti love animals very much, that's why i work as a farmer.   | Masked word: farmer\n",
        "\n",
        "Masked sentence 2: I live several years in Russia, but now I live in [MASK].\n",
        "Suggested unmasked sentences:\n",
        "\ti live several years in russia, but now i live in france. \t | Masked word: france\n",
        "\ti live several years in russia, but now i live in ukraine. \t| Masked word: ukraine\n",
        "\n",
        "Masked sentence 3: I love to play [MASK], so my bought me a ball.\n",
        "Suggested unmasked sentences:\n",
        "\ti love to play football, so my bought me a ball. \t | Masked word: football\n",
        "\ti love to play baseball, so my bought me a ball. \t | Masked word: baseball\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV9_ctyt9MkG"
      },
      "source": [
        "## **Question Answering**\n",
        "[Question Answering](https://huggingface.co/tasks/question-answering) (QA) is a type of language task that produces answers based on a given document/text or generates answers without context.\n",
        "\n",
        "### Inference with Question Answering Pipeline\n",
        "You can find on the Hugging Face Hub a pretrained model among a wide variety of models for [QA tasks](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads). [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.QuestionAnsweringPipeline) is a description of the parameters for the `question-answering` pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads) an original pretrained model to produce answers based on a given text. This model makes up the composition of transformer encoders, and it is an autoencoder. Architecture is: `24-layer`, `1024-hidden`, `16-heads`, `340M params`. This model was trained on lower-cased English text using the technique that masks all subwords corresponding to a word at once and was fine-tuned on the Stanford Question Answering Dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJGsKjaU2N_Z"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_qa\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "### START CODE HERE ### (≈ 1-2 lines of code)\n",
        "model_qa =\n",
        "### END CODE HERE ###\n",
        "\n",
        "context = r\"\"\"\n",
        "These fish have names.\n",
        "This is Finny.\n",
        "Finny has beautiful long fins that help her swim fast.\n",
        "This is Tayla.\n",
        "Tayla’s big tail moves from side to side and it helps her to go this way or that way.\n",
        "This is Igor.\n",
        "Igor has great big eyes. Igor’s great big eyes help him to see where he is going and they also help him to see\n",
        "big scary fish!\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"Which fish has long fins?\",\n",
        "    \"Which fish has great big eyes?\",\n",
        "    \"Which fish has a big tail?\",\n",
        "    \"Which of these fish would you most like to have as a pet?\"\n",
        "    ]\n",
        "\n",
        "for question in questions:\n",
        "    answ = model_qa(question=question, context=context)\n",
        "    print(f\"Question: {question}\\nAnswer: {answ['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1QjRCfXrbKL"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Question: Which fish has long fins?\n",
        "Answer: Finny\n",
        "Question: Which fish has great big eyes?\n",
        "Answer: Igor\n",
        "Question: Which fish has a big tail?\n",
        "Answer: Tayla\n",
        "Question: Which of these fish would you most like to have as a pet?\n",
        "Answer: Igor\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2oAMFt-_tFu"
      },
      "source": [
        "## **Table Question Answering**\n",
        "\n",
        "The task aims to find semantic question answering based on structured tabular data.\n",
        "\n",
        "### Inference with Table Question Answering Pipeline\n",
        "\n",
        "For solving the TQA task, you can find a model on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads) and use it for inference with the `table-question-answering` pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads) a pretrained model and use it to find answers based on the given table. It is a BERT-based model from Google with architecture: `24-layer`, `1024-hidden`, `16-heads`, `340M params`. The model is fine-tuned on WikiTable Questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogfrKfRG_2F6"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_tqa_de\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "data = {\n",
        "    \"country\": [\"USA\", \"France\", \"Poland\"],\n",
        "    \"donated money\": [\"1.5 billion\", \"2 billion\", \"0.5 billion\"],\n",
        "    \"humanitarian aid\": [\"30 tons\", \"2 tons\", \"50 tons\"],\n",
        "    \"date\": [\"7 march 2022\", \"10 march 2022\", \"28 march 2022\"],\n",
        "}\n",
        "\n",
        "questions = [\"how did the country donate the most?\",\n",
        "             \"how much humanitarian aid poland gave?\",\n",
        "             \"who donated 1.5 billion?\",\n",
        "             \"when did france donate the money?\",\n",
        "             \"whoes donated amount is lowest among all?\"]\n",
        "\n",
        "\n",
        "table = pd.DataFrame.from_dict(data)\n",
        "\n",
        "### START CODE HERE ### (≈ 1-2 lines of code)\n",
        "model_tqa =\n",
        "### END CODE HERE ###\n",
        "\n",
        "for question in questions:\n",
        "    output = model_tqa(table=table, query=question)\n",
        "    print(f\"Question: {question}, \\nAnswer: {output['cells'][0]}\")\n",
        "    print(30*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSGzJ0xerp0K"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Question: how did the country donate the most?,\n",
        "Answer: France\n",
        "------------------------------\n",
        "Question: how much humanitarian aid poland gave?,\n",
        "Answer: 50 tons\n",
        "------------------------------\n",
        "Question: who donated 1.5 billion?,\n",
        "Answer: USA\n",
        "------------------------------\n",
        "Question: when did france donate the money?,\n",
        "Answer: 10 march 2022\n",
        "------------------------------\n",
        "Question: whoes donated amount is lowest among all?,\n",
        "Answer: Poland\n",
        "------------------------------\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX4osP1xHLNa"
      },
      "source": [
        "## **Summarization**\n",
        "\n",
        "[Summarization](https://huggingface.co/tasks/summarization) is the task of producing a shorter version of one or several docs preserving the meaning of the text.\n",
        "\n",
        "### Inference with Summarization Pipeline\n",
        "\n",
        "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), you can find the Transformers library `summarization` pipeline to infer with models, which are pretrained for summarization tasks.\n",
        "\n",
        "**Excercise:**\n",
        ">Using a model description, find it and use it in the pipeline summarization for the given text. It is a transformer encoder-decoder that combines Google's BERT (encoder) and OpenAI's GPT (decoder). The model contains `1024-hidden` layers and `406M params` and has been fine-tuned using CNN (news summarization dataset).\n",
        "\n",
        "Generate summary using parameters:\n",
        "\n",
        " - `min_length=50`\n",
        " - `max_length=120`\n",
        " - `length_penalty=2.0`\n",
        " - `num_beams=4`\n",
        " - `early_stopping=True`\n",
        "\n",
        "[This article](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb) will help you understand a bit more about these parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7EE0WSDADZo"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_summarizer\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "text = \"\"\"Boris Akunin: Initially, the reason was that I have no trust for KGB people at all.\n",
        "So someone who comes from the KGB, from my point of view, should not become the president of Russia.\n",
        "So I never voted for Putin. I was always, let’s say, suspicious of him. And my suspicions proved to be true very fast,\n",
        "because the first thing that Putin did, he attacked the independent media. I remember quite well signing letters in support\n",
        "of independent media, and well, we lost.\n",
        "\n",
        "And then that man was very methodically killing all the branches of democracy. He started with television,\n",
        "he monopolised propaganda and used TV channels to zombify people, and this has been going on for more than 20 years.\n",
        "\n",
        "So it’s absolutely not surprising that lots of Russians believe all the lies that are being told about Ukraine and about\n",
        "the world in general, not surprising at all.\n",
        "\n",
        "Then he destroyed the independent courts and judicial system, he destroyed the parliamentary system, then he corrupted presidential\n",
        "elections. There hasn’t been any free presidential election during Putin’s times.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ### (≈ 2-8 lines of code)\n",
        "model_summarizer =\n",
        "summarized_text =\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(summarized_text[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vib5QdGUrwBF"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Akunin: 'I was always, let's say, suspicious of him' 'The first thing that Putin did, he attacked the independent media' 'He monopolised propaganda and used TV channels to zombify people' 'It's absolutely not surprising that lots of Russians believe all the lies'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c87ZWHtOKT3p"
      },
      "source": [
        "## **Sentence Similarity**\n",
        "\n",
        "[Sentence Similarity](https://huggingface.co/tasks/sentence-similarity) is the task of determining how similar the semantic similarity between two texts.\n",
        "\n",
        "### The Sentence Transformers library\n",
        "The [Sentence Transformers](https://www.sbert.net) library helps to calculate embeddings of textual documents. An embedding is just a vector that captures the semantic information. They help find how similar texts are.\n",
        "\n",
        "**Excercise:**\n",
        ">Your task is to find a [sentence transformer](https://www.sbert.net/docs/pretrained_models.html) on the [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers&sort=downloads) to calculate the similarity between given sentences. Transformer model description: it is an all-round tuned model with one of the highest encoding speeds (sentences/ sec ~ 14200). Model architecture: `max-seq-len 256`, `384-hidden`, `6-layer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAEgR0CFHs5N"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_sentence_similarity\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "sentences = ['I love data science!',\n",
        "             \"To be honest, I don't really like data science.\",\n",
        "             \"Data science is a very powerful thing.\",\n",
        "             \"I love programming in python.\"]\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "model_sentence_similarity =\n",
        "embeddings =\n",
        "### END CODE HERE ###\n",
        "\n",
        "# compute cosine similarities\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "# find the pairs with the highest cosine similarity scores\n",
        "seq_pairs = []\n",
        "for i in range(len(cosine_scores) - 1):\n",
        "    for j in range(i + 1, len(cosine_scores)):\n",
        "        seq_pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "# sort scores\n",
        "seq_pairs = sorted(seq_pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for seqs_pair in seq_pairs:\n",
        "    i, j = seqs_pair['index']\n",
        "    print(\"{} <--VS--> {} | Score: {:.3f}\\n\".format(sentences[i], sentences[j], seqs_pair['score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcor51mKrxeU"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "I love data science! <--VS--> To be honest, I don't really like data science. | Score: 0.805\n",
        "\n",
        "I love data science! <--VS--> Data science is a very powerful thing. | Score: 0.741\n",
        "\n",
        "To be honest, I don't really like data science. <--VS--> Data science is a very powerful thing. | Score: 0.739\n",
        "\n",
        "I love data science! <--VS--> I love programming in python. | Score: 0.482\n",
        "\n",
        "To be honest, I don't really like data science. <--VS--> I love programming in python. | Score: 0.417\n",
        "\n",
        "Data science is a very powerful thing. <--VS--> I love programming in python. | Score: 0.382\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etU2QJgkRvAY"
      },
      "source": [
        "## **Text Classification**\n",
        "\n",
        "[Text classification](https://huggingface.co/tasks/text-classification) is the process of classifying documents into predefined categories (labels) based on their content. Text classification models use in sentiment analysis, natural language inference, and assessing grammatical correctness cases.\n",
        "\n",
        "### Inference with Text Classification Pipeline\n",
        "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-classification), you can find the Transformers library `text-classification` pipeline to infer with Text Classification models.\n",
        "\n",
        "**Excercise:**\n",
        ">Find described model on the Hub and classify emotions for the given English text data. This model is a distilled version of the same model name and, on average, is twice as fast as the base version. It follows the same training procedure as DistilBERT. The model has `6-layers`, `768-hidden`, and `12-heads`, `82M params`.\n",
        "\n",
        "As one of the pipeline parameters, use `return_all_scores=False` to return one max scored emotion per sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPml7FgCO6HC"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_text_classifier_de\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# emotions model can predict\n",
        "emoji_dict = {\n",
        "    'sadness': \"\\U0001F62D\",\n",
        "    'anger': \"\\U0001F621\",\n",
        "    'disgust': \"\\U0001F922\",\n",
        "    'fear': \"\\U0001F628\",\n",
        "    'joy': \"\\U0001F601\",\n",
        "    'neutral': \"\\U0001F610\",\n",
        "    'surprise': \"\\U0001F62F\"\n",
        "}\n",
        "\n",
        "texts = [\"All human souls are connected, which explains why it hurts so much to detach from others.\",\n",
        "        \"The present moment is filled with joy and happiness. If you are attentive, you will see it.\",\n",
        "        \"Learn to use the criticism as fuel and you will never run out of energy.\"]\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_text_classifier =\n",
        "appropriate_emotions =\n",
        "### END CODE HERE ###\n",
        "\n",
        "for text, emotion in zip(texts, appropriate_emotions):\n",
        "    print(\"{} | Emoji: {}({}) | Score: {:.3f}\\n\".format(text, emoji_dict[emotion['label']],emotion['label'], emotion['score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1aM9VP6r0Fs"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "All human souls are connected, which explains why it hurts so much to detach from others. | Emoji: 😭(sadness) | Score: 0.807\n",
        "\n",
        "The present moment is filled with joy and happiness. If you are attentive, you will see it. | Emoji: 😁(joy) | Score: 0.925\n",
        "\n",
        "Learn to use the criticism as fuel and you will never run out of energy. | Emoji: 😐(neutral) | Score: 0.758\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E09mWONKcCi5"
      },
      "source": [
        "## **Token Classification**\n",
        "\n",
        "It is one of the NLP tasks in which a label is assigned to some tokens in a text. [Token Classification](https://huggingface.co/tasks/token-classification) models use in Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models are aimed to identify specific entities in a text (dates, individuals, and places). PoS tagging uses in identifying verbs, nouns, and punctuation marks.\n",
        "\n",
        "Our task here will be to identify the words' entity for the given text so that we will solve the NER problem.\n",
        "\n",
        "### Inference with Token Classification Pipeline\n",
        "\n",
        "Here on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads), you can find pretrained models that can solve the Named Entity Recognition problem and use them in `token-classification` or `ner` pipelines.\n",
        "\n",
        "**Excercise:**\n",
        ">You should do NER for the given sentence and find a described model on the [Hub](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads) to solve the task. The model is a fine-tuned famous bidirectional transformer for NER, trained on upper-cased English text. Model architecture: `768-hidden`, `12-heads`, and `6-layers`.\n",
        "\n",
        "Each token will be classified by the model as one of the following classes:\n",
        "\n",
        "| Abbreviation | Description             |\n",
        "|-------|--------------------------------|\n",
        "| O     | Outside of a named entity      |\n",
        "| B-MIS | Beginning of a miscellaneous entity right after another miscellaneous entity                     |\n",
        "| I-MIS | Miscellaneous entity           |\n",
        "| B-PER | Beginning of a person’s name right after another person’s name |\n",
        "| I-PER | Person’s name                  |\n",
        "| B-ORG | Beginning of an organization right after another organization |\n",
        "| I-ORG | Organization |\n",
        "| B-LOC | Beginning of a location right after another location |\n",
        "| I-LOC | Location |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L3j-pJscHYN"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_token_cls\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "sequence = \"Mark and Mary live in Ukraine. They love to travel around their country. Their favorite cities are Kharkiv and Chernihiv.\"\n",
        "\n",
        "### START CODE HERE ### (≈ 1-3 lines of code)\n",
        "model_token_cls =\n",
        "### END CODE HERE ###\n",
        "\n",
        "ner_result = model_token_cls(sequence)\n",
        "ner_result_df = pd.DataFrame(ner_result)\n",
        "ner_result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK5E4STJr1Qb"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <td>entity</td>\n",
        "        <td>score</td>\n",
        "        <td>index</td>\n",
        "        <td>word</td>\n",
        "        <td>start</td>\n",
        "        <td>end</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>0</td>\n",
        "        <td>B-PER</td>\n",
        "        <td>0.999165</td>\n",
        "        <td>1</td>\n",
        "        <td>Mark</td>\n",
        "        <td>0</td>\n",
        "        <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>1</td>\n",
        "        <td>B-PER</td>\n",
        "        <td>0.997683</td>\n",
        "        <td>3</td>\n",
        "        <td>Mary</td>\n",
        "        <td>9</td>\n",
        "        <td>13</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>2</td>\n",
        "        <td>B-LOC</td>\n",
        "        <td>0.999792</td>\n",
        "        <td>6</td>\n",
        "        <td>Ukraine</td>\n",
        "        <td>22</td>\n",
        "        <td>29</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>3</td>\n",
        "        <td>B-LOC</td>\n",
        "        <td>0.999481</td>\n",
        "        <td>20</td>\n",
        "        <td>K</td>\n",
        "        <td>99</td>\n",
        "        <td>100</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>4</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.998003</td>\n",
        "        <td>21</td>\n",
        "        <td>\\##har</td>\n",
        "        <td>100</td>\n",
        "        <td>103</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>5</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.996411</td>\n",
        "        <td>22</td>\n",
        "        <td>\\##ki</td>\n",
        "        <td>103</td>\n",
        "        <td>105</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>6</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.994031</td>\n",
        "        <td>23</td>\n",
        "        <td>\\##v</td>\n",
        "        <td>105</td>\n",
        "        <td>106</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>7</td>\n",
        "        <td>B-LOC</td>\n",
        "        <td>0.999337</td>\n",
        "        <td>25</td>\n",
        "        <td>Ch</td>\n",
        "        <td>111</td>\n",
        "        <td>113</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>8</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.938038</td>\n",
        "        <td>26</td>\n",
        "        <td>\\##ern</td>\n",
        "        <td>113</td>\n",
        "        <td>116</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>9</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.998129</td>\n",
        "        <td>27</td>\n",
        "        <td>\\##ih</td>\n",
        "        <td>116</td>\n",
        "        <td>118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>10</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.995637</td>\n",
        "        <td>28</td>\n",
        "        <td>\\##iv</td>\n",
        "        <td>118</td>\n",
        "        <td>120</td>\n",
        "    </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_mkqBSIgeha"
      },
      "source": [
        "## **Zero-Shot Classification**\n",
        "\n",
        "The zero-shot classification task is characterized by the lack of labeled examples for the classes of interest. So, you can specify any class names you want to classify for texts.\n",
        "\n",
        "### Inference with Zero-Shot Classification Pipeline\n",
        "\n",
        "A model can be loaded with the `zero-shot-classification` pipeline from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads). You can then use found pipeline to classify sequences into any class names you specify.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model and do zero-shot classification for the given sequences. The target model is an original transformer encoder-decoder that combines Google's BERT and OpenAI's GPT with a two-layer classification head, finetuned on the MNLI dataset. Architecture: `1024-hidden`, `16-heads` and `12-layers`. When calling the pipeline, pass `multi_label=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDaKMc9KcVlQ"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_zsc\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "sequences_to_classify = [\"My dog died yesterday\",\n",
        "                         \"Tomorrow I will have a very difficult exam\"]\n",
        "# you specify any class names\n",
        "candidate_labels = ['education', 'university', 'studies', 'pain', 'sadness', 'happy', 'love']\n",
        "\n",
        "### START CODE HERE ### (≈ 2-5 lines of code)\n",
        "model_zsc =\n",
        "labels_and_scores =\n",
        "### END CODE HERE ###\n",
        "\n",
        "for output_dict in labels_and_scores:\n",
        "    print(f\"\\nSequence: {output_dict['sequence']}\\nLabels and their Scores:\")\n",
        "    for labe, scr in zip(output_dict['labels'], output_dict['scores']):\n",
        "        print(\"\\t{} | {:.3}\".format(labe, scr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZW3dUBkr2Wu"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Sequence: My dog died yesterday\n",
        "Labels and their Scores:\n",
        "\tsadness | 0.961\n",
        "\tpain | 0.886\n",
        "\tlove | 0.0318\n",
        "\tstudies | 0.00189\n",
        "\thappy | 0.000906\n",
        "\tuniversity | 0.000197\n",
        "\teducation | 0.000145\n",
        "\n",
        "Sequence: Tomorrow I will have a very difficult exam\n",
        "Labels and their Scores:\n",
        "\tstudies | 0.923\n",
        "\tpain | 0.89\n",
        "\teducation | 0.766\n",
        "\tuniversity | 0.634\n",
        "\tsadness | 0.0558\n",
        "\thappy | 0.000531\n",
        "\tlove | 0.000432\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcyrzWGDsry-"
      },
      "source": [
        "## **Translation**\n",
        "\n",
        "The [translation](https://huggingface.co/tasks/translation) is the task of automatically converting one natural language into another.\n",
        "\n",
        "### Inference with Translation Pipeline\n",
        "You can use a specific model checkpoint in your pipeline with the `translation_xx_to_yy` pattern where `xx` is the source language code, and `yy` is the target language code. [Here](https://huggingface.co/models?pipeline_tag=translation&sort=downloads) you can find already pretrained models for translation tasks.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the Hugging Face Hub a described model for translating given sentences from English to German (`translation_en_to_de`). The model is Google's encoder-decoder Transformer that uses a text-to-text approach. Architecture: `~220M params`, `12-layers`, `768-hidden`, `3072 feed-forward hidden-state`, `12-heads`. Trained in the English text: the Colossal Clean Crawled Corpus (C4). Set `max_length=40`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaP1oHx4o0mU"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_tranlator\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "texts_to_translate = [\"I like data science! It is my hobby. I want to improve my knowledge in this sphere.\",\n",
        "                     \"Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\"]\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_tranlator =\n",
        "tranlated_texts =\n",
        "### END CODE HERE ###\n",
        "\n",
        "for original_text, tranlated_text in zip(texts_to_translate, tranlated_texts):\n",
        "    print(f\"Original text: {original_text}\\nTranslated text: {tranlated_text['translation_text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af3wjb7kr3Tu"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Original text: I like data science! It is my hobby. I want to improve my knowledge in this sphere.\n",
        "Translated text: Ich mag die Datenwissenschaft, sie ist mein Hobby und ich möchte meine Kenntnisse in diesem Bereich verbessern.\n",
        "\n",
        "Original text: Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\n",
        "Translated text: Transformers ist unsere Bibliothek zur Verarbeitung natürlicher Sprachen und unser Hub ist jetzt für alle Modelle des Machine Learning offen.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCV2ehIVBtkE"
      },
      "source": [
        "## **Text generation**\n",
        "[Text generation](https://huggingface.co/tasks/text-generation) is the task of generating texts to appear indistinguishable from human-written texts.\n",
        "\n",
        "### Inference with Text generation Pipeline\n",
        "You can use the Transformers library `text-generatio` pipeline from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads) to infer with Text Generation models. As input, the model takes an incomplete text and returns multiple outputs which can complete the text.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the Hub one of the most popular models for the text generation task, which is a GPT-based model. This model has architecture: `6-layers`, `768-hidden`, `12 heads`, and `82M params`. Use the found model to generate the given text using parameters `max_length=60`, `num_return_sequences=5` and `pad_token_id=50256`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GUznvHNBz8q"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_text_generator\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "base_text = \"Hello! I learn Deep Learning and I want to\"\n",
        "\n",
        "### START CODE HERE ### (≈ 2-6 lines of code)\n",
        "model_text_generator =\n",
        "genereted_texts =\n",
        "### END CODE HERE ###\n",
        "\n",
        "for gen_text in genereted_texts:\n",
        "    print(50 * \"-\")\n",
        "    print(gen_text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-IHDWcRr4nJ"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to teach you deep learning to create my own unique algorithm. My algorithm is based solely on finding the most perfect solution for your problem. In my post I give you examples for the algorithm. The best way to find and get started: Let me know what\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help you to learn how to do this!\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help you develop this knowledge over time!\n",
        "\n",
        "\n",
        "I want to thank my friend, Eric Dershowitz for allowing me to help me make this possible!\n",
        "Also, thank you for supporting me by supporting me and by helping us.\n",
        "\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help build a new learning pipeline that provides the best opportunities to learn and get started using deep learning algorithms. Deep learning is all about knowing the underlying principles of learning. So for the time being, I don't want to repeat this. Deep learning is\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help to help with this. It is in every student's life that as long as I can have fun learning the language, I will help you. For a class on what language is most like, there are lots of different ways to teach it.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnoHD6uJ-dpq"
      },
      "source": [
        "## **Text2Text Generation**\n",
        "\n",
        "It is the task for text-to-text generation using seq2seq models.\n",
        "\n",
        "### Inference with Text2Text Generation Pipeline\n",
        "\n",
        "Text2Text generation pipeline can be loaded using the following task identifier: `text2text-generation`. On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads), you can find an already pretrained model and use it in your pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Do paraphrase task using described model. The Google research team introduced the model in 2019 as a summarization model using self-supervised objective Gap Sentences Generation to train a transformer encoder-decoder. However, the model we are interested in was finetuned, especially for the paraphrasing task. Set `num_return_sequences=5`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG6-MLaRJnv_"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_paraphrase\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "text_to_paraphrase = \"Math helps a lot in everyday life, so you need to know it.\"\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_paraphrase =\n",
        "paraphrased_texts =\n",
        "### END CODE HERE ###\n",
        "\n",
        "for text in paraphrased_texts:\n",
        "    print(text['generated_text'] + '\\n' + 50*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra65oCSrr6Bi"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "You need to know that math helps a lot in everyday life.\n",
        "--------------------------------------------------\n",
        "You need to know that math helps in everyday life.\n",
        "--------------------------------------------------\n",
        "You need to know math because it helps a lot in everyday life.\n",
        "--------------------------------------------------\n",
        "It's important to know that math helps in everyday life.\n",
        "--------------------------------------------------\n",
        "You need to know about math because it helps a lot in everyday life.\n",
        "--------------------------------------------------\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401ltCDdBGKA"
      },
      "source": [
        "## **Conversational**\n",
        "\n",
        "The Conversational task mimics human conversation by recognizing speech and text, understanding intent, and deciphering different languages.\n",
        "\n",
        "### Inference with Conversational Pipeline\n",
        "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=conversational&sort=downloads), you can find a pretrained model to create a bot to communicate with you using the `conversational` pipeline. [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Conversation) you can find some examples of using this kind of pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Your task is to find the described model and use it in the pipeline to interact with a conversational bot. The model is based on a transformer architecture similar to BERT or Turing-NLG. The training was carried out on the Reddit dataset and fine-tuned with the [BST](https://arxiv.org/abs/2004.08449) dataset. The model architecture: `2-hidden` layers, `32-heads` and `400M` params.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H71ZqVEJgtV7"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_conv\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "### START CODE HERE ### (≈ 1-2 lines of code)\n",
        "model_conv =\n",
        "### END CODE HERE ###\n",
        "\n",
        "msgs_for_test_list = [\"Hello! Have you any movie recommendations?\", \"I also like comedies. What is your favorite comedy?\"]\n",
        "\n",
        "def test_bot(msgs_for_test):\n",
        "    for i, msg in enumerate(msgs_for_test_list):\n",
        "        conv_msg = msg\n",
        "        if i==0:\n",
        "            conv = Conversation(conv_msg)\n",
        "            model_conv([conv], pad_token_id=50256)\n",
        "            print(conv)\n",
        "        else:\n",
        "            conv.add_user_input(conv_msg)\n",
        "            print(model_conv([conv], pad_token_id=50256))\n",
        "\n",
        "# function to test the bot by yourself\n",
        "def test_bot_by_yourself():\n",
        "    conv_input = input()\n",
        "    customConv = Conversation(conv_input)\n",
        "    model_conv([customConv], pad_token_id=50256)\n",
        "\n",
        "    while conv_input != \"stop\": # the message you should input to STOP the bot\n",
        "        print(customConv)\n",
        "        conv_input = input()\n",
        "        customConv.add_user_input(conv_input)\n",
        "        model_conv([customConv], pad_token_id=50256)\n",
        "\n",
        "test_bot(msgs_for_test_list)\n",
        "#test_bot_by_yourself() # uncomment if you want to test the bot by yourself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHPGp5D1_Wsm"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Conversation id: 53d4d5f7-bd26-4f9a-929b-775f1ab4ab33\n",
        "user >> Hello! Have you any movie recommendations?\n",
        "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
        "\n",
        "Conversation id: 53d4d5f7-bd26-4f9a-929b-775f1ab4ab33\n",
        "user >> Hello! Have you any movie recommendations?\n",
        "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
        "user >> I also like comedies. What is your favorite comedy?\n",
        "bot >>  I don't really have a favourite comedy, but I do like a good drama.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6uDFGrV5gSZ"
      },
      "source": [
        "# **Audio Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx_UkBlUV8yl"
      },
      "source": [
        "## **Text to Speech**\n",
        "\n",
        "It is the [task](https://huggingface.co/tasks/text-to-speech) of converting text input into natural human speech.\n",
        "\n",
        "### Inference\n",
        "On the [Hub](https://huggingface.co/models?pipeline_tag=text-to-speech&sort=downloads), you can find over 160 TTS models that you can use right away by trying out the widgets directly in the browser or calling the models as a service using the Accelerated Inference API.\n",
        "\n",
        "We will use [gradio](https://gradio.app/docs/#load) library to handle the Inference directly on the Google Colab notebook. Gradio interface can be created by constructing an `Interface` object with `Interface.load()` with the path to a Hugging Face model repo.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model and use it to generate speech by inputting text. It is Facebook's non-autoregressive Transformer-based model, trained on The [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcECsZnsFUA"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_tts_de\n",
        "\n",
        "### START CODE HERE ### (≈ 1-4 lines of code)\n",
        "iface_tts =                                 # construct an Interface\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7xPpx7UsJJt"
      },
      "outputs": [],
      "source": [
        "# launch the UI for the interface\n",
        "iface_tts.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRYGiBzmkL2y"
      },
      "source": [
        "## **Automatic Speech Recognition**\n",
        "\n",
        "[ASR](https://huggingface.co/tasks/automatic-speech-recognition) is the task of transcribing given audio to text.\n",
        "### Inference\n",
        "\n",
        "On the [Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads), you can find over 1800 ASR models that you can use right away by trying out the widgets directly in the browser or calling the models as a service using the Accelerated Inference API.\n",
        "\n",
        "We will use [gradio](https://gradio.app/docs/#interface) library to handle the Inference directly on the Google Colab notebook. Gradio interface can be created by constructing an `Interface` object with `Interface.from_pipeline()` with a Transformers Pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model and try to use the model to recognize your speech converting into text. It is Facebook's ASR model, trained on the [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) dataset. Model architecture: `512-dim` encoder/decoder input, `12-hidden-layers`, `2-conv-layers`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx93UPNTeUIL"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_asr_de\n",
        "\n",
        "### START CODE HERE ### (≈ 2-5 lines of code)\n",
        "model_asr =                                 # create pipeline\n",
        "iface_asr =                                 # construct an Interface\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab_maHUPsOuM"
      },
      "outputs": [],
      "source": [
        "# launch the UI for the interface\n",
        "iface_asr.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-zWI77i5lZj"
      },
      "source": [
        "# **Computer Vision Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxQ1dwsVkhXv"
      },
      "source": [
        "## **Image Classification**\n",
        "\n",
        "It is the [task](https://huggingface.co/tasks/image-classification) of assigning a label or class for a given image. Models take an image as input and return a prediction about which category the image belongs to.\n",
        "\n",
        "### Inference\n",
        "\n",
        "You can use the `image-classification` pipeline to infer with image classification models, which you can find on the [Hub](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads). When you call the pipeline, you need to specify an image path or link. We will use [gradio](https://gradio.app/docs/#interface) library to make it easier to handle the Inference directly on the Google Colab notebook. We will use `Interface.from_pipeline()` with a Transformers Pipeline to create the Gradio interface.\n",
        "\n",
        "**Excercise:**\n",
        ">Try to do image classification using the described model pipeline. It is a Microsoft transformer encoder BERT-like model. This model is pretrained and fine-tuned on the ImageNet-22k dataset, consisting of 14 million images and 21k classes at 224x224 pixels. The model has a `768-hidden` size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VN3WuqhbkgsT"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_img_cls_de\n",
        "\n",
        "### START CODE HERE ### (≈ 2-5 lines of code)\n",
        "model_img_cls =\n",
        "iface_img_cls =\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gljDLHIOsg7x"
      },
      "outputs": [],
      "source": [
        "# launch the UI for the interface\n",
        "iface_img_cls.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHWBfF6JsgaD"
      },
      "source": [
        "## **Image Segmentation**\n",
        "\n",
        "It is the [task](https://huggingface.co/tasks/image-segmentation) of dividing an image into segments by creating a pixel-wise mask for each object in the picture. This task has multiple variants, such as instance segmentation, panoptic segmentation, and semantic segmentation.\n",
        "\n",
        "### Inference\n",
        "On the [Hub](https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads), you can find an already pretrained model and infer it using the `image-segmentation` pipeline. [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.ImageSegmentationPipeline) you also can discover segmentation pipeline parameters. For doing image segmentation, you need to install [timm](https://github.com/rwightman/pytorch-image-models).\n",
        "\n",
        "We will not use the gradio library because it doesn't support the `image-segmentation` pipeline type.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model to do image segmentation. This model consists of a convolutional (50-layers) backbone followed by an encoder-decoder transformer trained on [COCO 2017](https://cocodataset.org/#download). Set `threshold=0.95` and `mask_threshold=0.7`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owctmkWnzpeI"
      },
      "outputs": [],
      "source": [
        "!wget https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/fruits.jpg -O fruits.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg0_dabYtZI4"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] model_img_segm_de\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "model_img_segm =\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRGu1If6sx8D"
      },
      "outputs": [],
      "source": [
        "def print_img_and_masks(model_output, original_img_path):\n",
        "\n",
        "    img_masks_list = [i['mask'] for i in model_output][:9] #select top 9 results\n",
        "    masks_labels = [i['label'] for i in model_output][:9] #select top 9 results\n",
        "\n",
        "    original_img = Image.open(r\"fruits.jpg\")\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.title('Original Image', fontsize=18)\n",
        "    plt.imshow(original_img.resize((600, 400)));\n",
        "\n",
        "    # print segmentation masks\n",
        "    _, axs = plt.subplots(3, 3, figsize=(20, 20)) # 3x3 grid\n",
        "    axs = axs.flatten()\n",
        "    for img, ax, l in zip(img_masks_list, axs, masks_labels):\n",
        "        ax.set_title(f'{l}', fontsize=18)\n",
        "        ax.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "img_masks_out = model_img_segm(\"fruits.jpg\", threshold=0.95, mask_threshold=0.7)\n",
        "print_img_and_masks(img_masks_out, 'fruits.jpg')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}