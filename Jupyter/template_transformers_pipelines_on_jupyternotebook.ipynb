{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/Jupyter/template_transformers_pipelines_on_jupyternotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvziXoe21z_G_gh"
      },
      "source": [
        "# **Hugging Face Transformers Library**\n",
        "[Hugging Face](https://huggingface.co) is a large open-source community. Their [Hub](https://huggingface.co/models) has a very large number of pre-trained deep learning models, mainly aimed at NLP, using Transformers. In this lab, you will learn how to use pre-trained Hugging Face models to solve various NLP, CV, and Audio tasks.\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- How to use `pipeline()` from the [Models Hub](https://huggingface.co/models) for inference on a variety of NLP, CV, and Audio tasks\n",
        "- How to use [Gradio](https://gradio.app/docs/) library for easy creating user interface and handling the pipeline inference directly on Google Colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQuWgaHU13zo"
      },
      "source": [
        "# **Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbi2PMjy109L",
        "outputId": "3acc8a57-aac3-47fc-a469-2ad894c290bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.17.0\n",
            "  Using cached transformers-4.17.0-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (2023.10.3)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (2.32.2)\n",
            "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.17.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.17.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.17.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.17.0) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.17.0) (2024.7.4)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses->transformers==4.17.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses->transformers==4.17.0) (1.4.2)\n",
            "Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.43.3\n",
            "    Uninstalling transformers-4.43.3:\n",
            "      Successfully uninstalled transformers-4.43.3\n",
            "Successfully installed transformers-4.17.0\n",
            "Requirement already satisfied: sentence_transformers==1.2.1 in /opt/anaconda3/lib/python3.12/site-packages (1.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (4.17.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (2.3.1)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (0.18.1)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (1.4.2)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (1.13.1)\n",
            "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers==1.2.1) (0.2.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence_transformers==1.2.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence_transformers==1.2.1) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence_transformers==1.2.1) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence_transformers==1.2.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence_transformers==1.2.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->sentence_transformers==1.2.1) (2024.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (0.24.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (2023.10.3)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (2.32.2)\n",
            "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (0.19.1)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk->sentence_transformers==1.2.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk->sentence_transformers==1.2.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers==1.2.1) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->sentence_transformers==1.2.1) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->sentence_transformers==1.2.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers==1.2.1) (2024.7.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.6.0->sentence_transformers==1.2.1) (1.3.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-scatter in /opt/anaconda3/lib/python3.12/site-packages (2.1.2)\n",
            "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.12/site-packages (4.39.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (4.2.0)\n",
            "Requirement already satisfied: fastapi in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.111.1)\n",
            "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.1.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.24.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.10.6)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.5.3)\n",
            "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.5.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.30.3)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.1.1->gradio) (2024.3.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.1.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.21.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
            "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.0)\n",
            "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.7)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from timm) (2.3.1)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.18.1)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.24.2)\n",
            "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (23.2)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "!pip install transformers==4.17.0\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from transformers import Conversation\n",
        "\n",
        "!pip install sentence_transformers==1.2.1\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "\n",
        "!pip install gradio\n",
        "import gradio as gr\n",
        "\n",
        "!pip install sentencepiece\n",
        "\n",
        "!pip install timm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpLrb0yup3v4"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] set_seed\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG05rDRWtalu",
        "outputId": "dc38f00f-bb64-4068-84d5-51394c44f613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets==8.0.0b in /opt/anaconda3/lib/python3.12/site-packages (8.0.0b0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets==8.0.0b) (6.28.0)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets==8.0.0b) (8.25.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets==8.0.0b) (5.14.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets==8.0.0b) (5.9.2)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0b0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets==8.0.0b) (4.0.11)\n",
            "Requirement already satisfied: jupyterlab-widgets~=2.0b0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets==8.0.0b) (2.0.0b1)\n",
            "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (0.1.3)\n",
            "Requirement already satisfied: comm>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (0.2.1)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (1.6.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (1.6.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (23.2)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (5.9.0)\n",
            "Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (25.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.0b) (6.4.1)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.0.0b) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.0.0b) (0.18.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.0.0b) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.0.0b) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.0.0b) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.0.0b) (4.8.0)\n",
            "Requirement already satisfied: fastjsonschema in /opt/anaconda3/lib/python3.12/site-packages (from nbformat>=4.2.0->ipywidgets==8.0.0b) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /opt/anaconda3/lib/python3.12/site-packages (from nbformat>=4.2.0->ipywidgets==8.0.0b) (4.19.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.0.0b) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==8.0.0b) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==8.0.0b) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==8.0.0b) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==8.0.0b) (0.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==8.0.0b) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets==8.0.0b) (3.10.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.0.0b) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.0.0b) (0.2.5)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.0b) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.0b) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.0b) (0.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==8.0.0b) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets==8.0.0b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83w7BY3B1-HT"
      },
      "source": [
        "**List of tasks that we will cover:**\n",
        "\n",
        "- **NLP**:\n",
        "    - Masked language modeling\n",
        "    - Question Answering\n",
        "    - Table Question Answering\n",
        "    - Summarization\n",
        "    - Sentence Similarity\n",
        "    - Text Classification\n",
        "    - Token Classification\n",
        "    - Zero-Shot Classification\n",
        "    - Translation\n",
        "    - Text generation\n",
        "    - Text2Text Generation\n",
        "    - Conversational\n",
        "- **Audio**:\n",
        "    - Text-to-Speech\n",
        "    - Automatic Speech Recognition\n",
        "\n",
        "- **Computer Vision**:\n",
        "    - Image Classification\n",
        "    - Image Segmentation\n",
        "\n",
        "The easiest way to use a pre-trained Transformer model on a given task is to use a `pipeline()`.\n",
        "\n",
        "[Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) helps to perform all pre-processing and post-processing steps on your input data:\n",
        "1. **Tokenization:** Split the initial input into multiple sub-entities with properties (tokens).\n",
        "2. **Inference:** Maps every token into a more meaningful representation.\n",
        "3. **Decoding:** Use the above representation to generate and/or extract the final output for the given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhEId9ru5WJ2"
      },
      "source": [
        "# **NLP Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fg2fNqD2FHC"
      },
      "source": [
        "## **Masked language modeling**\n",
        "\n",
        "[Masked language modeling](https://huggingface.co/tasks/fill-mask) (MLM) is the task of masking tokens (e.g., replaced by `[MASK]`) in a sequence and predicting which words should replace this masked word with an appropriate token.\n",
        "\n",
        "### Inference with Fill-Mask Pipeline\n",
        "\n",
        "You can use the Transformers library `fill-mask` pipeline to infer with masked language models, which you can find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads). There are many pretrained models here that can be used to solve the MLM task.\n",
        "\n",
        "**Excercise:**\n",
        ">For the proposed `masked_sequences`, we suggest finding you among the models on the [hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads) a pretrained base BERT (uncased) model, which is already pretrained on a large corpus of English data, and use it in the pipeline for inference. Your model should return `top_k=2` predictions for each sentence in `unmasked_sequences`. All the parameters that the `fill-mask` pipeline uses are described [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.FillMaskPipeline).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BANDUtPF150z",
        "outputId": "5a99e181-eefe-4bea-d315-308c3ce5b1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Masked sentence 1: I love animals very much, that's why I work as a [MASK].\n",
            "Suggested unmasked sentences:\n",
            "\ti love animals very much, that's why i work as a vet. \t | Masked word: vet\n",
            "\ti love animals very much, that's why i work as a farmer. \t | Masked word: farmer\n",
            "\n",
            "Masked sentence 2: I live several years in Russia, but now I live in [MASK].\n",
            "Suggested unmasked sentences:\n",
            "\ti live several years in russia, but now i live in france. \t | Masked word: france\n",
            "\ti live several years in russia, but now i live in ukraine. \t | Masked word: ukraine\n",
            "\n",
            "Masked sentence 3: I love to play [MASK], so my bought me a ball.\n",
            "Suggested unmasked sentences:\n",
            "\ti love to play football, so my bought me a ball. \t | Masked word: football\n",
            "\ti love to play baseball, so my bought me a ball. \t | Masked word: baseball\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_mlm\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "masked_sequences = [\"I love animals very much, that's why I work as a [MASK].\",\n",
        "        \"I live several years in Russia, but now I live in [MASK].\",\n",
        "        \"I love to play [MASK], so my bought me a ball.\"\n",
        "        ]\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_mlm = pipeline(\"fill-mask\", model=\"bert-base-uncased\")                              # specify: task and model\n",
        "unmasked_sequences = model_mlm(masked_sequences, top_k=2)                       # use the pipeline with parameter top_k=2\n",
        "### END CODE HERE ###\n",
        "\n",
        "for text_i in range(len(masked_sequences)):\n",
        "    print(f\"\\nMasked sentence {text_i + 1}: {masked_sequences[text_i]}\\nSuggested unmasked sentences:\")\n",
        "    for output_i in unmasked_sequences[text_i]:\n",
        "        print(f\"\\t{output_i['sequence']} \\t | Masked word: {output_i['token_str']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpClPQK1qi5j"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Masked sentence 1: I love animals very much, that's why I work as a [MASK].\n",
        "Suggested unmasked sentences:\n",
        "\ti love animals very much, that's why i work as a vet. \t | Masked word: vet\n",
        "\ti love animals very much, that's why i work as a farmer.   | Masked word: farmer\n",
        "\n",
        "Masked sentence 2: I live several years in Russia, but now I live in [MASK].\n",
        "Suggested unmasked sentences:\n",
        "\ti live several years in russia, but now i live in france. \t | Masked word: france\n",
        "\ti live several years in russia, but now i live in ukraine. \t| Masked word: ukraine\n",
        "\n",
        "Masked sentence 3: I love to play [MASK], so my bought me a ball.\n",
        "Suggested unmasked sentences:\n",
        "\ti love to play football, so my bought me a ball. \t | Masked word: football\n",
        "\ti love to play baseball, so my bought me a ball. \t | Masked word: baseball\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV9_ctyt9MkG"
      },
      "source": [
        "## **Question Answering**\n",
        "[Question Answering](https://huggingface.co/tasks/question-answering) (QA) is a type of language task that produces answers based on a given document/text or generates answers without context.\n",
        "\n",
        "### Inference with Question Answering Pipeline\n",
        "You can find on the Hugging Face Hub a pretrained model among a wide variety of models for [QA tasks](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads). [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.QuestionAnsweringPipeline) is a description of the parameters for the `question-answering` pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads) an original pretrained model to produce answers based on a given text. This model makes up the composition of transformer encoders, and it is an autoencoder. Architecture is: `24-layer`, `1024-hidden`, `16-heads`, `340M params`. This model was trained on lower-cased English text using the technique that masks all subwords corresponding to a word at once and was fine-tuned on the Stanford Question Answering Dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJGsKjaU2N_Z",
        "outputId": "3f7ccb28-fc41-41e8-b6ad-c9224cd361b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which fish has long fins?\n",
            "Answer: Finny\n",
            "Question: Which fish has great big eyes?\n",
            "Answer: Igor\n",
            "Question: Which fish has a big tail?\n",
            "Answer: Tayla\n",
            "Question: Which of these fish would you most like to have as a pet?\n",
            "Answer: Igor\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_qa\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "### START CODE HERE ### (≈ 1-2 lines of code)\n",
        "model_qa = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "context = r\"\"\"\n",
        "These fish have names.\n",
        "This is Finny.\n",
        "Finny has beautiful long fins that help her swim fast.\n",
        "This is Tayla.\n",
        "Tayla’s big tail moves from side to side and it helps her to go this way or that way.\n",
        "This is Igor.\n",
        "Igor has great big eyes. Igor’s great big eyes help him to see where he is going and they also help him to see\n",
        "big scary fish!\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"Which fish has long fins?\",\n",
        "    \"Which fish has great big eyes?\",\n",
        "    \"Which fish has a big tail?\",\n",
        "    \"Which of these fish would you most like to have as a pet?\"\n",
        "    ]\n",
        "\n",
        "for question in questions:\n",
        "    answ = model_qa(question=question, context=context)\n",
        "    print(f\"Question: {question}\\nAnswer: {answ['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1QjRCfXrbKL"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Question: Which fish has long fins?\n",
        "Answer: Finny\n",
        "Question: Which fish has great big eyes?\n",
        "Answer: Igor\n",
        "Question: Which fish has a big tail?\n",
        "Answer: Tayla\n",
        "Question: Which of these fish would you most like to have as a pet?\n",
        "Answer: Igor\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2oAMFt-_tFu"
      },
      "source": [
        "## **Table Question Answering**\n",
        "\n",
        "The task aims to find semantic question answering based on structured tabular data.\n",
        "\n",
        "### Inference with Table Question Answering Pipeline\n",
        "\n",
        "For solving the TQA task, you can find a model on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads) and use it for inference with the `table-question-answering` pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads) a pretrained model and use it to find answers based on the given table. It is a BERT-based model from Google with architecture: `24-layer`, `1024-hidden`, `16-heads`, `340M params`. The model is fine-tuned on WikiTable Questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogfrKfRG_2F6",
        "outputId": "5bb08564-d268-4437-8077-69b6ccc284d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/tapas/tokenization_tapas.py:2690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  text = normalize_for_match(row[col_index].text)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/tapas/tokenization_tapas.py:1501: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  cell = row[col_index]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: how did the country donate the most?, \n",
            "Answer: France\n",
            "------------------------------\n",
            "Question: how much humanitarian aid poland gave?, \n",
            "Answer: 50 tons\n",
            "------------------------------\n",
            "Question: who donated 1.5 billion?, \n",
            "Answer: USA\n",
            "------------------------------\n",
            "Question: when did france donate the money?, \n",
            "Answer: 10 march 2022\n",
            "------------------------------\n",
            "Question: whoes donated amount is lowest among all?, \n",
            "Answer: Poland\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_tqa_de\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "data = {\n",
        "    \"country\": [\"USA\", \"France\", \"Poland\"],\n",
        "    \"donated money\": [\"1.5 billion\", \"2 billion\", \"0.5 billion\"],\n",
        "    \"humanitarian aid\": [\"30 tons\", \"2 tons\", \"50 tons\"],\n",
        "    \"date\": [\"7 march 2022\", \"10 march 2022\", \"28 march 2022\"],\n",
        "}\n",
        "\n",
        "questions = [\"how did the country donate the most?\",\n",
        "             \"how much humanitarian aid poland gave?\",\n",
        "             \"who donated 1.5 billion?\",\n",
        "             \"when did france donate the money?\",\n",
        "             \"whoes donated amount is lowest among all?\"]\n",
        "\n",
        "\n",
        "table = pd.DataFrame.from_dict(data)\n",
        "\n",
        "### START CODE HERE ### (≈ 1-2 lines of code)\n",
        "model_tqa = pipeline(\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "for question in questions:\n",
        "    output = model_tqa(table=table, query=question)\n",
        "    print(f\"Question: {question}, \\nAnswer: {output['cells'][0]}\")\n",
        "    print(30*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSGzJ0xerp0K"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Question: how did the country donate the most?,\n",
        "Answer: France\n",
        "------------------------------\n",
        "Question: how much humanitarian aid poland gave?,\n",
        "Answer: 50 tons\n",
        "------------------------------\n",
        "Question: who donated 1.5 billion?,\n",
        "Answer: USA\n",
        "------------------------------\n",
        "Question: when did france donate the money?,\n",
        "Answer: 10 march 2022\n",
        "------------------------------\n",
        "Question: whoes donated amount is lowest among all?,\n",
        "Answer: Poland\n",
        "------------------------------\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX4osP1xHLNa"
      },
      "source": [
        "## **Summarization**\n",
        "\n",
        "[Summarization](https://huggingface.co/tasks/summarization) is the task of producing a shorter version of one or several docs preserving the meaning of the text.\n",
        "\n",
        "### Inference with Summarization Pipeline\n",
        "\n",
        "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), you can find the Transformers library `summarization` pipeline to infer with models, which are pretrained for summarization tasks.\n",
        "\n",
        "**Excercise:**\n",
        ">Using a model description, find it and use it in the pipeline summarization for the given text. It is a transformer encoder-decoder that combines Google's BERT (encoder) and OpenAI's GPT (decoder). The model contains `1024-hidden` layers and `406M params` and has been fine-tuned using CNN (news summarization dataset).\n",
        "\n",
        "Generate summary using parameters:\n",
        "\n",
        " - `min_length=50`\n",
        " - `max_length=120`\n",
        " - `length_penalty=2.0`\n",
        " - `num_beams=4`\n",
        " - `early_stopping=True`\n",
        "\n",
        "[This article](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb) will help you understand a bit more about these parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7EE0WSDADZo",
        "outputId": "6a55195a-6cab-49a4-a2cb-8b4f5f06bb03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boris Akunin: 'I was always, let's say, suspicious of him' 'The first thing that Putin did, he attacked the independent media' 'He monopolised propaganda and used TV channels to zombify people' 'It's absolutely not surprising that lots of Russians believe all the lies'\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_summarizer\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "text = \"\"\"Boris Akunin: Initially, the reason was that I have no trust for KGB people at all.\n",
        "So someone who comes from the KGB, from my point of view, should not become the president of Russia.\n",
        "So I never voted for Putin. I was always, let’s say, suspicious of him. And my suspicions proved to be true very fast,\n",
        "because the first thing that Putin did, he attacked the independent media. I remember quite well signing letters in support\n",
        "of independent media, and well, we lost.\n",
        "\n",
        "And then that man was very methodically killing all the branches of democracy. He started with television,\n",
        "he monopolised propaganda and used TV channels to zombify people, and this has been going on for more than 20 years.\n",
        "\n",
        "So it’s absolutely not surprising that lots of Russians believe all the lies that are being told about Ukraine and about\n",
        "the world in general, not surprising at all.\n",
        "\n",
        "Then he destroyed the independent courts and judicial system, he destroyed the parliamentary system, then he corrupted presidential\n",
        "elections. There hasn’t been any free presidential election during Putin’s times.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ### (≈ 2-8 lines of code)\n",
        "model_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", min_length=50,\n",
        "    max_length=120,\n",
        "    length_penalty=2.0,\n",
        "    num_beams=4,\n",
        "    early_stopping=True) # \"facebook/bart-large-cnn\")\n",
        "summarized_text = model_summarizer(text)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(summarized_text[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vib5QdGUrwBF"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Akunin: 'I was always, let's say, suspicious of him' 'The first thing that Putin did, he attacked the independent media' 'He monopolised propaganda and used TV channels to zombify people' 'It's absolutely not surprising that lots of Russians believe all the lies'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c87ZWHtOKT3p"
      },
      "source": [
        "## **Sentence Similarity**\n",
        "\n",
        "[Sentence Similarity](https://huggingface.co/tasks/sentence-similarity) is the task of determining how similar the semantic similarity between two texts.\n",
        "\n",
        "### The Sentence Transformers library\n",
        "The [Sentence Transformers](https://www.sbert.net) library helps to calculate embeddings of textual documents. An embedding is just a vector that captures the semantic information. They help find how similar texts are.\n",
        "\n",
        "**Excercise:**\n",
        ">Your task is to find a [sentence transformer](https://www.sbert.net/docs/pretrained_models.html) on the [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers&sort=downloads) to calculate the similarity between given sentences. Transformer model description: it is an all-round tuned model with one of the highest encoding speeds (sentences/ sec ~ 14200). Model architecture: `max-seq-len 256`, `384-hidden`, `6-layer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAEgR0CFHs5N",
        "outputId": "52f306b7-0b66-44e5-d264-9129972e4940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I love data science! <--VS--> To be honest, I don't really like data science. | Score: 0.805\n",
            "\n",
            "I love data science! <--VS--> Data science is a very powerful thing. | Score: 0.741\n",
            "\n",
            "To be honest, I don't really like data science. <--VS--> Data science is a very powerful thing. | Score: 0.739\n",
            "\n",
            "I love data science! <--VS--> I love programming in python. | Score: 0.482\n",
            "\n",
            "To be honest, I don't really like data science. <--VS--> I love programming in python. | Score: 0.417\n",
            "\n",
            "Data science is a very powerful thing. <--VS--> I love programming in python. | Score: 0.382\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_sentence_similarity\n",
        "#!pip install --upgrade sentence_transformers\n",
        "set_seed(42)\n",
        "\n",
        "sentences = ['I love data science!',\n",
        "             \"To be honest, I don't really like data science.\",\n",
        "             \"Data science is a very powerful thing.\",\n",
        "             \"I love programming in python.\"]\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "model_sentence_similarity = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model_sentence_similarity.encode(sentences)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# compute cosine similarities\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "# find the pairs with the highest cosine similarity scores\n",
        "seq_pairs = []\n",
        "for i in range(len(cosine_scores) - 1):\n",
        "    for j in range(i + 1, len(cosine_scores)):\n",
        "        seq_pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "# sort scores\n",
        "seq_pairs = sorted(seq_pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for seqs_pair in seq_pairs:\n",
        "    i, j = seqs_pair['index']\n",
        "    print(\"{} <--VS--> {} | Score: {:.3f}\\n\".format(sentences[i], sentences[j], seqs_pair['score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcor51mKrxeU"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "I love data science! <--VS--> To be honest, I don't really like data science. | Score: 0.805\n",
        "\n",
        "I love data science! <--VS--> Data science is a very powerful thing. | Score: 0.741\n",
        "\n",
        "To be honest, I don't really like data science. <--VS--> Data science is a very powerful thing. | Score: 0.739\n",
        "\n",
        "I love data science! <--VS--> I love programming in python. | Score: 0.482\n",
        "\n",
        "To be honest, I don't really like data science. <--VS--> I love programming in python. | Score: 0.417\n",
        "\n",
        "Data science is a very powerful thing. <--VS--> I love programming in python. | Score: 0.382\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etU2QJgkRvAY"
      },
      "source": [
        "## **Text Classification**\n",
        "\n",
        "[Text classification](https://huggingface.co/tasks/text-classification) is the process of classifying documents into predefined categories (labels) based on their content. Text classification models use in sentiment analysis, natural language inference, and assessing grammatical correctness cases.\n",
        "\n",
        "### Inference with Text Classification Pipeline\n",
        "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-classification), you can find the Transformers library `text-classification` pipeline to infer with Text Classification models.\n",
        "\n",
        "**Excercise:**\n",
        ">Find described model on the Hub and classify emotions for the given English text data. This model is a distilled version of the same model name and, on average, is twice as fast as the base version. It follows the same training procedure as DistilBERT. The model has `6-layers`, `768-hidden`, and `12-heads`, `82M params`.\n",
        "\n",
        "As one of the pipeline parameters, use `return_all_scores=False` to return one max scored emotion per sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPml7FgCO6HC",
        "outputId": "ceb9962e-65a1-4c86-b9bd-e5be02c3b255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All human souls are connected, which explains why it hurts so much to detach from others. | Emoji: 😭(sadness) | Score: 0.807\n",
            "\n",
            "The present moment is filled with joy and happiness. If you are attentive, you will see it. | Emoji: 😁(joy) | Score: 0.925\n",
            "\n",
            "Learn to use the criticism as fuel and you will never run out of energy. | Emoji: 😐(neutral) | Score: 0.758\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_text_classifier_de\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# emotions model can predict\n",
        "emoji_dict = {\n",
        "    'sadness': \"\\U0001F62D\",\n",
        "    'anger': \"\\U0001F621\",\n",
        "    'disgust': \"\\U0001F922\",\n",
        "    'fear': \"\\U0001F628\",\n",
        "    'joy': \"\\U0001F601\",\n",
        "    'neutral': \"\\U0001F610\",\n",
        "    'surprise': \"\\U0001F62F\"\n",
        "}\n",
        "\n",
        "texts = [\"All human souls are connected, which explains why it hurts so much to detach from others.\",\n",
        "        \"The present moment is filled with joy and happiness. If you are attentive, you will see it.\",\n",
        "        \"Learn to use the criticism as fuel and you will never run out of energy.\"]\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_text_classifier = pipeline(\"sentiment-analysis\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=False)\n",
        "appropriate_emotions = model_text_classifier(texts)\n",
        "### END CODE HERE ###\n",
        "\n",
        "for text, emotion in zip(texts, appropriate_emotions):\n",
        "    print(\"{} | Emoji: {}({}) | Score: {:.3f}\\n\".format(text, emoji_dict[emotion['label']],emotion['label'], emotion['score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1aM9VP6r0Fs"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "All human souls are connected, which explains why it hurts so much to detach from others. | Emoji: 😭(sadness) | Score: 0.807\n",
        "\n",
        "The present moment is filled with joy and happiness. If you are attentive, you will see it. | Emoji: 😁(joy) | Score: 0.925\n",
        "\n",
        "Learn to use the criticism as fuel and you will never run out of energy. | Emoji: 😐(neutral) | Score: 0.758\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E09mWONKcCi5"
      },
      "source": [
        "## **Token Classification**\n",
        "\n",
        "It is one of the NLP tasks in which a label is assigned to some tokens in a text. [Token Classification](https://huggingface.co/tasks/token-classification) models use in Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models are aimed to identify specific entities in a text (dates, individuals, and places). PoS tagging uses in identifying verbs, nouns, and punctuation marks.\n",
        "\n",
        "Our task here will be to identify the words' entity for the given text so that we will solve the NER problem.\n",
        "\n",
        "### Inference with Token Classification Pipeline\n",
        "\n",
        "Here on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads), you can find pretrained models that can solve the Named Entity Recognition problem and use them in `token-classification` or `ner` pipelines.\n",
        "\n",
        "**Excercise:**\n",
        ">You should do NER for the given sentence and find a described model on the [Hub](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads) to solve the task. The model is a fine-tuned famous bidirectional transformer for NER, trained on upper-cased English text. Model architecture: `768-hidden`, `12-heads`, and `6-layers`.\n",
        "\n",
        "Each token will be classified by the model as one of the following classes:\n",
        "\n",
        "| Abbreviation | Description             |\n",
        "|-------|--------------------------------|\n",
        "| O     | Outside of a named entity      |\n",
        "| B-MIS | Beginning of a miscellaneous entity right after another miscellaneous entity                     |\n",
        "| I-MIS | Miscellaneous entity           |\n",
        "| B-PER | Beginning of a person’s name right after another person’s name |\n",
        "| I-PER | Person’s name                  |\n",
        "| B-ORG | Beginning of an organization right after another organization |\n",
        "| I-ORG | Organization |\n",
        "| B-LOC | Beginning of a location right after another location |\n",
        "| I-LOC | Location |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L3j-pJscHYN",
        "outputId": "08a5b138-c74d-4b0b-f3be-89045b45d56c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity</th>\n",
              "      <th>score</th>\n",
              "      <th>index</th>\n",
              "      <th>word</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B-PER</td>\n",
              "      <td>0.999165</td>\n",
              "      <td>1</td>\n",
              "      <td>Mark</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B-PER</td>\n",
              "      <td>0.997683</td>\n",
              "      <td>3</td>\n",
              "      <td>Mary</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B-LOC</td>\n",
              "      <td>0.999792</td>\n",
              "      <td>6</td>\n",
              "      <td>Ukraine</td>\n",
              "      <td>22</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B-LOC</td>\n",
              "      <td>0.999481</td>\n",
              "      <td>20</td>\n",
              "      <td>K</td>\n",
              "      <td>99</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I-LOC</td>\n",
              "      <td>0.998003</td>\n",
              "      <td>21</td>\n",
              "      <td>##har</td>\n",
              "      <td>100</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I-LOC</td>\n",
              "      <td>0.996411</td>\n",
              "      <td>22</td>\n",
              "      <td>##ki</td>\n",
              "      <td>103</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I-LOC</td>\n",
              "      <td>0.994031</td>\n",
              "      <td>23</td>\n",
              "      <td>##v</td>\n",
              "      <td>105</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>B-LOC</td>\n",
              "      <td>0.999337</td>\n",
              "      <td>25</td>\n",
              "      <td>Ch</td>\n",
              "      <td>111</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I-LOC</td>\n",
              "      <td>0.938038</td>\n",
              "      <td>26</td>\n",
              "      <td>##ern</td>\n",
              "      <td>113</td>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I-LOC</td>\n",
              "      <td>0.998129</td>\n",
              "      <td>27</td>\n",
              "      <td>##ih</td>\n",
              "      <td>116</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I-LOC</td>\n",
              "      <td>0.995637</td>\n",
              "      <td>28</td>\n",
              "      <td>##iv</td>\n",
              "      <td>118</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   entity     score  index     word  start  end\n",
              "0   B-PER  0.999165      1     Mark      0    4\n",
              "1   B-PER  0.997683      3     Mary      9   13\n",
              "2   B-LOC  0.999792      6  Ukraine     22   29\n",
              "3   B-LOC  0.999481     20        K     99  100\n",
              "4   I-LOC  0.998003     21    ##har    100  103\n",
              "5   I-LOC  0.996411     22     ##ki    103  105\n",
              "6   I-LOC  0.994031     23      ##v    105  106\n",
              "7   B-LOC  0.999337     25       Ch    111  113\n",
              "8   I-LOC  0.938038     26    ##ern    113  116\n",
              "9   I-LOC  0.998129     27     ##ih    116  118\n",
              "10  I-LOC  0.995637     28     ##iv    118  120"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_token_cls\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "sequence = \"Mark and Mary live in Ukraine. They love to travel around their country. Their favorite cities are Kharkiv and Chernihiv.\"\n",
        "\n",
        "### START CODE HERE ### (≈ 1-3 lines of code)\n",
        "model_token_cls = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "ner_result = model_token_cls(sequence)\n",
        "ner_result_df = pd.DataFrame(ner_result)\n",
        "ner_result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK5E4STJr1Qb"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <td>entity</td>\n",
        "        <td>score</td>\n",
        "        <td>index</td>\n",
        "        <td>word</td>\n",
        "        <td>start</td>\n",
        "        <td>end</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>0</td>\n",
        "        <td>B-PER</td>\n",
        "        <td>0.999165</td>\n",
        "        <td>1</td>\n",
        "        <td>Mark</td>\n",
        "        <td>0</td>\n",
        "        <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>1</td>\n",
        "        <td>B-PER</td>\n",
        "        <td>0.997683</td>\n",
        "        <td>3</td>\n",
        "        <td>Mary</td>\n",
        "        <td>9</td>\n",
        "        <td>13</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>2</td>\n",
        "        <td>B-LOC</td>\n",
        "        <td>0.999792</td>\n",
        "        <td>6</td>\n",
        "        <td>Ukraine</td>\n",
        "        <td>22</td>\n",
        "        <td>29</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>3</td>\n",
        "        <td>B-LOC</td>\n",
        "        <td>0.999481</td>\n",
        "        <td>20</td>\n",
        "        <td>K</td>\n",
        "        <td>99</td>\n",
        "        <td>100</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>4</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.998003</td>\n",
        "        <td>21</td>\n",
        "        <td>\\##har</td>\n",
        "        <td>100</td>\n",
        "        <td>103</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>5</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.996411</td>\n",
        "        <td>22</td>\n",
        "        <td>\\##ki</td>\n",
        "        <td>103</td>\n",
        "        <td>105</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>6</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.994031</td>\n",
        "        <td>23</td>\n",
        "        <td>\\##v</td>\n",
        "        <td>105</td>\n",
        "        <td>106</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>7</td>\n",
        "        <td>B-LOC</td>\n",
        "        <td>0.999337</td>\n",
        "        <td>25</td>\n",
        "        <td>Ch</td>\n",
        "        <td>111</td>\n",
        "        <td>113</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>8</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.938038</td>\n",
        "        <td>26</td>\n",
        "        <td>\\##ern</td>\n",
        "        <td>113</td>\n",
        "        <td>116</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>9</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.998129</td>\n",
        "        <td>27</td>\n",
        "        <td>\\##ih</td>\n",
        "        <td>116</td>\n",
        "        <td>118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>10</td>\n",
        "        <td>I-LOC</td>\n",
        "        <td>0.995637</td>\n",
        "        <td>28</td>\n",
        "        <td>\\##iv</td>\n",
        "        <td>118</td>\n",
        "        <td>120</td>\n",
        "    </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_mkqBSIgeha"
      },
      "source": [
        "## **Zero-Shot Classification**\n",
        "\n",
        "The zero-shot classification task is characterized by the lack of labeled examples for the classes of interest. So, you can specify any class names you want to classify for texts.\n",
        "\n",
        "### Inference with Zero-Shot Classification Pipeline\n",
        "\n",
        "A model can be loaded with the `zero-shot-classification` pipeline from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads). You can then use found pipeline to classify sequences into any class names you specify.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model and do zero-shot classification for the given sequences. The target model is an original transformer encoder-decoder that combines Google's BERT and OpenAI's GPT with a two-layer classification head, finetuned on the MNLI dataset. Architecture: `1024-hidden`, `16-heads` and `12-layers`. When calling the pipeline, pass `multi_label=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDaKMc9KcVlQ",
        "outputId": "9d4418d9-fc3a-43b3-db7b-550838a13ef7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sequence: My dog died yesterday\n",
            "Labels and their Scores:\n",
            "\tsadness | 0.961\n",
            "\tpain | 0.886\n",
            "\tlove | 0.0318\n",
            "\tstudies | 0.00189\n",
            "\thappy | 0.000906\n",
            "\tuniversity | 0.000197\n",
            "\teducation | 0.000145\n",
            "\n",
            "Sequence: Tomorrow I will have a very difficult exam\n",
            "Labels and their Scores:\n",
            "\tstudies | 0.923\n",
            "\tpain | 0.89\n",
            "\teducation | 0.766\n",
            "\tuniversity | 0.634\n",
            "\tsadness | 0.0558\n",
            "\thappy | 0.000531\n",
            "\tlove | 0.000432\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_zsc\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "sequences_to_classify = [\"My dog died yesterday\",\n",
        "                         \"Tomorrow I will have a very difficult exam\"]\n",
        "# you specify any class names\n",
        "candidate_labels = ['education', 'university', 'studies', 'pain', 'sadness', 'happy', 'love']\n",
        "\n",
        "### START CODE HERE ### (≈ 2-5 lines of code)\n",
        "model_zsc = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "labels_and_scores = model_zsc(sequences_to_classify, candidate_labels, multi_label=True)\n",
        "### END CODE HERE ###\n",
        "\n",
        "for output_dict in labels_and_scores:\n",
        "    print(f\"\\nSequence: {output_dict['sequence']}\\nLabels and their Scores:\")\n",
        "    for labe, scr in zip(output_dict['labels'], output_dict['scores']):\n",
        "        print(\"\\t{} | {:.3}\".format(labe, scr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZW3dUBkr2Wu"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Sequence: My dog died yesterday\n",
        "Labels and their Scores:\n",
        "\tsadness | 0.961\n",
        "\tpain | 0.886\n",
        "\tlove | 0.0318\n",
        "\tstudies | 0.00189\n",
        "\thappy | 0.000906\n",
        "\tuniversity | 0.000197\n",
        "\teducation | 0.000145\n",
        "\n",
        "Sequence: Tomorrow I will have a very difficult exam\n",
        "Labels and their Scores:\n",
        "\tstudies | 0.923\n",
        "\tpain | 0.89\n",
        "\teducation | 0.766\n",
        "\tuniversity | 0.634\n",
        "\tsadness | 0.0558\n",
        "\thappy | 0.000531\n",
        "\tlove | 0.000432\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcyrzWGDsry-"
      },
      "source": [
        "## **Translation**\n",
        "\n",
        "The [translation](https://huggingface.co/tasks/translation) is the task of automatically converting one natural language into another.\n",
        "\n",
        "### Inference with Translation Pipeline\n",
        "You can use a specific model checkpoint in your pipeline with the `translation_xx_to_yy` pattern where `xx` is the source language code, and `yy` is the target language code. [Here](https://huggingface.co/models?pipeline_tag=translation&sort=downloads) you can find already pretrained models for translation tasks.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the Hugging Face Hub a described model for translating given sentences from English to German (`translation_en_to_de`). The model is Google's encoder-decoder Transformer that uses a text-to-text approach. Architecture: `~220M params`, `12-layers`, `768-hidden`, `3072 feed-forward hidden-state`, `12-heads`. Trained in the English text: the Colossal Clean Crawled Corpus (C4). Set `max_length=40`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaP1oHx4o0mU",
        "outputId": "ffb265c3-ee3e-44e7-9a04-b69a0d773ef7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:652: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: I like data science! It is my hobby. I want to improve my knowledge in this sphere.\n",
            "Translated text: Ich mag die Datenwissenschaft, sie ist mein Hobby und ich möchte meine Kenntnisse in diesem Bereich verbessern.\n",
            "\n",
            "Original text: Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\n",
            "Translated text: Transformers ist unsere Bibliothek zur Verarbeitung natürlicher Sprachen und unser Hub ist jetzt für alle Modelle des Machine Learning offen.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_tranlator\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "texts_to_translate = [\"I like data science! It is my hobby. I want to improve my knowledge in this sphere.\",\n",
        "                     \"Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\"]\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "model_tranlator = pipeline(\"translation\", model='google-t5/t5-base', max_length=40)\n",
        "tranlated_texts = model_tranlator(texts_to_translate)\n",
        "### END CODE HERE ###\n",
        "\n",
        "for original_text, tranlated_text in zip(texts_to_translate, tranlated_texts):\n",
        "    print(f\"Original text: {original_text}\\nTranslated text: {tranlated_text['translation_text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af3wjb7kr3Tu"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Original text: I like data science! It is my hobby. I want to improve my knowledge in this sphere.\n",
        "Translated text: Ich mag die Datenwissenschaft, sie ist mein Hobby und ich möchte meine Kenntnisse in diesem Bereich verbessern.\n",
        "\n",
        "Original text: Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\n",
        "Translated text: Transformers ist unsere Bibliothek zur Verarbeitung natürlicher Sprachen und unser Hub ist jetzt für alle Modelle des Machine Learning offen.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCV2ehIVBtkE"
      },
      "source": [
        "## **Text generation**\n",
        "[Text generation](https://huggingface.co/tasks/text-generation) is the task of generating texts to appear indistinguishable from human-written texts.\n",
        "\n",
        "### Inference with Text generation Pipeline\n",
        "You can use the Transformers library `text-generatio` pipeline from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads) to infer with Text Generation models. As input, the model takes an incomplete text and returns multiple outputs which can complete the text.\n",
        "\n",
        "**Excercise:**\n",
        ">Find on the Hub one of the most popular models for the text generation task, which is a GPT-based model. This model has architecture: `6-layers`, `768-hidden`, `12 heads`, and `82M params`. Use the found model to generate the given text using parameters `max_length=60`, `num_return_sequences=5` and `pad_token_id=50256`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GUznvHNBz8q",
        "outputId": "faf2b753-5456-4b5e-ff09-c215c588f725"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-community/openai-gpt and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Hello! I learn Deep Learning and I want to help you. \" \n",
            " \" and i need you to teach me. \" \n",
            " \" but i can help with that part. what do we do if i make the call? \" \n",
            " \" look, i know we've been together for years, but\n",
            "--------------------------------------------------\n",
            "Hello! I learn Deep Learning and I want to learn all of it. we are going on our first training session. \" i told her. \n",
            " \" i am looking forward to it. \" she told me. she looked at me and raised her eyebrows in question. she was really into the subject\n",
            "--------------------------------------------------\n",
            "Hello! I learn Deep Learning and I want to share this, so the best way to explain it is that i can communicate with my own brain, it's the only thing that will explain what i have to say. \" \n",
            " she felt him give her hand a squeeze, making her feel cherished and\n",
            "--------------------------------------------------\n",
            "Hello! I learn Deep Learning and I want to learn deep learning very hard, like in all our studies, and what would you know - it was right in the middle of a beautiful desert. \" \n",
            " there was no reply from her, and he knew it was done, or better yet, that\n",
            "--------------------------------------------------\n",
            "Hello! I learn Deep Learning and I want to learn about life with my mom, my brother and my brothers as well, i thought about making this the perfect college study i always talked about, as i sat at my desk, on my bed of my old wooden couch, a bed with a wooden\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_text_generator\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "base_text = \"Hello! I learn Deep Learning and I want to\"\n",
        "\n",
        "### START CODE HERE ### (≈ 2-6 lines of code)\n",
        "model_text_generator = pipeline('text-generation', model='openai-community/openai-gpt')\n",
        "genereted_texts = model_text_generator(base_text, max_length=60, num_return_sequences=5, pad_token_id=50256)\n",
        "### END CODE HERE ###\n",
        "\n",
        "for gen_text in genereted_texts:\n",
        "    print(50 * \"-\")\n",
        "    print(gen_text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-IHDWcRr4nJ"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to teach you deep learning to create my own unique algorithm. My algorithm is based solely on finding the most perfect solution for your problem. In my post I give you examples for the algorithm. The best way to find and get started: Let me know what\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help you to learn how to do this!\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help you develop this knowledge over time!\n",
        "\n",
        "\n",
        "I want to thank my friend, Eric Dershowitz for allowing me to help me make this possible!\n",
        "Also, thank you for supporting me by supporting me and by helping us.\n",
        "\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help build a new learning pipeline that provides the best opportunities to learn and get started using deep learning algorithms. Deep learning is all about knowing the underlying principles of learning. So for the time being, I don't want to repeat this. Deep learning is\n",
        "--------------------------------------------------\n",
        "Hello! I learn Deep Learning and I want to help to help with this. It is in every student's life that as long as I can have fun learning the language, I will help you. For a class on what language is most like, there are lots of different ways to teach it.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnoHD6uJ-dpq"
      },
      "source": [
        "## **Text2Text Generation**\n",
        "\n",
        "It is the task for text-to-text generation using seq2seq models.\n",
        "\n",
        "### Inference with Text2Text Generation Pipeline\n",
        "\n",
        "Text2Text generation pipeline can be loaded using the following task identifier: `text2text-generation`. On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads), you can find an already pretrained model and use it in your pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Do paraphrase task using described model. The Google research team introduced the model in 2019 as a summarization model using self-supervised objective Gap Sentences Generation to train a transformer encoder-decoder. However, the model we are interested in was finetuned, especially for the paraphrasing task. Set `num_return_sequences=5`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG6-MLaRJnv_",
        "outputId": "d4f5d16a-19bf-4183-b487-a37f204dd956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You need to know that math helps a lot in everyday life.\n",
            "--------------------------------------------------\n",
            "You need to know that math helps in everyday life.\n",
            "--------------------------------------------------\n",
            "You need to know math because it helps a lot in everyday life.\n",
            "--------------------------------------------------\n",
            "You need to know about math because it helps a lot in everyday life.\n",
            "--------------------------------------------------\n",
            "You need to know about math because it helps a lot.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_paraphrase\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "text_to_paraphrase = \"Math helps a lot in everyday life, so you need to know it.\"\n",
        "\n",
        "### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "paraphrase_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer, num_beams=5)\n",
        "paraphrased_texts = paraphrase_pipeline(text_to_paraphrase, num_return_sequences=5)\n",
        "# model_paraphrase = pipeline('text2text-generation', model='transformersbook/pegasus-samsum', num_beams=5)#tuner007/pegasus_paraphrase\n",
        "# paraphrased_texts = model_paraphrase(text_to_paraphrase, num_return_sequences=5)\n",
        "### END CODE HERE ###\n",
        "\n",
        "for text in paraphrased_texts:\n",
        "    print(text['generated_text'] + '\\n' + 50*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra65oCSrr6Bi"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "You need to know that math helps a lot in everyday life.\n",
        "--------------------------------------------------\n",
        "You need to know that math helps in everyday life.\n",
        "--------------------------------------------------\n",
        "You need to know math because it helps a lot in everyday life.\n",
        "--------------------------------------------------\n",
        "It's important to know that math helps in everyday life.\n",
        "--------------------------------------------------\n",
        "You need to know about math because it helps a lot in everyday life.\n",
        "--------------------------------------------------\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401ltCDdBGKA"
      },
      "source": [
        "## **Conversational**\n",
        "\n",
        "The Conversational task mimics human conversation by recognizing speech and text, understanding intent, and deciphering different languages.\n",
        "\n",
        "### Inference with Conversational Pipeline\n",
        "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=conversational&sort=downloads), you can find a pretrained model to create a bot to communicate with you using the `conversational` pipeline. [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Conversation) you can find some examples of using this kind of pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Your task is to find the described model and use it in the pipeline to interact with a conversational bot. The model is based on a transformer architecture similar to BERT or Turing-NLG. The training was carried out on the Reddit dataset and fine-tuned with the [BST](https://arxiv.org/abs/2004.08449) dataset. The model architecture: `2-hidden` layers, `32-heads` and `400M` params.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H71ZqVEJgtV7",
        "outputId": "53b061d4-e758-4513-a859-8cc23a4a5e44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Conversation input is to long (34), trimming it to (20 - 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation id: eceac8f0-b314-4aff-82f9-8f130b9efe39 \n",
            "user >> Hello! Have you any movie recommendations? \n",
            "bot >> The first of the three new \"The Big Bang Theory \n",
            "\n",
            "Conversation id: eceac8f0-b314-4aff-82f9-8f130b9efe39 \n",
            "user >> Hello! Have you any movie recommendations? \n",
            "bot >> The first of the three new \"The Big Bang Theory \n",
            "user >> I also like comedies. What is your favorite comedy? \n",
            "bot >>  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_conv\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "### START CODE HERE ### (≈ 1-2 lines of code)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, ConversationalPipeline\n",
        "# model_name = \"google/flan-t5-xl\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# model_conv = ConversationalPipeline(model=model, tokenizer=tokenizer)\n",
        "#model_conv = pipeline(model=\"jhaochenz/finetuned_gpt2-large_sst2_negation0.001_pretrainedTrue_epochs1\")#pipeline('text-generation', model='distilgpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
        "model_conv = ConversationalPipeline(model=model, tokenizer=tokenizer)\n",
        "### END CODE HERE ###\n",
        "\n",
        "msgs_for_test_list = [\"Hello! Have you any movie recommendations?\", \"I also like comedies. What is your favorite comedy?\"]\n",
        "\n",
        "def test_bot(msgs_for_test):\n",
        "    for i, msg in enumerate(msgs_for_test_list):\n",
        "        conv_msg = msg\n",
        "        if i==0:\n",
        "            conv = Conversation(conv_msg)\n",
        "            model_conv([conv], pad_token_id=50256)\n",
        "            print(conv)\n",
        "        else:\n",
        "            conv.add_user_input(conv_msg)\n",
        "            print(model_conv([conv], pad_token_id=50256))\n",
        "\n",
        "# function to test the bot by yourself\n",
        "def test_bot_by_yourself():\n",
        "    conv_input = input()\n",
        "    customConv = Conversation(conv_input)\n",
        "    model_conv([customConv], pad_token_id=50256)\n",
        "\n",
        "    while conv_input != \"stop\": # the message you should input to STOP the bot\n",
        "        print(customConv)\n",
        "        conv_input = input()\n",
        "        customConv.add_user_input(conv_input)\n",
        "        model_conv([customConv], pad_token_id=50256)\n",
        "\n",
        "test_bot(msgs_for_test_list)\n",
        "#test_bot_by_yourself() # uncomment if you want to test the bot by yourself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHPGp5D1_Wsm"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Conversation id: 53d4d5f7-bd26-4f9a-929b-775f1ab4ab33\n",
        "user >> Hello! Have you any movie recommendations?\n",
        "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
        "\n",
        "Conversation id: 53d4d5f7-bd26-4f9a-929b-775f1ab4ab33\n",
        "user >> Hello! Have you any movie recommendations?\n",
        "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
        "user >> I also like comedies. What is your favorite comedy?\n",
        "bot >>  I don't really have a favourite comedy, but I do like a good drama.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6uDFGrV5gSZ"
      },
      "source": [
        "# **Audio Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx_UkBlUV8yl"
      },
      "source": [
        "## **Text to Speech**\n",
        "\n",
        "It is the [task](https://huggingface.co/tasks/text-to-speech) of converting text input into natural human speech.\n",
        "\n",
        "### Inference\n",
        "On the [Hub](https://huggingface.co/models?pipeline_tag=text-to-speech&sort=downloads), you can find over 160 TTS models that you can use right away by trying out the widgets directly in the browser or calling the models as a service using the Accelerated Inference API.\n",
        "\n",
        "We will use [gradio](https://gradio.app/docs/#load) library to handle the Inference directly on the Google Colab notebook. Gradio interface can be created by constructing an `Interface` object with `Interface.load()` with the path to a Hugging Face model repo.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model and use it to generate speech by inputting text. It is Facebook's non-autoregressive Transformer-based model, trained on The [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcECsZnsFUA",
        "outputId": "6080a299-1c33-40ec-b9f7-5d13fedc462c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching model from: https://huggingface.co/facebook/fastspeech2-en-ljspeech\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_tts_de\n",
        "\n",
        "### START CODE HERE ### (≈ 1-4 lines of code)\n",
        "models = {\n",
        "    \"text-to-speech\": 'facebook/fastspeech2-en-ljspeech' #\"facebook/mms-tts-eng\",\n",
        "}\n",
        "\n",
        "with gr.Blocks() as iface_tts:\n",
        "    for k, v in models.items():\n",
        "        with gr.Tab(k):\n",
        "            gr.load(v, src=\"models\")                                # construct an Interface\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7xPpx7UsJJt",
        "outputId": "d4e9226b-8ece-41a7-d13b-f5edd3574bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/facebook/fastspeech2-en-ljspeech\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1923, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1508, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/utils.py\", line 818, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/external.py\", line 371, in query_huggingface_inference_endpoints\n",
            "    data = fn(*data)  # type: ignore\n",
            "           ^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/inference/_client.py\", line 2297, in text_to_speech\n",
            "    return self.post(json={\"inputs\": text}, model=model, task=\"text-to-speech\")\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
            "    raise HfHubHTTPError(str(e), response=response) from e\n",
            "huggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/facebook/fastspeech2-en-ljspeech (Request ID: luKB0uVM194TC6CFpSbHA)\n",
            "\n",
            "Rate limit reached. Please log in or use a HF access token\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# launch the UI for the interface\n",
        "iface_tts.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRYGiBzmkL2y"
      },
      "source": [
        "## **Automatic Speech Recognition**\n",
        "\n",
        "[ASR](https://huggingface.co/tasks/automatic-speech-recognition) is the task of transcribing given audio to text.\n",
        "### Inference\n",
        "\n",
        "On the [Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads), you can find over 1800 ASR models that you can use right away by trying out the widgets directly in the browser or calling the models as a service using the Accelerated Inference API.\n",
        "\n",
        "We will use [gradio](https://gradio.app/docs/#interface) library to handle the Inference directly on the Google Colab notebook. Gradio interface can be created by constructing an `Interface` object with `Interface.from_pipeline()` with a Transformers Pipeline.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model and try to use the model to recognize your speech converting into text. It is Facebook's ASR model, trained on the [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) dataset. Model architecture: `512-dim` encoder/decoder input, `12-hidden-layers`, `2-conv-layers`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx93UPNTeUIL",
        "outputId": "e2a228f4-ace1-415f-dd4b-8421c66c4b6a",
        "colab": {
          "referenced_widgets": [
            "94f68e7d76e944b3984b711e27d03cfb",
            "d49b847e9c64414db52426b8f25b278b",
            "c9d981691429416e89ac1da5e0fd8eaa",
            "8a8143d3ff62497193825fe8f14b5fba",
            "8de97e84af1849809befc5c2108f2a89",
            "cbb961619a494bc9b8974d31da17c0cb",
            "86a58f7fffc8456a9947f3e56da452f1"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94f68e7d76e944b3984b711e27d03cfb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d49b847e9c64414db52426b8f25b278b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9d981691429416e89ac1da5e0fd8eaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/457 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8143d3ff62497193825fe8f14b5fba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/230k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8de97e84af1849809befc5c2108f2a89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/407k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbb961619a494bc9b8974d31da17c0cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86a58f7fffc8456a9947f3e56da452f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_asr_de\n",
        "\n",
        "### START CODE HERE ### (≈ 2-5 lines of code)\n",
        "model_asr = pipeline(\"automatic-speech-recognition\", model=\"facebook/s2t-large-librispeech-asr\") # create pipeline\n",
        "iface_asr = gr.Interface.from_pipeline(model_asr) # construct an Interface\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab_maHUPsOuM",
        "outputId": "adb5e846-971c-4f4a-dee9-81a11271ca78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/audio_utils.py\", line 32, in ffmpeg_read\n",
            "    with subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE) as ffmpeg_process:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/opt/anaconda3/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1897, in process_api\n",
            "    list(zip(*inputs)),\n",
            "         ^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1483, in call_function\n",
            "    blocks=self,\n",
            "                 \n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/utils.py\", line 816, in wrapper\n",
            "    if before_fn:\n",
            "               ^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/gradio/pipelines.py\", line 55, in fn\n",
            "    data = pipeline(**data)  # type: ignore\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 168, in __call__\n",
            "    return super().__call__(inputs, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1027, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1048, in run_single\n",
            "    for model_inputs in self.preprocess(inputs, **preprocess_params):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 192, in preprocess\n",
            "    inputs = ffmpeg_read(inputs, self.feature_extractor.sampling_rate)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/audio_utils.py\", line 35, in ffmpeg_read\n",
            "    raise ValueError(\"ffmpeg was not found but is required to load audio files from filename\") from error\n",
            "ValueError: ffmpeg was not found but is required to load audio files from filename\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# launch the UI for the interface\n",
        "iface_asr.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-zWI77i5lZj"
      },
      "source": [
        "# **Computer Vision Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxQ1dwsVkhXv"
      },
      "source": [
        "## **Image Classification**\n",
        "\n",
        "It is the [task](https://huggingface.co/tasks/image-classification) of assigning a label or class for a given image. Models take an image as input and return a prediction about which category the image belongs to.\n",
        "\n",
        "### Inference\n",
        "\n",
        "You can use the `image-classification` pipeline to infer with image classification models, which you can find on the [Hub](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads). When you call the pipeline, you need to specify an image path or link. We will use [gradio](https://gradio.app/docs/#interface) library to make it easier to handle the Inference directly on the Google Colab notebook. We will use `Interface.from_pipeline()` with a Transformers Pipeline to create the Gradio interface.\n",
        "\n",
        "**Excercise:**\n",
        ">Try to do image classification using the described model pipeline. It is a Microsoft transformer encoder BERT-like model. This model is pretrained and fine-tuned on the ImageNet-22k dataset, consisting of 14 million images and 21k classes at 224x224 pixels. The model has a `768-hidden` size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "b216d4acf9154c55af525055220a129a",
            "35555df4df304dd7a39609489529bbb6",
            "6e92234affaf4888a09244639ef2cc61"
          ]
        },
        "id": "VN3WuqhbkgsT",
        "outputId": "8e7b92ad-f0ed-4366-ff83-2e2361daf44a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b216d4acf9154c55af525055220a129a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/737 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35555df4df304dd7a39609489529bbb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.16G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3588.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of the model checkpoint at microsoft/beit-large-patch16-224-pt22k were not used when initializing BeitForImageClassification: ['lm_head.weight', 'layernorm.bias', 'lm_head.bias', 'layernorm.weight']\n",
            "- This IS expected if you are initializing BeitForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BeitForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['classifier.weight', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e92234affaf4888a09244639ef2cc61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/276 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_img_cls_de\n",
        "\n",
        "### START CODE HERE ### (≈ 2-5 lines of code)\n",
        "model_img_cls = pipeline(\"image-classification\", model=\"microsoft/beit-large-patch16-224-pt22k\")\n",
        "iface_img_cls = iface_img_cls = gr.Interface.from_pipeline(model_img_cls)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gljDLHIOsg7x",
        "outputId": "3d66ef45-5722-45df-ff0a-dba673f01a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# launch the UI for the interface\n",
        "iface_img_cls.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHWBfF6JsgaD"
      },
      "source": [
        "## **Image Segmentation**\n",
        "\n",
        "It is the [task](https://huggingface.co/tasks/image-segmentation) of dividing an image into segments by creating a pixel-wise mask for each object in the picture. This task has multiple variants, such as instance segmentation, panoptic segmentation, and semantic segmentation.\n",
        "\n",
        "### Inference\n",
        "On the [Hub](https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads), you can find an already pretrained model and infer it using the `image-segmentation` pipeline. [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.ImageSegmentationPipeline) you also can discover segmentation pipeline parameters. For doing image segmentation, you need to install [timm](https://github.com/rwightman/pytorch-image-models).\n",
        "\n",
        "We will not use the gradio library because it doesn't support the `image-segmentation` pipeline type.\n",
        "\n",
        "**Excercise:**\n",
        ">Find the described model to do image segmentation. This model consists of a convolutional (50-layers) backbone followed by an encoder-decoder transformer trained on [COCO 2017](https://cocodataset.org/#download). Set `threshold=0.95` and `mask_threshold=0.7`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owctmkWnzpeI",
        "outputId": "797b7787-b019-4ad7-8894-1183b6e7b0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-07-28 21:52:50--  https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/fruits.jpg\n",
            "Распознаётся dru.fra1.digitaloceanspaces.com (dru.fra1.digitaloceanspaces.com)… 5.101.109.44\n",
            "Подключение к dru.fra1.digitaloceanspaces.com (dru.fra1.digitaloceanspaces.com)|5.101.109.44|:443... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
            "Длина: 432774 (423K) [image/jpeg]\n",
            "Сохранение в: «fruits.jpg»\n",
            "\n",
            "fruits.jpg          100%[===================>] 422,63K  --.-KB/s    за 0,1s    \n",
            "\n",
            "2024-07-28 21:52:51 (2,97 MB/s) - «fruits.jpg» сохранён [432774/432774]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/fruits.jpg -O fruits.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg0_dabYtZI4",
        "outputId": "b7dbe903-282b-49d3-bf3c-07229bb77312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.17.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
            "Using cached transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.17.0\n",
            "    Uninstalling transformers-4.17.0:\n",
            "      Successfully uninstalled transformers-4.17.0\n",
            "Successfully installed transformers-4.43.3\n",
            "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.12/site-packages (0.24.2)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.7)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from timm) (2.3.1)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.18.1)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.24.2)\n",
            "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (23.2)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] model_img_segm_de\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install timm\n",
        "model_name = \"facebook/detr-resnet-50-panoptic\"  # 50-layer convolutional backbone, encoder-decoder transformer, COCO 2017 trained\n",
        "model_img_segm = pipeline(\"image-segmentation\", model=model_name)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRGu1If6sx8D",
        "outputId": "309d646f-d991-4ed9-d307-363d4dece529"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'float' and 'dict'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         ax\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 19\u001b[0m img_masks_out \u001b[38;5;241m=\u001b[39m model_img_segm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfruits.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, mask_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m     20\u001b[0m print_img_and_masks(img_masks_out, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfruits.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/image_segmentation.py:97\u001b[0m, in \u001b[0;36mImageSegmentationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Predictions, List[Prediction]]:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Perform segmentation (detect masks & classes) in the image(s) passed as inputs.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m          \"object\" described by the label and the mask.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1027\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1033\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1033\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1034\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1035\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/image_segmentation.py:102\u001b[0m, in \u001b[0;36mImageSegmentationPipeline.preprocess\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    100\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(image)\n\u001b[1;32m    101\u001b[0m target_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([[image\u001b[38;5;241m.\u001b[39mheight, image\u001b[38;5;241m.\u001b[39mwidth]])\n\u001b[0;32m--> 102\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(images\u001b[38;5;241m=\u001b[39m[image], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target_size\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/detr/feature_extraction_detr.py:568\u001b[0m, in \u001b[0;36mDetrFeatureExtractor.__call__\u001b[0;34m(self, images, annotations, return_segmentation_masks, masks_path, pad_and_return_pixel_mask, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[0;32m--> 568\u001b[0m             images[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize(image\u001b[38;5;241m=\u001b[39mimage, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_normalize:\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/detr/feature_extraction_detr.py:350\u001b[0m, in \u001b[0;36mDetrFeatureExtractor._resize\u001b[0;34m(self, image, size, target, max_size)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;66;03m# size returned must be (w, h) since we use PIL to resize images\u001b[39;00m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;66;03m# so we revert the tuple\u001b[39;00m\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_size_with_aspect_ratio(image_size, size, max_size)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m size \u001b[38;5;241m=\u001b[39m get_size(image\u001b[38;5;241m.\u001b[39msize, size, max_size)\n\u001b[1;32m    351\u001b[0m rescaled_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/detr/feature_extraction_detr.py:348\u001b[0m, in \u001b[0;36mDetrFeatureExtractor._resize.<locals>.get_size\u001b[0;34m(image_size, size, max_size)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m size\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# size returned must be (w, h) since we use PIL to resize images\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# so we revert the tuple\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_size_with_aspect_ratio(image_size, size, max_size)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/detr/feature_extraction_detr.py:327\u001b[0m, in \u001b[0;36mDetrFeatureExtractor._resize.<locals>.get_size_with_aspect_ratio\u001b[0;34m(image_size, size, max_size)\u001b[0m\n\u001b[1;32m    325\u001b[0m     min_original_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mmin\u001b[39m((w, h)))\n\u001b[1;32m    326\u001b[0m     max_original_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mmax\u001b[39m((w, h)))\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_original_size \u001b[38;5;241m/\u001b[39m min_original_size \u001b[38;5;241m*\u001b[39m size \u001b[38;5;241m>\u001b[39m max_size:\n\u001b[1;32m    328\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(max_size \u001b[38;5;241m*\u001b[39m min_original_size \u001b[38;5;241m/\u001b[39m max_original_size))\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (w \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m h \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;241m==\u001b[39m size) \u001b[38;5;129;01mor\u001b[39;00m (h \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m w \u001b[38;5;129;01mand\u001b[39;00m h \u001b[38;5;241m==\u001b[39m size):\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'dict'"
          ]
        }
      ],
      "source": [
        "def print_img_and_masks(model_output, original_img_path):\n",
        "\n",
        "    img_masks_list = [i['mask'] for i in model_output][:9] #select top 9 results\n",
        "    masks_labels = [i['label'] for i in model_output][:9] #select top 9 results\n",
        "\n",
        "    original_img = Image.open(r\"fruits.jpg\")\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.title('Original Image', fontsize=18)\n",
        "    plt.imshow(original_img.resize((600, 400)));\n",
        "\n",
        "    # print segmentation masks\n",
        "    _, axs = plt.subplots(3, 3, figsize=(20, 20)) # 3x3 grid\n",
        "    axs = axs.flatten()\n",
        "    for img, ax, l in zip(img_masks_list, axs, masks_labels):\n",
        "        ax.set_title(f'{l}', fontsize=18)\n",
        "        ax.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "img_masks_out = model_img_segm(\"fruits.jpg\", threshold=0.95, mask_threshold=0.7)\n",
        "print_img_and_masks(img_masks_out, 'fruits.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWYF3ua_tal1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}