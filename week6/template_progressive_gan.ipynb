{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/week6/template_progressive_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jz9pAK46N3e-gh"
      },
      "source": [
        "# High Quality Image Generation with Progressive Growing of GANs\n",
        "\n",
        "Welcome! In the previous lab, we learned how to generate pixelated CryptoPunks using Wasserstein GAN. In this, we will learn how to generate high resolution images of human faces using Progressive Growing of GANs.\n",
        "\n",
        "**GPU** is recomended for this assignment. `Runtime` -> `Change runtime type` -> `GPU`\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- How to implement weight scaling for a better optimization\n",
        "- How to use pixel-wise normalization\n",
        "- How to build a Progressive GAN model\n",
        "- How to train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrBY8Tlh6V_a"
      },
      "source": [
        "# 0 - Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zi73Cz6nzia"
      },
      "outputs": [],
      "source": [
        "!pip install torchviz\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFw7jw34agMh"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1Yl2yyLagMi"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/6_generative_models/progressive_gan/template.zip')\n",
        "!unzip -q template.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxOHLvmlagMi"
      },
      "source": [
        "# 1 - Import packages\n",
        "\n",
        "We will use [torchviz](https://github.com/szagoruyko/pytorchviz) package to visualize our networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovCe5Uld6XXP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "from torchviz import make_dot\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMQlZ5bagMl"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab, we will implement Progressive GAN. Since the original [paper](https://arxiv.org/pdf/1710.10196.pdf) was published at ICLR 2018, the model became a breakthrough for generating high-resolution images. Its revolutionary idea of learning how to generate images **progressively**, from low resolutions to high, became a central focus of the modern GAN research. Its successors, [StyleGAN](https://arxiv.org/pdf/1710.10196.pdf), [StyleGAN2](https://arxiv.org/pdf/1812.04948.pdf), and [AliasFree GAN](https://nvlabs.github.io/alias-free-gan/), inherited many features of Progressive GAN and became current state-of-the-art models in image generation.\n",
        "\n",
        "### Progressive GAN Abstract [[src](https://arxiv.org/pdf/1710.10196.pdf)]:\n",
        "\n",
        "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at $1024^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdUFinTxsWi"
      },
      "source": [
        "## Config\n",
        "\n",
        "During training of Progressive GAN, `level` variable defines the resolution of training images. In original, page it's a float number between $0$ and $8$, where $8$ is a `max_level`. We will set `max_level` to $5$ because of the computation limitations.\n",
        "\n",
        "* `max_level` - number of 2x upscaling from $4 × 4$ resolution. The size of the image during training is defined as $4*2^{level}$\n",
        "* `max_resolution` - is defined as $4*2^{max\\_level}$, which is $128$ in our case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb0EANc_xsWi"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] Config\n",
        "\n",
        "class Config:\n",
        "\n",
        "    # data\n",
        "    data_dir = './CelebA'\n",
        "    batch_size = 128\n",
        "    norm_mean = (0.5, 0.5, 0.5)\n",
        "    norm_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "    # model\n",
        "    max_level = 5\n",
        "    max_resolution = 128\n",
        "    nc = 3\n",
        "    nz = 150\n",
        "    ngf = 16\n",
        "    ndf = 16\n",
        "\n",
        "    # training\n",
        "    logdir = './logs'\n",
        "    seed = 21\n",
        "    num_epochs = 150\n",
        "    learning_rate = 0.0001\n",
        "    train_D_steps = 1\n",
        "    eval_every = 50\n",
        "    save_every = 100\n",
        "    clip = 0.01\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51-60564agMo"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(Config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAZS3MnGMmz"
      },
      "source": [
        "# 2 - Data Preprocessing and loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weBYmHIQ9HFs"
      },
      "source": [
        "## Data Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3xH5n5y8aKh"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(Config.max_resolution),\n",
        "    transforms.CenterCrop(Config.max_resolution),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(Config.norm_mean, Config.norm_std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eChXJGZE-BT"
      },
      "source": [
        "## Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9LH3OND7e19"
      },
      "outputs": [],
      "source": [
        "data = ImageFolder(Config.data_dir, transform=transform)\n",
        "dataloader = DataLoader(\n",
        "    data,\n",
        "    batch_size=Config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVy_Ab00FDXt"
      },
      "source": [
        "## Visualize a batch of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdLhR4Tq-Ly8"
      },
      "outputs": [],
      "source": [
        "def visualize_batch(batch, title=None, save=None, mean=Config.norm_mean, std=Config.norm_std):\n",
        "    # create grid\n",
        "    grid = make_grid(batch, nrow=int(np.sqrt(len(batch))))\n",
        "    # tensor -> array\n",
        "    np_grid = grid.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array(mean)\n",
        "    std = np.array(std)\n",
        "    np_grid = (np_grid * std + mean).clip(0, 1)\n",
        "\n",
        "    # show or save figure\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(np_grid)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if save:\n",
        "        plt.savefig(save)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgOjqeaY-pMk"
      },
      "outputs": [],
      "source": [
        "batch,_ = next(dataloader.__iter__())\n",
        "grid = visualize_batch(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZyZn7W5SjN"
      },
      "source": [
        "## Downsample the images\n",
        "\n",
        "As mentioned before, we will use the `level` variable to denote the image resolution that we are training at. The resolution of the image is defined using the formula $resolution = 4*2^{level}$. Therefore, we need to have a function that downsamples the batch of images.\n",
        "\n",
        "| level      | 0   | 1   | 2     | 3     | 4     | 5       | 6       | 7       | 8         |\n",
        "|------------|-----|-----|-------|-------|-------|---------|---------|---------|-----------|\n",
        "| resolution | 4x4 | 8x8 | 16x16 | 32x32 | 64x64 | 128x128 | 256x256 | 512x512 | 1024x1024 |\n",
        "\n",
        "\n",
        "**Excercise:**\n",
        "\n",
        "Implement a function `downsample` that downsamples a batch of images to the needed resolution. **Note** that level is a float number, **NOT** an integer. So, for example, if the `level` is a number between $0$ and $1$, you should use the resolution of the next integer level ($8x8$ in this case). Also, your resolution **CANNOT** exceed `max_resolution`. **Use** `F.adaptive_avg_pool2d` to perform the resize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz3b-Uqh5UoB"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] downsample\n",
        "\n",
        "def downsample(batch, level, max_resolution = Config.max_resolution):\n",
        "    ### START CODE HERE ### (≈4 lines of code)\n",
        "    ceiled_level =\n",
        "    resolution =\n",
        "    downsampled_batch =\n",
        "    ### END CODE HERE ###\n",
        "    return downsampled_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eetRhrOx5uMx"
      },
      "outputs": [],
      "source": [
        "downsampled = downsample(batch, 0)\n",
        "visualize_batch(downsampled, title=f'Images of shape {downsampled.shape} at Level 0');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJTs_B3agMu"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>image shape</b> </td>\n",
        "    <td> torch.Size([128, 3, 4, 4]) </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnuWvePB6941"
      },
      "outputs": [],
      "source": [
        "downsampled = downsample(batch, 1.3)\n",
        "visualize_batch(downsampled, title=f'Images of shape {downsampled.shape} at Level 3');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K2Kv1hgagMu"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>image shape</b> </td>\n",
        "    <td> torch.Size([128, 3, 16, 16]) </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FEsPYkUh75"
      },
      "source": [
        "# 3 - Model\n",
        "\n",
        "The idea behind the progressive growing of GAN is that we start with low-resolution images and then progressively increase the resolution by adding layers to the networks as visualized in the figure from [the original paper](https://arxiv.org/pdf/1710.10196.pdf):\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/progressive_training.png\">\n",
        "\n",
        "The training starts with both the generator ($G$) and discriminator ($D$) having a low spatial resolution of $4×4$ pixels. As the training advances, we incrementally add layers to $G$ and $D$, thus increasing the spatial resolution of the generated images. All existing layers remain trainable throughout the process. Here $\\boxed{N × N}$ refers to convolutional layers operating on $N × N$ spatial resolution. This allows stable synthesis in high resolutions and also speeds up training considerably. On the right, we show six example images generated using progressive growing at $1024 × 1024$.\n",
        "\n",
        "### Level of training\n",
        "\n",
        "| level      | 0   | 1   | 2     | 3     | 4     | 5       | 6       | 7       | 8         |\n",
        "|------------|-----|-----|-------|-------|-------|---------|---------|---------|-----------|\n",
        "| resolution | 4x4 | 8x8 | 16x16 | 32x32 | 64x64 | 128x128 | 256x256 | 512x512 | 1024x1024 |\n",
        "\n",
        "As we mentioned before, during training, we will use `level` to control the current resolution of the images. For example, when `level=3`, we will train on $64 × 64$ resolution. To do so, we will pass the latent vector through the first $4$ convolutional blocks to obtain a tensor with resolution $64 × 64$. For the discriminator, everything is the same but in reverse. We will pass the output through the 4th, 3rd, 2nd, and 1st convolutional block (in that order) to obtain the output.\n",
        "\n",
        "### Alpha transition\n",
        "\n",
        "When we reach a new level of training, we do not want to simply stack a new untrained layer on the top of our network. That will destabilize the training process, which will lead to poor optimization. Instead, we want to fade in the new layers smoothly.\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/alpha_transition.png\">\n",
        "\n",
        "This example illustrates the transition from $16 × 16$ images (a) to $32 × 32$ images (c). During the transition (b), we treat the layers that operate on the higher resolution like a residual block, whose weight $\\alpha$ increases linearly from 0 to 1. Here $\\boxed{2×}$ and $\\boxed{0.5×}$ refer to doubling and halving the image resolution using nearest neighbor filtering and average pooling, respectively. The $\\boxed{\\text{toRGB}}$ represents a layer that projects feature vectors to RGB colors and $\\boxed{\\text{fromRGB}}$ does the reverse; both use $1 × 1$ convolutions. When training the discriminator, we feed in real images that are downscaled to match the current resolution of the network. During a resolution transition, we interpolate between two resolutions of the real images, similarly to how the generator output combines two resolutions.\n",
        "\n",
        "### Implementation details\n",
        "\n",
        "Note that in the original architecture, authors used **same** padding in `Conv2d` blocks of the discriminator, and then downscaled the images using average pooling. In our implementation, we will use a strided convolution (`stride=2`) to combine two operations in one layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SJ9guTifByW"
      },
      "source": [
        "## Weight Scaling\n",
        "\n",
        "One of the innovations of the Progressive GAN is a new weight scaling method, which applies to every trainable weight in the model. Instead of careful weight initialization, authors of Progressive GAN choose to use a trivial $N(0, 1)$ initialization and then explicitly scale the weights at runtime. More concretely, they multiply each weight in the layer by a scaling factor $c = \\sqrt{2/N}$, where N is a number of parameters in the given layer.\n",
        "\n",
        "**Excercise:** Implement `WScaleLayer` that calculates the scaling factor $c$, and then multiplies the inputs to this layer by $c$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h945HDgUfPDg"
      },
      "source": [
        "$$ \\hat{w}_{i}=w_{i} * c $$\n",
        "\n",
        "$$ c = \\sqrt{ 2 / N } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgim2ToWXp8p"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] WScaleLayer\n",
        "\n",
        "class WScaleLayer(nn.Module):\n",
        "    def __init__(self, incoming):\n",
        "        super(WScaleLayer, self).__init__()\n",
        "        N = incoming.weight[0].numel()\n",
        "        ### START CODE HERE ### (1 line of code)\n",
        "        self.c =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (1 line of code)\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vam7r-hdagMw"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "layer = nn.Conv2d(4, 3, 3)\n",
        "scaler = WScaleLayer(layer)\n",
        "print('scale factor:', scaler.c)\n",
        "\n",
        "x = torch.randn(1, 4, 3, 3)\n",
        "x = scaler(x)\n",
        "x = layer(x)\n",
        "print('output:', x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "950TAnNIagMw"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>scale factor</b> </td>\n",
        "    <td> 0.23570226039551584 </td>\n",
        "</tr>\n",
        "  <tr>\n",
        "    <td> <b>output</b> </td>\n",
        "    <td> tensor([[[[-0.3053]],<br>\n",
        "<br>\n",
        "         [[ 0.1217]],<br>\n",
        "<br>\n",
        "         [[ 0.4188]]]] </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap8S-yf3F5_C"
      },
      "source": [
        "## Pixelwise Normalization\n",
        "\n",
        "To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the\n",
        "generator after each convolutional layer:\n",
        "\n",
        "$$ b_{x, y}=a_{x, y} / \\sqrt{\\frac{1}{N} \\sum_{j=0}^{N-1}\\left(a_{x, y}^{j}\\right)^{2}+\\epsilon} $$\n",
        "\n",
        "where $\\epsilon=10^{-8}$, $N$ is a number of feature maps (number of channels), and $a_{x, y}$ and $b_{x, y}$ are the original and normalized feature vector in pixel $(x, y)$, respectively.\n",
        "\n",
        "**Excercise:** Implement pixelwise normalization layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2uMDkmTEjvk"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] PixelNormLayer\n",
        "\n",
        "class PixelNormLayer(torch.nn.Module):\n",
        "    def __init__(self, e=1e-8):\n",
        "        super(PixelNormLayer, self).__init__()\n",
        "        self.e = e\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (1 line of code)\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OZSluTXagMx"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "l = PixelNormLayer()\n",
        "pixels = torch.randn(1, 3, 2, 2)\n",
        "print('original:', pixels)\n",
        "print('normalized:', l(pixels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09GFGV6sGxDG",
        "outputId": "fdbc0c27-c408-41d3-d82c-b966059d2c01"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>original</b> </td>\n",
        "    <td> tensor([[[[ 0.1081, -0.4376],<br>\n",
        "          [-0.7697, -0.1929]],<br>\n",
        "<br>\n",
        "         [[-0.3626, -2.8451],<br>\n",
        "          [ 1.4435,  0.4976]],<br>\n",
        "<br>\n",
        "         [[ 0.6542,  0.0754],<br>\n",
        "          [-1.0767,  0.1269]]]]) </td>\n",
        "</tr>\n",
        "  <tr>\n",
        "    <td> <b>normalized</b> </td>\n",
        "    <td> tensor([[[[ 0.2478, -0.2632],<br>\n",
        "          [-0.6807, -0.6091]],<br>\n",
        "<br>\n",
        "         [[-0.8310, -1.7113],<br>\n",
        "          [ 1.2767,  1.5711]],<br>\n",
        "<br>\n",
        "         [[ 1.4994,  0.0454],<br>\n",
        "          [-0.9522,  0.4008]]]])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv4EVQDtY0SC"
      },
      "source": [
        "## Generic Convolutional Block\n",
        "\n",
        "Most of the convolutional blocks in the Progressive GAN are composed of the following layers:\n",
        "\n",
        "1. WScaleLayer\n",
        "2. nn.ConvTranspose2d in $G$ or nn.Conv2d in $D$\n",
        "3. nn.BatchNorm2d\n",
        "4. ReLU in $G$ or LeakyReLU in $D$\n",
        "5. PixelNormLayer\n",
        "\n",
        "Below, we will define a generic convolutional layer that combines all 5 layers into one `nn.Sequential` block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWdwl--1Y5j7"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] get_conv_block\n",
        "\n",
        "def get_conv_block(in_ch, out_ch, k_size, stride, padding, bias=False,\n",
        "                   layer=nn.Conv2d, batch_norm=False, activation=nn.ReLU,\n",
        "                   pixel_norm=False, wscaler=False):\n",
        "\n",
        "    conv = layer(in_ch, out_ch, k_size, stride, padding, bias=bias)\n",
        "    layers = OrderedDict()\n",
        "\n",
        "    if wscaler:\n",
        "        layers['weight_scale'] = WScaleLayer(conv)\n",
        "\n",
        "    layers['conv'] = conv\n",
        "\n",
        "    if batch_norm:\n",
        "        layers['batch_norm'] = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    if activation:\n",
        "        layers['activ'] = activation()\n",
        "\n",
        "    if pixel_norm:\n",
        "        layers['pixel_norm'] = PixelNormLayer()\n",
        "\n",
        "    return nn.Sequential(layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDt19AhsNdFs"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLGZasVaUmb5"
      },
      "source": [
        "### Generator's Double Convolutional Block\n",
        "\n",
        "The main block of the Generator is a double convolutional block, similar to the one we implemented in the WGAN lab. The first convolutional block uses a transpose convolution with `kernel_size=4` and `stride=2` to upscale the resolution of the input `x` by a factor of $2$. The second convolutional layer uses a $1 × 1$ convolutions to change the number of channels in the `x`.\n",
        "\n",
        "**Excercise:** Implement `GDoubleConv` using `get_conv_block` and specifications below (all other arguments are default).\n",
        "\n",
        "* `self.conv1`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = in_ch\n",
        "    * kernel size = 4\n",
        "    * stride = 2\n",
        "    * padding = 1\n",
        "    * layers = `WScaler -> ConvTranspose2d -> BatchNorm2d -> ReLU -> PixelNormLayer`\n",
        "* `self.conv2`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = out_ch\n",
        "    * kernel size = 1\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * layers = `WScaler -> ConvTranspose2d -> BatchNorm2d -> ReLU -> PixelNormLayer`\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEdNcc-2Tki5"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] GDoubleConv\n",
        "\n",
        "class GDoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(GDoubleConv, self).__init__()\n",
        "        ### START CODE HERE ### (≈4 lines of code)\n",
        "        # upsample to 2x original size\n",
        "        self.conv1 =\n",
        "        # 1x1 convolutions to decrease number of channels\n",
        "        self.conv2 =\n",
        "        ### END CODE HERE ###\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (≈2 lines of code)\n",
        "        x =\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecceHvmGagMy"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "l = GDoubleConv(10, 5)\n",
        "x = torch.randn(1, 10, 2, 2)\n",
        "print('output shape:', l(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHCRCYTCagMz"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>output shape</b> </td>\n",
        "    <td>torch.Size([1, 5, 4, 4])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPWXZYYrbqhG"
      },
      "source": [
        "### Generator's first Convolutional Block\n",
        "\n",
        "This block is slightly different from `GDoubleConv` because it has `stride=1` instead of $2$ and `padding` is $0$. This helps to convert the input $z$ from shape `[batch, 150, 1, 1]` to `[batch, ngf, 4, 4]`, skipping $2 × 2$ resolution.\n",
        "\n",
        "**Excercise:** Implement `GInputConv` asccording to the specifications. (all other arguments are default).\n",
        "\n",
        "* `self.conv1`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = in_ch\n",
        "    * kernel size = 4\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * layers = `WScaler -> ConvTranspose2d -> BatchNorm2d -> ReLU -> PixelNormLayer`\n",
        "* `self.conv2`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = out_ch\n",
        "    * kernel size = 1\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * layers = `WScaler -> ConvTranspose2d -> BatchNorm2d -> ReLU -> PixelNormLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5COVQPubyDY"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] GInputConv\n",
        "\n",
        "class GInputConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(GInputConv, self).__init__()\n",
        "        ### START CODE HERE ### (≈4 lines of code)\n",
        "        self.conv1 =\n",
        "        self.conv2 =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (≈2 lines of code)\n",
        "        x =\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhBi4UvXagMz"
      },
      "outputs": [],
      "source": [
        "layer = GInputConv(150, 32)\n",
        "x = torch.randn(1, 150, 1, 1)\n",
        "print('output shape:', layer(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97DpUtdYagMz"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>output shape</b> </td>\n",
        "    <td>torch.Size([1, 32, 4, 4])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZQOfe-P2lG9"
      },
      "source": [
        "### toRGB block\n",
        "\n",
        "`toRGB` block is a single convolutional block that uses $1 × 1$ convolutions to transform the input number of channels to three RGB channels.\n",
        "\n",
        "**Excercise:** Implement `toRGB` block according to the specifications below. (all other arguments are default).\n",
        "\n",
        "* `self.conv`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = nc (3 in our config)\n",
        "    * kernel size = 1\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * layers = `WScaler -> ConvTranspose2d -> Tanh`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cWfy1EJt-mS"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] toRGB\n",
        "\n",
        "class toRGB(nn.Module):\n",
        "    def __init__(self, in_ch, nc):\n",
        "        super(toRGB, self).__init__()\n",
        "        ### START CODE HERE ### (≈4 lines of code)\n",
        "        self.conv =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x, activation=True):\n",
        "        ### START CODE HERE ### (≈1 lines of code)\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvqLfQKYagM0"
      },
      "outputs": [],
      "source": [
        "layer = toRGB(16, 3)\n",
        "x = torch.randn(1, 16, 4, 4)\n",
        "print('output shape:', layer(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGYlyfR4agM0"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>output shape</b> </td>\n",
        "    <td>torch.Size([1, 3, 4, 4])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WNIJQ0a9nef"
      },
      "source": [
        "### Generator design\n",
        "\n",
        "Before we start describing the Configuration of the Generator, let's define some notations.\n",
        "\n",
        "**Vocab:**\n",
        "\n",
        "* `level` - float number that characterizes the current state of training.\n",
        "* `c_level` - (ceiled `level`) the least integer greater than or equal to `level`\n",
        "* `max_level` - number of 2x upscaling from 4x4 resolution. The size of the image during training is defined as $4*2^{level}$. Therefore, the maximum resolution is $4*2^{max\\_level}$\n",
        "* `block in` - input number of channels for the convolutional block at a given level\n",
        "* `block out` - output number of channels for the convolutional block at a given level\n",
        "* `toRGB in` - input number of channels for the `toRGB` layer at a given level\n",
        "* `toRGB out` - output number of channels for the `toRGB` layer at a given level\n",
        "* `nz` - size of the input noize vector (150 by default)\n",
        "* `ngf` - default number of feature maps in the generator\n",
        "* `nc` - number of image channels (3 by default)\n",
        "\n",
        "### Design\n",
        "\n",
        "The role of the generator is to transform a latent vector $z$ (of shape $[\\text{batch}, \\text{nz}, 1, 1]$) into a batch of images (of shape $[\\text{batch}, 3, 4*2^{\\text{c_level}}, 4*2^{\\text{c_level}}]$ where $\\text{c_level}$ is the least integer greater than or equal to `level`.\n",
        "\n",
        "So, for example, if we are at the $\\text{level}=2.5$, $\\text{c_level}=3$, our generator should return $[\\text{batch}, 3, 32, 32]$. This is done through a sequence of convolutional layers (`blocks[i]`, where $i\\in[0, \\text{c_level}]$) that upscales the resolution of the image, and $\\text{c_level}$-th `toRGB` layer (`toRGB[c_level]`) that maps the output to a $3$ channel space. At each layer, generator **decreases** the number of channels and **increases** the spatial dimension of the input.\n",
        "\n",
        "\n",
        "### `__init__`\n",
        "\n",
        "Generally, a generator operates using $\\text{level}\\in{[0, \\text{max_level}]}$. So, in the `__init__` function you should initialize $\\text{max_level} + 1$ number of `toRGB` and `block` layers (for each resolution of images). We always want the last `blocks` layer to have $\\text{ngf}×1$ number of channels, and we do not want our number of channels to ever exceed $\\text{ngf}×32$. See examples of correct configuration for different $\\text{max_level}$ below.\n",
        "\n",
        "\n",
        "### `forward`\n",
        "\n",
        "In forward, we just have to pass the latent vector $z$ through $\\text{c_level}$ layers of our `block` layers. We also have to remember our second to last output because we will need it for alpha transition. The last output of the neural network we have to pass through the `toRGB[c_level]` layer. During the **alpha transition**, we have to upsample the second to last output using `F.interpolate`, pass it through `toRGB[c_level-1]` layer, and combine two outputs using the formula below.\n",
        "\n",
        "$$ \\text{out} = x_1 \\alpha + x_2 (1 - \\alpha) $$\n",
        "\n",
        "Where $x_1$ is the last output of `blocks`, and $x_2$ is a second to last output of `blocks`.\n",
        "\n",
        "**Excercise:** Implement `__init__` and `forward` functions of the generator. See configurations below for more details.\n",
        "\n",
        "\n",
        "---\n",
        "**Network Configuration if `max_level=3`**\n",
        "\n",
        "| level         | 0         | 1         | 2         | 3         |\n",
        "|-------------  |-------    |-------    |-------    |-------    |\n",
        "| resolution    |  4x4      | 8x8       | 16x16     | 32x32     |\n",
        "| block in      | nz        | ngfx8     | ngfx4     | ngfx2     |\n",
        "| block out     | ngfx8     | ngfx4     | ngfx2     | ngfx1     |\n",
        "| toRGB in      | ngfx8     | ngfx4     | ngfx2     | ngfx1     |\n",
        "| toRGB out     | nc        | nc        | nc        | nc        |\n",
        "\n",
        "---\n",
        "**Network Configuration if `max_level=5`**\n",
        "\n",
        "|     level     | 0         | 1         | 2         | 3         | 4         | 5         |\n",
        "|-------------  |--------   |--------   |--------   |-------    |-------    |---------  |\n",
        "| resolution    |  4x4      | 8x8       | 16x16     | 32x32     | 64x64     | 128x128   |\n",
        "| block in      | nz        | ngfx32    | ngfx16    | ngfx8     | ngfx4     | ngfx2     |\n",
        "| block out     | ngfx32    | ngfx16    | ngfx8     | ngfx4     | ngfx2     | ngfx1     |\n",
        "| toRGB in      | ngfx32    | ngfx16    | ngfx8     | ngfx4     | ngfx2     | ngfx1     |\n",
        "| toRGB out     | nc        | nc        | nc        | nc        | nc        | nc        |\n",
        "\n",
        "---\n",
        "**Network Configuration if `max_level=8`**\n",
        "\n",
        "|     level     | 0         | 1         | 2         | 3         | 4         | 5         | 6         | 7         | 8             |\n",
        "|-------------  |--------   |--------   |--------   |--------   |--------   |---------  |---------  |---------  |-----------    |\n",
        "| resolution    |  4x4      | 8x8       | 16x16     | 32x32     | 64x64     | 128x128   | 256x256   | 512x512   | 1024x1024     |\n",
        "| block in      | nz        | ngfx32    | ngfx32    | ngfx32    | ngfx32    | ngfx16    | ngfx8     | ngfx4     | ngfx2         |\n",
        "| block out     | ngfx32    | ngfx32    | ngfx32    | ngfx32    | ngfx16    | ngfx8     | ngfx4     | ngfx2     | ngfx1         |\n",
        "| toRGB in      | ngfx32    | ngfx32    | ngfx32    | ngfx32    | ngfx16    | ngfx8     | ngfx4     | ngfx2     | ngfx1         |\n",
        "| toRGB out     | nc        | nc        | nc        | nc        | nc        | nc        | nc        | nc        | nc            |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGQntjX_Uv41"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] Generator\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        nz=Config.nz,\n",
        "        nc=Config.nc,\n",
        "        ngf=Config.ngf,\n",
        "        max_level=Config.max_level\n",
        "    ):\n",
        "        super(Generator, self).__init__()\n",
        "        self.max_level=max_level\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList()\n",
        "        self.toRGBs = torch.nn.ModuleList()\n",
        "\n",
        "        ### START CODE HERE ### (≈9 lines of code)\n",
        "        # add first blocks for level=0\n",
        "        max_channels = ngf * min(32, 2**max_level)\n",
        "        self.blocks.append( # first block\n",
        "        self.toRGBs.append( # first toRGB\n",
        "\n",
        "        for level in range(1, max_level+1):\n",
        "            in_channels =\n",
        "            out_channels =\n",
        "            self.blocks.append(GDoubleConv(in_channels, out_channels))\n",
        "            self.toRGBs.append(toRGB(out_channels, nc))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x, level):\n",
        "        ### START CODE HERE ### (≈10 lines of code)\n",
        "        # calculate alpha and c_level using level\n",
        "        alpha = level - int(level) if level < self.max_level else 0\n",
        "        c_level = min(int(np.ceil(level)), self.max_level)\n",
        "\n",
        "        # froward through level number of layers (saving two last outputs)\n",
        "        for l in range(0, c_level):     # pass through all but last layer\n",
        "            x =\n",
        "        x1 =  # pass through the last layer\n",
        "        x1 =  # convert last output to rgb\n",
        "\n",
        "        # alpha transition\n",
        "        if alpha != 0:\n",
        "            # upsample\n",
        "            x2 =\n",
        "            # pass through the toRGB\n",
        "            x2 =\n",
        "            out =\n",
        "        else:\n",
        "            out = x1\n",
        "        ### END CODE HERE ###\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL7zuMBtagM2"
      },
      "outputs": [],
      "source": [
        "# test block shapes\n",
        "for max_lvl in [3, 5, 8]:\n",
        "    x = torch.randn(1, Config.nz, 1, 1)\n",
        "    G = Generator(max_level=max_lvl)\n",
        "    for i, block in enumerate(G.blocks):\n",
        "        x = block(x)\n",
        "        out = G.toRGBs[i](x)\n",
        "        resolution = 4 * 2**i\n",
        "        num_channels = min(Config.ngf * 32, Config.ngf * 2**(max_lvl-i))\n",
        "        x_shape = (1, num_channels, resolution, resolution)\n",
        "        out_shape = (1, 3, resolution, resolution)\n",
        "        assert x.shape == x_shape, f'output of block[{i}] should be {x_shape}'\n",
        "        assert out.shape == out_shape, f'output of toRGB[{i}] should be {out_shape}'\n",
        "    print(f'max_level={max_lvl} is correct!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaffv_JpagM2"
      },
      "outputs": [],
      "source": [
        "# test forward\n",
        "set_seed(Config.seed)\n",
        "G = Generator(max_level=5)\n",
        "x = torch.randn(1, Config.nz, 1, 1)\n",
        "out = G(x, 0.5)\n",
        "print('shape:', out.shape)\n",
        "print('mean:', out.mean().item())\n",
        "print('std:', out.std().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lYddLkFagM3"
      },
      "source": [
        "**Expected output:**\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>shape</b> </td>\n",
        "    <td>torch.Size([1, 3, 8, 8])</td>\n",
        "</tr>\n",
        "    <tr>\n",
        "    <td> <b>mean</b> </td>\n",
        "    <td>-0.10339345782995224</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td> <b>std</b> </td>\n",
        "    <td>0.6227303743362427</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epofNab7NmxP"
      },
      "source": [
        "## Discrimnator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRvqJuDXfs2"
      },
      "source": [
        "### Discrimnator's Convolutional Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeAlm2rHaQgh"
      },
      "source": [
        "It is very similar to Generator's Block except this one uses `Conv2d` instead of `ConvTranspose2d` and activation is `LeakyReLU()`\n",
        "\n",
        "**Excercise:** Implement `DDoubleConv` using `get_conv_block` and specifications below (all other arguments are default).\n",
        "\n",
        "* `self.conv1`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = in_ch\n",
        "    * kernel size = 4\n",
        "    * stride = 2\n",
        "    * padding = 1\n",
        "    * layers = `WScaler -> ConvTranspose2d -> BatchNorm2d -> ReLU`\n",
        "* `self.conv2`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = out_ch\n",
        "    * kernel size = 1\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * layers = `WScaler -> ConvTranspose2d -> BatchNorm2d -> ReLU`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL-65M7NZgMA"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] DDoubleConv\n",
        "\n",
        "class DDoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(DDoubleConv, self).__init__()\n",
        "        ### START CODE HERE ### (≈14 lines of code)\n",
        "        # downsample to 2x original size\n",
        "        self.conv1 =\n",
        "        # 1x1 convolutions to increase number of channels\n",
        "        self.conv2 =\n",
        "        ### END CODE HERE ###\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (≈2 lines of code)\n",
        "        x =\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9urvEJxeJrF"
      },
      "outputs": [],
      "source": [
        "layer = DDoubleConv(3, 6)\n",
        "x = torch.randn(1, 3, 32, 32)\n",
        "print('output shape:', layer(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFJ7cEkSeoji"
      },
      "source": [
        "**Expected output**:\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>output shape</b> </td>\n",
        "    <td> torch.Size([1, 6, 16, 16])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4F5BCxIagRW"
      },
      "source": [
        "### Discrimnator's Last Convolutional Block\n",
        "\n",
        "**Excercise:** Implement `DOutputBlock` using `get_conv_block` and specifications below (all other arguments are default).\n",
        "\n",
        "* `self.conv`:\n",
        "    * input number of channels = in_ch\n",
        "    * output number of channels = out_ch\n",
        "    * kernel size = 2\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * bias = False\n",
        "    * activation = None\n",
        "    * layer = `Conv2d`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFPH5WwSassB"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] DOutputBlock\n",
        "\n",
        "class DOutputBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(DOutputBlock, self).__init__()\n",
        "        ### START CODE HERE ### (≈1 line of code)\n",
        "        self.conv =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (≈1 line of code)\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30pUali3fLSJ"
      },
      "outputs": [],
      "source": [
        "layer = DOutputBlock(3, 1)\n",
        "x = torch.randn(1, 3, 2, 2)\n",
        "print('output shape:', layer(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lU9NYe0hHiv"
      },
      "source": [
        "**Expected output**:\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>output shape</b> </td>\n",
        "    <td> torch.Size([1, 1, 1, 1])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1w96RWxbTuJ"
      },
      "source": [
        "### fromRGB Block\n",
        "\n",
        "`fromRGB` is essentially the reverse operation of `toRGB`. The input to this layer has `nc` channels, and it maps it to `out_ch` using $1 × 1$ convolutions.\n",
        "\n",
        "**Excercise:** Implement `fromRGB` using `get_conv_block` and specifications below (all other arguments are default).\n",
        "\n",
        "* `self.conv`:\n",
        "    * input number of channels = nc\n",
        "    * output number of channels = out_ch\n",
        "    * kernel size = 1\n",
        "    * stride = 1\n",
        "    * padding = 0\n",
        "    * layers = `WScaler -> Conv2d -> LeakyReLU`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dXtJ_YmbXmt"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] fromRGB\n",
        "\n",
        "class fromRGB(nn.Module):\n",
        "    def __init__(self, nc, out_ch):\n",
        "        super(fromRGB, self).__init__()\n",
        "        ### START CODE HERE ### (≈5 lines of code)\n",
        "        self.conv =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ###\n",
        "        x =\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tkrg4g7iEAe"
      },
      "outputs": [],
      "source": [
        "layer = fromRGB(3, 64)\n",
        "x = torch.randn(1, 3, 16, 16)\n",
        "print('output shape:', layer(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nunLLveHiZld"
      },
      "source": [
        "**Expected output**:\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>output shape</b> </td>\n",
        "    <td> torch.Size([1, 64, 16, 16])</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es84UWZtUoZD"
      },
      "source": [
        "### Discrimnator Design\n",
        "\n",
        "The role of the discrimnator is to output a critic  for a batch of input images (of shape $[\\text{batch}, 3, 4*2^{\\text{c_level}}, 4*2^{\\text{c_level}}]$ where $\\text{c_level}$ is the least integer greater than or equal to `level`. Similarly to the generator, this is accomplished through a stack of convolutional `blocks` and `fromRGB` layers. At each layer, discriminator **increases** the number of channels and **decreases** the spatial dimension of the input.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### `__init__`\n",
        "\n",
        "In terms of initialization, the discriminator is very symmetrical to the generator with one small exception — there is an additional output block that is **separate** from blocks.\n",
        "\n",
        "### `forward`\n",
        "\n",
        "For the discriminator, the forward pass should be done in reverse. For example, the image at resolution $128 × 128$, should be passed through the 5th `fromRGB` layer and then through the blocks of layers 5, 4, 3, 2, 1, 0 (in that order), and finally through the output.\n",
        "\n",
        "Similarly, the **alpha transition** is also reversed, and we will implement it at the begining of the `forward` method. First, we will pass the inputs through the `fromRGB[c_level]` and `blocks[c_level]` to obtain `x1`, where $\\text{c_level}$ is ceiled level . Next, we will take original inputs again, downscale them by a factor of $2$ using `F.avg_pool2d`, and feed them through `fromRGB[c_level-1]` to obtain `x2`. Then, we will apply our alpha transition formula:\n",
        "\n",
        "$$ \\text{out} = x_1 \\alpha + x_2 (1 - \\alpha) $$\n",
        "\n",
        "After that, we will have to pass the inputs through the remaining blocks in reverse (starting from $\\text{c_level}$ to 0). To improve the variation in the dataset, we will also compute standard deviation across the minibatch — another major contribution of Progressive GAN paper — and concatenate it with the output of the last block. Finally, we will pass the outputs through our `output` block and celebrate!\n",
        "\n",
        "**Excercise:** Implement `__init__` and `forward` functions of the generator. See configurations below for more details. Good luck...\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Discrimnator Configuration if `max_level=3`**\n",
        "\n",
        "| level       | 0      | 1     | 2     | 3     |\n",
        "|-------------|--------|-------|-------|-------|\n",
        "| resolution  | 4x4    | 8x8   | 16x16 | 32x32 |\n",
        "| block in    | ngdx8  | ngdx4 | ngdx2 | ngdx1 |\n",
        "| block out   | ngdx16 | ngdx8 | ngdx4 | ngdx2 |\n",
        "| fromRGB in  | nc     | nc    | nc    | nc    |\n",
        "| fromRGB out | ngdx8  | ngdx4 | ngdx2 | ngdx1 |\n",
        "\n",
        "---\n",
        "\n",
        "**Discrimnator Configuration if `max_level=5`**\n",
        "\n",
        "|     level     | 0         | 1         | 2         | 3         | 4         | 5         |\n",
        "|-------------  |--------   |--------   |--------   |-------    |-------    |---------  |\n",
        "| resolution    |  4x4      | 8x8       | 16x16     | 32x32     | 64x64     | 128x128   |\n",
        "| block in      | ngdx32    | ngdx16    | ngdx8     | ngdx4     | ngdx2     | ngdx1     |\n",
        "| block out     | ngdx32    | ngdx32    | ngdx16    | ngdx8     | ngdx4     | ngdx2     |\n",
        "| fromRGB in    | nc        | nc        | nc        | nc        | nc        | nc        |\n",
        "| fromRGB out   | ngdx32    | ngdx16    | ngdx8     | ngdx4     | ngdx2     | ngdx1     |\n",
        "\n",
        "---\n",
        "**Discrimnator Configuration if `max_level=8`**\n",
        "\n",
        "| level       | 0      | 1      | 2      | 3      | 4      | 5       | 6       | 7       | 8         |\n",
        "|-------------|--------|--------|--------|--------|--------|---------|---------|---------|-----------|\n",
        "| resolution  | 4x4    | 8x8    | 16x16  | 32x32  | 64x64  | 128x128 | 256x256 | 512x512 | 1024x1024 |\n",
        "| block in    | ngdx32 | ngdx32 | ngdx32 | ngdx32 | ngdx16 | ngdx8   | ngdx4   | ngdx2   | ngdx1     |\n",
        "| block out   | ngdx32 | ngdx32 | ngdx32 | ngdx32 | ngdx32 | ngdx16  | ngdx8   | ngdx4   | ngdx2     |\n",
        "| fromRGB in  | nc     | nc     | nc     | nc     | nc     | nc      | nc      | nc      | nc        |\n",
        "| fromRGB out | ngdx32 | ngdx32 | ngdx32 | ngdx32 | ngdx16 | ngdx8   | ngdx4   | ngdx2   | ngdx1     |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4qrD5lnckjk"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] Discriminator\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc=Config.nc, ndf=Config.ndf, max_level=Config.max_level):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.max_level=max_level\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList()\n",
        "        self.fromRGBs = torch.nn.ModuleList()\n",
        "        ### START CODE HERE ### (≈7 lines of code)\n",
        "        for level in range(max_level+1):\n",
        "            in_channels =\n",
        "            out_channels =\n",
        "            self.fromRGBs.append(fromRGB(nc, in_channels))\n",
        "            self.blocks.append(DDoubleConv(in_channels, out_channels))\n",
        "\n",
        "        # initialize output layer\n",
        "        in_channels = 2 * ndf * min(32, 2 ** (max_level+1))\n",
        "        self.out =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x, level):\n",
        "        ### START CODE HERE ### (≈10 lines of code)\n",
        "        # calculate alpha and c_level using level\n",
        "        alpha =\n",
        "        c_level =\n",
        "\n",
        "        # pass through the fromRGB layer\n",
        "        x1 =\n",
        "        # pass through the block\n",
        "        x1 =\n",
        "\n",
        "        # alpha transition\n",
        "        if alpha != 0:\n",
        "            # downsample\n",
        "            x2 =\n",
        "            # fromRGB\n",
        "            x2 =\n",
        "            # calculate alpha transition\n",
        "            x =\n",
        "        else:\n",
        "            x = x1\n",
        "\n",
        "        # forward pass through all but the last layer\n",
        "        for i in range(c_level)[::-1]:\n",
        "            x =\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # std layer across the batch dimension\n",
        "        std = torch.std(x, dim=0).unsqueeze(0).expand_as(x)\n",
        "        x = torch.cat((x, std), dim=1)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x.view(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPZOc3hMtv6T"
      },
      "outputs": [],
      "source": [
        "# test block shapes\n",
        "for max_lvl in [3, 5, 8]:\n",
        "    D = Discriminator(max_level=max_lvl)\n",
        "    res = 4*2**max_lvl\n",
        "    img = torch.randn(1, Config.nc, res, res)\n",
        "    for i, fRGB in enumerate(D.fromRGBs):\n",
        "        x = fRGB(img)\n",
        "        from_shape = (1, Config.ndf * min(32, 2 ** (max_lvl - i)), res, res)\n",
        "        assert x.shape == from_shape, f'output of fromRGB[{i}] should be {from_shape}'\n",
        "\n",
        "    for i in range(len(D.blocks))[::-1]:\n",
        "        x = D.blocks[i](x)\n",
        "        num_channels = Config.ndf * min(32, 2 ** (max_lvl - i + 1))\n",
        "        x_shape = (1, num_channels, 4*2**(i - 1) , 4*2**(i - 1) )\n",
        "        assert x.shape == x_shape, f'output of block[{i}] should be {x_shape}'\n",
        "\n",
        "    x = torch.cat((x, x), dim=1)\n",
        "    out_shape = (1, 1, 1, 1)\n",
        "    assert D.out(x).shape == out_shape, f'final output should be {out_shape}'\n",
        "    print(f'max_level={max_lvl} is correct!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv7Gm5ug0O_J"
      },
      "outputs": [],
      "source": [
        "# test forward\n",
        "set_seed(Config.seed)\n",
        "D = Discriminator(max_level=5)\n",
        "img = torch.randn(4, Config.nc, 16, 16)\n",
        "out = D(img, 1.9)\n",
        "print('output:', out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpqq7Wo2l6n"
      },
      "source": [
        "**Expected output**:\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>out</b> </td>\n",
        "    <td> tensor([[-0.0078],<br>\n",
        "        [ 0.1246],<br>\n",
        "        [ 0.2804],<br>\n",
        "        [ 0.4500]], grad_fn=<ViewBackward>)</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lOm9DQRlgZD"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU7eLI7fmqp8"
      },
      "outputs": [],
      "source": [
        "def test(netG, netD):\n",
        "    for level in np.arange(0, Config.max_level + 0.5, 0.1):\n",
        "        c_level = min(int(np.ceil(level)), Config.max_level)\n",
        "        out_res = 4 * 2 ** (c_level)\n",
        "        z = torch.zeros(1, Config.nz, 1, 1)\n",
        "        out = netG(z, level)\n",
        "        assert out.shape == (1, 3, out_res, out_res), 'G Output Shape={}'.format(out.shape)\n",
        "\n",
        "        valid = netD(out, level)\n",
        "        assert valid.shape == (1, 1), 'D Output Shape={}'.format(valid.shape)\n",
        "\n",
        "        print(f'Verified Size {out_res:3} x {out_res:3} @ Level {level:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86wkgaQbzRIU"
      },
      "outputs": [],
      "source": [
        "netG = Generator()\n",
        "netD = Discriminator()\n",
        "test(netG, netD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7V8coKkoBj6"
      },
      "source": [
        "## Model Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIarGe95oE_a"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kFbeTKJoEQh"
      },
      "outputs": [],
      "source": [
        "out = netG(torch.zeros(1, 150, 1, 1), level=0.5)\n",
        "print(netG)\n",
        "make_dot(out, params=dict(netG.named_parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpbms_qsoHx1"
      },
      "source": [
        "### Discrimnator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaDKET0roKsM"
      },
      "outputs": [],
      "source": [
        "netD = Discriminator()\n",
        "print(netD)\n",
        "out = netD(torch.zeros(1, 3, 8, 8), level=0.5)\n",
        "make_dot(out, params=dict(netD.named_parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NIBLRkFm4cZ"
      },
      "source": [
        "# 4 - Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1j3-5NswZi"
      },
      "source": [
        "## Get Level\n",
        "\n",
        "We want to train faster on the lower resolutions, and spend more time training on bigger resolutions. To do so, we will implement `LevelProgress` class that controls the level prorgression.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqY4KETzWvf"
      },
      "outputs": [],
      "source": [
        "class LevelProgress():\n",
        "    def __init__(self, total_step, max_level):\n",
        "        self.total_step = total_step\n",
        "        self.max_level = max_level\n",
        "    def __call__(self, step):\n",
        "        level = np.sqrt(step/self.total_step) * (self.max_level + 2) - 1\n",
        "        level = max(min(level, self.max_level), 0)\n",
        "        return level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjqJfXyahh2x"
      },
      "source": [
        "Let's visualize how our level progression looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTJinfDczzoD"
      },
      "outputs": [],
      "source": [
        "num_steps = 200\n",
        "Level = LevelProgress(num_steps, Config.max_level)\n",
        "progress = [Level(step) for step in range(0, num_steps)]\n",
        "\n",
        "plt.figure(figsize=((18, 12)))\n",
        "plt.scatter(range(0, num_steps), progress, s=4)\n",
        "plt.ylabel('Level of resolution')\n",
        "plt.xlabel('Training Step')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLnMJ32Uq7Dg"
      },
      "source": [
        "## Train discriminator\n",
        "\n",
        "Here we will implement a regular GAN training loop with W-GAN loss (just like we did in the previous lab).\n",
        "\n",
        "**Excercise:** Implement `train_discriminator` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK-aS7ZbiXkZ"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] train_discriminator\n",
        "\n",
        "def train_discriminator(netD, netG, optimizerD, imgs, level,\n",
        "                        k_steps=Config.train_D_steps,\n",
        "                        nz=Config.nz,\n",
        "                        clip=Config.clip,\n",
        "                        device=Config.device):\n",
        "\n",
        "    overall_loss = 0\n",
        "    for k in range(k_steps):\n",
        "        ### START CODE HERE ### (≈12 lines of code)\n",
        "        # downsample images\n",
        "        real_imgs =\n",
        "        # get latent vector z\n",
        "        z =\n",
        "        # generate fake images\n",
        "        fake_images =\n",
        "\n",
        "        # forward pass\n",
        "        real_outputs =\n",
        "        fake_outputs =\n",
        "        loss =\n",
        "\n",
        "        # optimize (≈3 lines of code)\n",
        "\n",
        "\n",
        "        # enforcing 1-L condition\n",
        "        # clip the parameters of the discriminator (≈2 lines of code)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        overall_loss += loss.item()\n",
        "\n",
        "    return overall_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96NjpfJOruA1"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "level = 0\n",
        "netG = Generator().to(Config.device)\n",
        "netD = Discriminator().to(Config.device)\n",
        "optimizerD = optim.RMSprop(netD.parameters(), lr=Config.learning_rate)\n",
        "imgs, _ = next(dataloader.__iter__())\n",
        "loss = train_discriminator(netD, netG, optimizerD, imgs, level)\n",
        "print('loss:', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz4Pc64kr86m"
      },
      "source": [
        "**Expected output**:\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>loss</b> </td>\n",
        "    <td> 0.0740940272808075</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXv7IxOhs7K8"
      },
      "source": [
        "## Train Generator\n",
        "\n",
        "Let's also define a function to train the generator\n",
        "\n",
        "**Excercise:** Implement `train_generator` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHrUlG-FtcRz"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] train_generator\n",
        "\n",
        "def train_generator(netD, netG, optimizerG, level,\n",
        "                    batch_size=Config.batch_size,\n",
        "                    nz=Config.nz,\n",
        "                    device=Config.device):\n",
        "    ### START CODE HERE ### (≈6 lines of code)\n",
        "    # generate data\n",
        "    z =\n",
        "    outputs =\n",
        "    loss =\n",
        "\n",
        "    # optimize\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5TyOjQpuEDL"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "level = 0\n",
        "netG = Generator().to(Config.device)\n",
        "netD = Discriminator().to(Config.device)\n",
        "optimizerD = optim.RMSprop(netG.parameters(), lr=Config.learning_rate)\n",
        "loss = train_generator(netD, netG, optimizerD, level)\n",
        "print('loss:', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHbLtJqmuYs4"
      },
      "source": [
        "**Expected output**:\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <b>loss</b> </td>\n",
        "    <td> -0.3071933388710022</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcOAVHgIucF9"
      },
      "source": [
        "## Train\n",
        "\n",
        "Let's put everything together in one big `train` function. We will perform evaluation every `eval_every` step of the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I6zQutjq8vk"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    netG,\n",
        "    netD,\n",
        "    optimizerD,\n",
        "    optimizerG,\n",
        "    train_loader,\n",
        "    clip=Config.clip,\n",
        "    nz = Config.nz,\n",
        "    max_level=Config.max_level,\n",
        "    k_steps=Config.train_D_steps,\n",
        "    num_epochs=Config.num_epochs,\n",
        "    save_every=Config.save_every,\n",
        "    eval_every=Config.eval_every,\n",
        "    logdir=Config.logdir,\n",
        "    device=Config.device\n",
        "    ):\n",
        "\n",
        "    total_steps = num_epochs * len(train_loader)\n",
        "    print(f'Training for {num_epochs} epochs and {total_steps} steps')\n",
        "    Level = LevelProgress(total_steps, max_level)\n",
        "\n",
        "    if not os.path.exists(os.path.join(logdir, 'generated')):\n",
        "        os.makedirs(os.path.join(logdir, 'generated'))\n",
        "\n",
        "    tb_writer = SummaryWriter(log_dir=logdir)\n",
        "    fixed_noise = torch.randn(32, nz, 1, 1).to(device)\n",
        "\n",
        "    current_step = 0\n",
        "    d_loss, g_loss = 0.0, 0.0\n",
        "    num_imgs = 0\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        for imgs, _ in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            loss = train_discriminator(netD, netG, optimizerD, imgs,\n",
        "                                       level, k_steps, nz, clip, device)\n",
        "            d_loss += loss * imgs.shape[0]\n",
        "            loss = train_generator(netD, netG, optimizerG,\n",
        "                                   level, imgs.shape[0], nz, device)\n",
        "            g_loss += loss * imgs.shape[0]\n",
        "            current_step += 1\n",
        "            num_imgs += imgs.shape[0]\n",
        "\n",
        "            ############\n",
        "            # Evaluate #\n",
        "            ############\n",
        "            if current_step % eval_every == 0:\n",
        "\n",
        "                # print losses\n",
        "                d_loss = d_loss / num_imgs / k_steps\n",
        "                g_loss = g_loss / num_imgs\n",
        "                print_line = f\"Epoch: {e}/{num_epochs}, Step: {current_step}/{total_steps}\"\n",
        "                print_line += f\", Level:{level:.6f}, G Loss:{g_loss:.6f}, D Loss:{d_loss:.6f}\"\n",
        "                print(print_line)\n",
        "\n",
        "                # Tensorboards Logging\n",
        "                tb_writer.add_scalar('ProgressiveGAN/D Loss', d_loss, current_step)\n",
        "                tb_writer.add_scalar('ProgressiveGAN/G Loss', g_loss, current_step)\n",
        "                tb_writer.add_scalar('ProgressiveGAN/Level', level, current_step)\n",
        "\n",
        "                # null the counters\n",
        "                d_loss, g_loss = 0.0, 0.0\n",
        "                num_imgs = 0\n",
        "\n",
        "                # make grid of examples\n",
        "                with torch.no_grad():\n",
        "                    netG.eval() # we don't really have to call eval because there are no dropouts\n",
        "                    generated = netG(fixed_noise, level).cpu()\n",
        "                    netG.train()\n",
        "\n",
        "                grid = visualize_batch(\n",
        "                    generated,\n",
        "                    title='Generated images',\n",
        "                    save=os.path.join(logdir, 'generated', f'{current_step}.png')\n",
        "                )\n",
        "\n",
        "                # save image\n",
        "                tb_writer.add_image('ProgressiveGAN/Output', grid, current_step)\n",
        "\n",
        "            ##############\n",
        "            # Save model #\n",
        "            ##############\n",
        "            if current_step % save_every == 0:\n",
        "\n",
        "                # create directory\n",
        "                save_path = os.path.join(logdir, f'{current_step:010}')\n",
        "                if not os.path.exists(save_path):\n",
        "                    os.makedirs(save_path)\n",
        "\n",
        "                # Save checkpoints\n",
        "                torch.save(netG.state_dict(), os.path.join(save_path, f'netG.pt'))\n",
        "                torch.save(netD.state_dict(), os.path.join(save_path, f'netD.pt'))\n",
        "\n",
        "                # Save optimizers\n",
        "                torch.save(optimizerG.state_dict(), os.path.join(save_path, f'optG.pt'))\n",
        "                torch.save(optimizerD.state_dict(), os.path.join(save_path, f'optD.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcS521DQv1qR"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAe_iAmFq6Im"
      },
      "outputs": [],
      "source": [
        "# to start TensorBoard run the following command\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHxWlVIZq2T8"
      },
      "source": [
        "## Run\n",
        "\n",
        "Unfortunately, the model is too big to train on google colab. So, instead we will just test our train loop on 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABe1AnmKqHUb"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "netG = Generator().to(Config.device)\n",
        "netD = Discriminator().to(Config.device)\n",
        "\n",
        "optimizerD = optim.RMSprop(netD.parameters(), lr=Config.learning_rate)\n",
        "optimizerG = optim.RMSprop(netG.parameters(), lr=Config.learning_rate)\n",
        "\n",
        "\n",
        "train(netG, netD, optimizerD, optimizerG, dataloader, num_epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkRO4QzFyucU"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "Training for 1 epochs and 1583 steps\n",
        "Epoch: 0/1, Step: 50/1583, Level:0.000000, G Loss:0.000052, D Loss:0.000222\n",
        "Epoch: 0/1, Step: 100/1583, Level:0.000000, G Loss:0.000047, D Loss:0.000011\n",
        "Epoch: 0/1, Step: 150/1583, Level:0.000000, G Loss:0.000057, D Loss:0.000010\n",
        "Epoch: 0/1, Step: 200/1583, Level:0.000000, G Loss:0.000068, D Loss:0.000006\n",
        "Epoch: 0/1, Step: 250/1583, Level:0.000000, G Loss:0.000078, D Loss:-0.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO3kfpZZw5aS"
      },
      "source": [
        "# 5 - Finish\n",
        "\n",
        "Congratulations! Go ahead, and submit the labe to `@DRU_Bot`. If you have any questions, DM `@DRU_Team` and we will help you!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "deeplearning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}