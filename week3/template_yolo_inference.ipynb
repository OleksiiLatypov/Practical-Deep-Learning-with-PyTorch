{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/week3/template_yolo_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svvsr46KmQqg-gh"
      },
      "source": [
        "# You Only Look Once Algorithm (YOLO)\n",
        "\n",
        "Welcome! In this lab, you will have an opportunity to implement some parts of the YOLO algorithm. In the ResNet18 lab, we learned how to use a pre-trained model for image classification. Now, we will learn how to classify multiple objects into n classes and find a bounding box around each object. In short, we will implement an image detection algorithm. Let's get started!\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- How to make predictions with YOLOv3 algorithm\n",
        "    - How to preprocess an image\n",
        "    - How to filter boxes by their confidence\n",
        "    - How to use Non-maximum Suppression (NMS)\n",
        "    - How to visualize the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7S7w0rm5aq1"
      },
      "source": [
        "# 0 - Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuMU0clK5aq2",
        "outputId": "edfde52f-8284-4794-b9c5-7239575339b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=0d3019e9bee4d20ff1babdb7ee731fbc878eacb1641b3c3636276e63b263af25\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/3_conv_nn/yolo_inference/template.zip')\n",
        "!unzip -q template.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brHvab3XqSM4"
      },
      "source": [
        "# 1 - Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WrWAhX3FEuS"
      },
      "source": [
        "In addition to our regular packages, we will use [Pillow](https://pillow.readthedocs.io/en/stable/) — a common imaging library for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sZ2ZI6Cq9CeQ"
      },
      "outputs": [],
      "source": [
        "# local imports\n",
        "from models import Darknet\n",
        "\n",
        "# packages\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX3miXz3FQIa"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cbn_MNul9SqH"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] Config\n",
        "class Config:\n",
        "    # path\n",
        "    config_path = 'config/yolov3.cfg'\n",
        "    weights_path = 'yolov3.weights'\n",
        "    classes_path = 'config/coco.names'\n",
        "    image_path = 'zoo.png'\n",
        "\n",
        "    # model\n",
        "    img_size = 416\n",
        "    conf_thresh = 0.2\n",
        "    nms_thresh = 0.4\n",
        "    seed = 21\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4rDbY8zk5aq4"
      },
      "outputs": [],
      "source": [
        "random.seed(Config.seed)\n",
        "np.random.seed(Config.seed)\n",
        "torch.manual_seed(Config.seed);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbyv11xvWbMg"
      },
      "source": [
        "## Load pretrained model for inference\n",
        "\n",
        "In this lab we used a pretrained model from [cfotache](https://github.com/cfotache/pytorch_objectdetecttrack.git) implemetation of YOLO_v3 algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rBrB6JqvrRxn"
      },
      "outputs": [],
      "source": [
        "with open(Config.classes_path, \"r\") as fp:\n",
        "    class_labels = fp.read().split(\"\\n\")[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KmwlC66g7Rlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86afc1da-5de2-454b-cd95-05970de605bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "# Load model and weights\n",
        "model = Darknet(Config.config_path, img_size=Config.img_size)\n",
        "model.load_weights(Config.weights_path)\n",
        "model.to(Config.device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqLNtk9a5aq5"
      },
      "source": [
        "# 2 - Theory behind YOLO_v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bd1Oeeb5aq5"
      },
      "source": [
        "## Underlying model\n",
        "The model that we just imported is a core of the YOLO algorithm. The key property of this model is that it's **fully convolutional** (doesn't use linear layers), meaning that it can operate with images of any size. The **stride** (reduction factor) of this neural network is 32, meaning that the input image of the shape `[1, 3, 416, 416]` will be transformed to the size of `[1, num_features, 13, 13]`, where $1$ is the batch size, and `num_features` is an output number of channels. We can also interpret the output of a **fully convolutional** network as if it makes a prediction for each 13x13 section of the original image.\n",
        "\n",
        "An updated version of the YOLO algorithm, `YOLO_v3`, makes predictions across three different scales. So, instead of just sending an image through an entire neural network of a stride 32, `YOLO_v3` has two additional output heads in different parts of the architecture that reduce the image by a factor of 16 and 8, respectively. To sum up, if we have an input image of shape `[1, 3, 416, 416]`, the raw output will be three tensors of shapes `[1, num_features, 13, 13]`, `[1, num_features, 26, 26]`, and `[1, num_features, 52, 52]`. **Note** that each output has the same `num_features`, which is achieved by using 1x1 convolutions to give the output a fixed number of dimensions.\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/yolo_3_scales.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwA9_Viy5aq5"
      },
      "source": [
        "## How to interpret `num_features`?\n",
        "\n",
        "So, how do we know what's happening in each section of our image? In the simplest case, for each section, YOLO will output an **85** dimensional vector that would define our box. By our configuration, first **4** values will represent absolute x and y coordinates of a center, width, and height that we will label $b_x$, $b_y$, $b_w$, and $b_h$ respectively. **5**th value will represent a probability of having a center of **ANY** object in the box, $p_c$, and the remaining values will be probabilities of each individual class $p_i$ (where $i\\in[1, n_{classes}])$ that we are trying to detect. Since the **COCO** dataset has 80 classes, there will be 80 values for class probabilities. This leaves us with an **85** dimensional vector that describes one box prediction. In reality, however, there might be more than one object in each section of an image. So, instead, YOLO_v3 predicts three boxes for each section of an image, resulting in $3 * 85 = 255$ dimensional vector for each section.  \n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/box_vector.png\">\n",
        "\n",
        "**Note that in YOLO only the cell containing the center of the object is responsible for prediction.** So, if the center of the object is located in the section, that section should have $p_c\\approx1$. While, the nearby sections should predict $p_c\\approx0$, meaning that the object center was not detected, even if the object itself spans across several boxes. But which of the three boxes should detect our object? To answer this question, we need to understand the concept of **anchor** boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU6F1BG-5aq5"
      },
      "source": [
        "## Anchors\n",
        "\n",
        "Imagine you are building a dataset of target labels for some images. If you have two people $A$ and $B$ in a single section of the image, how do you form a target label for your prediction using your three boxes? If you put $A$ into the first box, $B$ into the second, and no-object into the third, that will be very different from putting $B$ into the first box, no-object into the second, $A$ into the third. The reason for that is that boxes are **NOT order invariant,** meaning that the order in which you place your predictions into three target boxes matters for a Neural Network. So, how do you come up with a system to order the objects in the boxes?\n",
        "\n",
        "YOLO developers came up with a solution - **anchor boxes**. Anchor box is simply two predefined numbers $(w, h)$ that represent the width and height of a box used for comparison. In config, we define three anchor boxes for each scale of the output image.\n",
        "\n",
        "| Scale | Anchors                        |\n",
        "|-------|--------------------------------|\n",
        "| 13x13 | (10,13), (16,30), (33,23)      |\n",
        "| 26x26 | (30,61), (62,45), (59,119)     |\n",
        "| 52x52 | (116,90), (156,198), (373,326) |\n",
        "\n",
        "Then, to determine what object goes where, we simply compare the actual height and width of the object to the height and width of the anchor. If, for example, our object has dimensions $(12, 12)$, we will place it into the first box because anchor $(10, 13)$ is the closest to our prediction. More concretely, we will compute **Intersection Over Union(IoU)** between our object and three anchor boxes to decide which of the three boxes to use for our object.\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/anchors.png\">\n",
        "\n",
        "This setting for target labels will force our model to predict three boxes of different parameters. For example, in a $13x13$ resolution, first box (corresponds to the $(10,13)$ anchor) will predict roughly square objects, second box (anchor $(16, 30)$) — tall objects, and the last box ($(33, 23)$ anchor) — wide objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_9z3XOs5aq5"
      },
      "source": [
        "## Restrictions on the output vector\n",
        "\n",
        "Remember that all these settings are made so far on our **target labels**. The model itself doesn't know what we are trying to learn, so it will simply output **real-valued** vector. Therefore, we need to set some restrictions on the output vector.\n",
        "\n",
        "Let's denote target box-vector using $[b_x, b_y, b_w, b_h, p_c, p_1, p_2, \\dots , p_{80}]$, where:\n",
        "* $b_x\\in(0, 416)$ - absolute $x$ coordinate of the image center measured from top-left corner\n",
        "* $b_y\\in(0, 416)$ - absolute $y$ coordinate of the image center measured from top-left corner\n",
        "* $b_w\\in(0, 416)$ - width of the object (assuming an object is fully present on the image)\n",
        "* $b_h\\in(0, 416)$ - height of the object\n",
        "* $p_c\\in(0, 1)  $ - probability of detecting an object\n",
        "* $p_i\\in(0, 1)  $ - probability of the i-th class on the image\n",
        "\n",
        "**Restrictions:**\n",
        "* we want all of our predicted values to be in appropriate ranges\n",
        "* $b_x$ and $b_y$ must be inside of a given box\n",
        "* $b_w$ and $b_h$ must be as close as possible to a given anchor box $(A_w, A_h)$.\n",
        "\n",
        "The output vector from Neural Network is $[t_x, t_y, t_h, t_w, t_c, t_1, t_2, \\dots , t_{80}]$, where $t_{j}\\in(-\\infty, \\infty)$. With this huge range, the task of learning our target labels becomes incredibly difficult for neural network. To resolve this problem, we define the following postprocessing steps for the output vector:\n",
        "\n",
        "\n",
        "$$b_x = \\sigma(t_x) + c_x \\tag{1}$$\n",
        "$$b_y = \\sigma(t_y) + c_y \\tag{2}$$<br>\n",
        "$$b_w = p_w\\cdot e^{t_w} \\tag{3}$$\n",
        "$$b_h = p_h\\cdot e^{t_h} \\tag{4} $$<br>\n",
        "$$p_c = \\sigma(t_c) \\tag{5}$$\n",
        "$$p_i = \\sigma(t_i) \\tag{6}$$\n",
        "\n",
        "where $c_x$ and $c_y$ are the top-left co-ordinates of the grid, and $(p_w, p_h)$ is an anchor box.\n",
        "\n",
        "So, for all probabilities, we simply apply sigmoid activation (equations 5, 6). For $b_x$ and $b_y$, we first apply sigmoid on the output to normalize it in the range (0, 1), and then add a co-ordinate of the top-left corner of the section where we are located. This way, we ensure that we will always stay inside of the section. Finally, for the height and width of each box, we exponentiate the value of $t_w$ and $t_h$ and only then multiply it by the size of the anchor box. If $t_w$ is a number close to 0, the value of the $b_w$ will be close to the width of the anchor box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw483svG5aq6"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now, after our post-processing steps (1-6) it will be fair to compare the output of a neural network to the target labels. Let's define some notation shortucts:\n",
        "\n",
        "* BCE(target, predicted) - Binary Cross Entropy Loss\n",
        "* CE(target, predicted) - Cross Entropy Loss\n",
        "* MSE(target, predicted) - Mean Square Error Loss\n",
        "* $\\textbf{b} = [b_x, b_y, b_w, b_h]$\n",
        "* $\\textbf{p} = [p_1, p_2, \\dots, p_c]$\n",
        "\n",
        "\n",
        "If target $p_c = 0$ (there is no object in the section) we don't care about anything other than $p_c$:\n",
        "\n",
        "$$Loss(box_{predicted}, box_{target}) = BCE(0, \\hat{p_c}) = - \\log{(1-\\hat{p_c})}$$\n",
        "\n",
        "if target $p_c = 1$:\n",
        "\n",
        "$$Loss(box_{predicted}, box_{target}) = BCE(1, \\hat{p_c}) + MSE(\\textbf{b},\\hat{\\textbf{b}}) + CE(\\textbf{p},\\hat{\\textbf{p}})$$\n",
        "\n",
        "Or to put it together:\n",
        "\n",
        "$$Loss(box_{predicted}, box_{target}) = BCE(p_c, \\hat{p_c}) + p_c\\cdot MSE(\\textbf{b},\\hat{\\textbf{b}}) + p_c\\cdot CE(\\textbf{p},\\hat{\\textbf{p}})$$\n",
        "\n",
        "Then we backpropagate through our model and optimize our weights. Easy!!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5-vfn3P5aq6"
      },
      "source": [
        "## To sum up\n",
        "\n",
        "Let's say you trained your **fully convolutional** neural network, and you want to see what it outputs. If you feed in a $416x416$ image, you will get outputs at three different scales ($13x13$, $26x26$, and $52x52$):\n",
        "\n",
        "We will generate three boxes for each section of the resulting output (using three pre-defined **anchors**). Finally, each box will contain a prediction, which is a vector of size 85.\n",
        "\n",
        "So, for each scale the output shape of the network will be ($1$ denotes a batch size):\n",
        "* $13x13$: `[1, 3 * 85, 13, 13]`\n",
        "* $26x26$: `[1, 3 * 85, 26, 26]`\n",
        "* $52x52$: `[1, 3 * 85, 52, 52]`\n",
        "\n",
        "If you recall part 1 of our explanation this is the same thing as `[1, num_features, 13, 13]`, `[1, num_features, 26, 26]`, and `[1, num_features, 52, 52]`, given that `num_features = 3 * 85 = 255`.\n",
        "\n",
        "Next, we will flatten last two dimensions and concatenate the outputs:\n",
        "* `[1, 3 * 85, 13, 13]` => `[1, 3 * 85, 169]`\n",
        "* `[1, 3 * 85, 26, 26]` => `[1, 3 * 85, 676]`\n",
        "* `[1, 3 * 85, 52, 52]` => `[1, 3 * 85, 2704]`\n",
        "* =============== Concatenate ===============\n",
        "* result = `[1, 3 * 85, 3549]`\n",
        "\n",
        "Or, we can also separate the dimension of our boxes and get a final shape of:\n",
        "* final = `[1, 10647, 85]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQZZxO9G5aq6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    fake_img = torch.randn((1, 3, 416, 416)).to(Config.device)\n",
        "    print('Final shape:', model(fake_img).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YiUdYUP5aq6"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>shape:</b></td>\n",
        "      <td>torch.Size([1, 10647, 85])</td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxoEFOqeGOxR"
      },
      "source": [
        "# 3 - YOLO_v3 inference\n",
        "\n",
        "Yay! We finally produced 10647 of boxes for a single image. What do we do next with all of those boxes? If we visualize all of them we will get a resulting image that looks something like this:\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/all_boxes.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "847Bc27q5aq6"
      },
      "source": [
        "Our image is a little overcrowded, to say the least! To filter boxes that we don't need, we will apply two methods:\n",
        "* Confidence thresholding\n",
        "* Non Maximum Supression\n",
        "\n",
        "But before we implement them, let's import the input image that we will be working with and apply some preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wteYoM5aq6"
      },
      "source": [
        "## Load the image example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkzYgEZGHsAD"
      },
      "outputs": [],
      "source": [
        "def show(img, title=None):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGMwxMa5HK8n"
      },
      "outputs": [],
      "source": [
        "img = Image.open(Config.image_path)\n",
        "show(img, title=f'PIL Image with width={img.size[0]} and height={img.size[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joZGBYmeW0rj"
      },
      "source": [
        "# 4 - Helper functions\n",
        "\n",
        "Before we start implementing our filtering methods, we need to define some utility functions. We will implement:\n",
        "* `adaptive_resize` that resizes input image preserving the aspect ratio.\n",
        "* `convert_boxes` that perform conversion of box coordinates.\n",
        "* `iou` that measures Intersection Over Union between two tensors of boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiYme23a5aq7"
      },
      "source": [
        "## Note about coordinates\n",
        "\n",
        "In this lab we will follow standard numpy convention. Using upper-left corner of the image as the origin $(0, 0)$, increasing x coordinates to the right, and y coordinates to the bottom. So, if we compare **upper left corner** of the box that has coordinates $(x_1, y_1)$ and **bottom right corner** with coordinates $(x_2, y_2)$, $x_1$ would be smaller than $x_2$, and $y_1$ would be smaller than $y_2$ Also, in `numpy` and `torch` image size is in (height, width) format, while in `PIL` image has size of (width, height).\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/coordinates.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LynTHwMdqx5F"
      },
      "source": [
        "## Adaptive resize funtion\n",
        "\n",
        "The input image to the model should be a square with the size defined in `Config`. However, we also don't want to lose the original ratio of the image and make an elephant out of zebra by using a standard resize. For this purpose, we will implement an **adaptive resize function** that resizes the image, preserving the original ratio of the image dimensions, and then pads by pixels with values $(128, 128, 128)$ to get the desired size.\n",
        "\n",
        "**Excercise:** implement `adaptive_recise` function:\n",
        "* calculate resize parameters `new_h` and `new_w` before padding\n",
        "    * both `new_h` and `new_w` has to be <= `size`\n",
        "    * when calculating the number of pixels, don't round down!\n",
        "        * Use `round()` instead of `int()`\n",
        "* determine the number of pixels to pad from left, top, right, and bottom\n",
        "    * if the image is wide, padding should be equally distributed between top and bottom\n",
        "    * if the image is tall, padding should be equally distributed between right and left\n",
        "    * if the padding amount is not divisible by 2, you should give an extra pixel to the left or top, depending on the image ratio\n",
        "* use `transforms.Resize((height, width))` to resize the image\n",
        "* use `transforms.Pad` to padd the image (padding value is `(128, 128, 128)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdMlS7aEGYwh",
        "outputId": "8f873afb-cf3c-499e-ea86-eb5b56d0ca27"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3953584564.py, line 25)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/ft/r6f6z8990w76n_c6qbdds39m0000gp/T/ipykernel_31014/3953584564.py\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    new_h =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# VALIDATION_FIELD[func] adaptive_resize\n",
        "\n",
        "def adaptive_resize(img, size=Config.img_size, fill=(128, 128, 128)):\n",
        "    \"\"\"\n",
        "    Converts PIL image from (w, h) to (size, size),\n",
        "    preserving aspect ratio and padding the needed dimensions\n",
        "    by pixels with values (128, 128, 128)\n",
        "\n",
        "    Arguments:\n",
        "    img - PIL.Image.Image, input image\n",
        "    size - int, desired size of the output image\n",
        "    fill - tuple, value of pixels to fill with\n",
        "\n",
        "    Returns:\n",
        "    padded - PIL.Image.Image, output resized image\n",
        "    resize_params - dict, containing:\n",
        "                    (h, w): original size,\n",
        "                    (new_h, new_w): resized size before padding,\n",
        "                    padding: tuple of number of pixels to padd from left, top, right and bottom (in that order)\n",
        "    \"\"\"\n",
        "    w, h = img.size\n",
        "    ### START CODE HERE ### (≈12 lines of code)\n",
        "    # calculate the dimensions of a resized image (before padding)\n",
        "    new_h =\n",
        "    new_w =\n",
        "\n",
        "    # determine the bigger side\n",
        "    max_wh =\n",
        "\n",
        "    # calculate the amount of padding\n",
        "    h_padding =\n",
        "    w_padding =\n",
        "\n",
        "    # add one pixel to left and top in case if totall padding is odd\n",
        "    l_pad =\n",
        "    t_pad =\n",
        "    r_pad =\n",
        "    b_pad =\n",
        "    padding = (int(l_pad), int(t_pad), int(r_pad), int(b_pad))\n",
        "\n",
        "    # use torch.transforms to transform the image\n",
        "    resized =\n",
        "    padded =\n",
        "    ### END CODE HERE ###\n",
        "    resize_param = {\n",
        "        'original': (h, w),\n",
        "        'resized': (new_h, new_w),\n",
        "        'padding': padding\n",
        "    }\n",
        "    return padded, resize_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl005F4RMBgn"
      },
      "outputs": [],
      "source": [
        "resized_img, resize_params = adaptive_resize(img)\n",
        "print('original size:', resize_params['original'])\n",
        "print('resized size:', resize_params['resized'])\n",
        "print('padding:', resize_params['padding'])\n",
        "show(resized_img, title=f'Rescaled PIL Image with width={resized_img.size[0]} and height={resized_img.size[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAKiLePO5aq8"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>original:</b></td>\n",
        "      <td>(4065, 6094)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>resized:</b></td>\n",
        "      <td>(277, 416)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>padding:</b></td>\n",
        "      <td>(0, 70, 0, 69)</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "resized_img, resize_params = adaptive_resize(img.transpose(Image.ROTATE_90))\n",
        "print('original size:', resize_params['original'])\n",
        "print('resized size:', resize_params['resized'])\n",
        "print('padding:', resize_params['padding'])\n",
        "show(resized_img, title=f'Rescaled PIL Image with width={resized_img.size[0]} and height={resized_img.size[1]}')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "g42C5E4c5aq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>original:</b></td>\n",
        "      <td>(6094, 4065)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>resized:</b></td>\n",
        "      <td>(416, 277)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>padding:</b></td>\n",
        "      <td>(70, 0, 69, 0)</td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "collapsed": false,
        "id": "8yyrYsf-5aq8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi0_-9jK5aq8"
      },
      "source": [
        "### Convert Boxes\n",
        "\n",
        "**Excercise:** implement `convert_boxes` that converts a tensor of boxes from format (x center, y center, width, height) to (x1, y1, x2, y2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSRxLSMCkdV6"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] convert_boxes\n",
        "\n",
        "def convert_boxes(boxes):\n",
        "    \"\"\"\n",
        "    Converts boxes from (x center, y center, width, height) to\n",
        "    (x1, y1, x2, y2) where x1, y1 is an upper left corner\n",
        "    and x2, y2 is a lower right corner\n",
        "\n",
        "    Arguments:\n",
        "    boxes - torch.Tensor, input boxes of shape [batch_size, 4]\n",
        "\n",
        "    Returns:\n",
        "    new_boxes - torch.Tensor, boxes in x1y1x2y2 format of shape [batch_size, 4]\n",
        "    \"\"\"\n",
        "    new_boxes = boxes.clone()\n",
        "    ### START CODE HERE ### (≈4 lines of code)\n",
        "    new_boxes[:,0] =\n",
        "    new_boxes[:,1] =\n",
        "    new_boxes[:,2] =\n",
        "    new_boxes[:,3] =\n",
        "    ### END CODE HERE ###\n",
        "    return new_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncOHZwIa5aq8"
      },
      "outputs": [],
      "source": [
        "boxes = torch.tensor([[5, 5, 2, 2],[1, 2, 1, 1]]).float()\n",
        "print('converted:', convert_boxes(boxes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnKHKNNY5aq8"
      },
      "source": [
        "**Expected Output:**\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>converted</b></td>\n",
        "        <td>tensor([[4.0000, 4.0000, 6.0000, 6.0000],<br>\n",
        "        [0.5000, 1.5000, 1.5000, 2.5000]])</td>\n",
        "    </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHSGyl4o5aq8"
      },
      "source": [
        "### Intersection over Union (iou)\n",
        "\n",
        "Finally, the last function we need to implement is `iou` or **Intersection over Union**. The diagram below shows how to calculate this value:\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/IoU.png\">\n",
        "\n",
        "**Excercise:** implement intersection over union between a comparison box `box1` and a batch of boxes `box2`.\n",
        "* Make sure that your function supports parallelization (performs computations without for-loops)!\n",
        "    * `box1` is a comparison box of shape [1, 4]\n",
        "    * `box2` is a **batch** of boxes to compare with `box1`\n",
        "* Also, be careful with the 0-division. If the union area of two boxes is 0, you should assign `iou = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF3Ydcxvq-Co"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] iou\n",
        "\n",
        "def iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "\n",
        "    The function has to support a batched input.\n",
        "    So, it should work when box1 has a shape of [1, 4]\n",
        "    and box2 has a shape [15, 4], for example.\n",
        "\n",
        "    Arguments:\n",
        "    box1 - torch.Tensor of shape [1, 4] of boxes in x1y1x2y2 format\n",
        "    box2 - torch.Tensor of shape [batch_size, 4] of boxes x1y1x2y2 format\n",
        "\n",
        "    Returns:\n",
        "    iou - torch.Tensor of shape [batch_size] that contains iou predictions\n",
        "    \"\"\"\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # get the coordinates of the intersection rectangle (≈ 4 lines of code)\n",
        "    inter_rect_x1 =\n",
        "    inter_rect_y1 =\n",
        "    inter_rect_x2 =\n",
        "    inter_rect_y2 =\n",
        "\n",
        "     # Intersection area (≈ 3 lines of code)\n",
        "    inter_width =\n",
        "    inter_height =\n",
        "    inter_area =\n",
        "\n",
        "    # Union Area (≈ 3 lines of code)\n",
        "    b1_area =\n",
        "    b2_area =\n",
        "    union_area =\n",
        "    iou =\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # replace nan with 0\n",
        "    iou[torch.isinf(iou)] = 0\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8_h_Kh55aq9"
      },
      "outputs": [],
      "source": [
        "box1 = torch.tensor([[1, 2, 3, 4]]).float()\n",
        "box2 = torch.tensor([[1, 2, 3, 4],\n",
        "                     [2, 1, 4, 3],\n",
        "                     [5, 6, 7, 8],\n",
        "                     [1, 4, 2, 5]]).float()\n",
        "\n",
        "print('IoU:', iou(box1, box2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htW26MDf5aq9"
      },
      "source": [
        "**Expected Output:**\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>IoU</b></td>\n",
        "        <td>tensor([1.0000, 0.1429, 0.0000, 0.0000])</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MvR8skxWg9p"
      },
      "source": [
        "# 5 - Filtering\n",
        "### Probability score filtering\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/probability_score_filtering.png\">\n",
        "\n",
        "The first filtering tecnhique we will be using is called **score** filtering. To use this filtering method we first need to compute the joint probabilty that the detected object is of the given class:\n",
        "$$P(\\text{object} \\cap \\text{class[i]}) = P(\\text{object}) \\cdot P(\\text{class[i]}) = p_c * p_i$$\n",
        "\n",
        "Then using these probabilities, we can:\n",
        "1. find the most likely class candidate for a given box\n",
        "2. filter out unlikely boxes whose joint probability score is less than a threshold value\n",
        "\n",
        "**Excercise:** Implement confidence thresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZEpGWXclOic"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] filter_by_confidence\n",
        "\n",
        "def filter_by_confidence(boxes, p_c_prob, class_prob, conf_thresh=Config.conf_thresh):\n",
        "    \"\"\"\n",
        "    Filters boxes by confidence\n",
        "\n",
        "    Arguments:\n",
        "    boxes - torch.tensor of shape [batch, 4] boxes in the format [x_center, y_center, width, height]\n",
        "    p_c_prob - torch.tensor of shape [batch, 1] probabilities of an object\n",
        "    class_prob - torch.tensor of shape [batch, 80]\n",
        "\n",
        "    Returns:\n",
        "    boxes - torch.tensor of shape [filtered, 4] filtered boxes\n",
        "    box_scores - torch.tensor of shape [filtered, 1] joint proboability scores of boxes\n",
        "    box_classes - torch.tensor of shape [filtered, 1] the most likely class index for a box\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈3 lines of code)\n",
        "    # calculate joint probability p_c * p_i\n",
        "    scores =\n",
        "    # find the class with maximum probability for each box using scores\n",
        "    box_scores, box_classes =\n",
        "    # create a filter for removing unlikely boxes using threshold\n",
        "    mask =\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # apply mask\n",
        "    boxes = boxes[mask]\n",
        "    box_scores = box_scores[mask]\n",
        "    box_classes = box_classes[mask]\n",
        "    return boxes, box_scores, box_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDo_3CYe5aq9"
      },
      "outputs": [],
      "source": [
        "boxes = torch.tensor([[ 8.,  9., 16., 18.],\n",
        "                      [ 4.,  6.,  8., 12.],\n",
        "                      [ 6.,  7., 12., 14.],\n",
        "                      [ 2.,  9.,  4., 18.],\n",
        "                      [ 8.,  3., 16.,  6.]])\n",
        "p_c_prob = torch.tensor([[0.4058],[0.8206],[0.2455],[0.2990],[0.0327]])\n",
        "# for testing we will use `num_classes=8`\n",
        "class_prob = torch.tensor([[0.1671, 0.8980, 0.5258, 0.9354, 0.9485, 0.3799, 0.0241, 0.2106],\n",
        "                           [0.8294, 0.4328, 0.6154, 0.0256, 0.5209, 0.1788, 0.7997, 0.4718],\n",
        "                           [0.8507, 0.1390, 0.2791, 0.3575, 0.6461, 0.3769, 0.6058, 0.1949],\n",
        "                           [0.5336, 0.9761, 0.8537, 0.6989, 0.2038, 0.5689, 0.5464, 0.1366],\n",
        "                           [0.0486, 0.0660, 0.3141, 0.8351, 0.5842, 0.9983, 0.4926, 0.5212]])\n",
        "boxes, box_scores, box_classes = filter_by_confidence(boxes, p_c_prob, class_prob, conf_thresh=0.35)\n",
        "print('boxes:', boxes)\n",
        "print('scores:', box_scores)\n",
        "print('class index:', box_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85d5wYC05aq-"
      },
      "source": [
        "**Expected Output:**\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>boxes</b></td>\n",
        "        <td>tensor([[ 8.,  9., 16., 18.], <br>\n",
        "        [ 4.,  6.,  8., 12.]]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>scores</b></td>\n",
        "        <td>tensor([0.3849, 0.6806])</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>class index</b></td>\n",
        "        <td>tensor([4, 0])</td>\n",
        "    </tr>               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l1QeuZTW4-I"
      },
      "source": [
        "## Non-Max Suppression filtering\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/nms_filtering.png\">\n",
        "\n",
        "After applying confidence thresholding, we are left with ~ 20 boxes when the actual number of objects is 3! The problem here is that all of them pretty accurately describe the location of the object. The solution is simple: find the most likely candidates, and remove the ones that have a high overlap. This is basically the whole idea behind **Non-Max Suppression** algorithm.\n",
        "\n",
        "More concretely, to implement the Non-Max Suppression Algorithm, we need to do the following:\n",
        "1. Iterate over each unique class `c` in the predictions:\n",
        "2. Select a subset of boxes and scores that correspond to class `c`\n",
        "3. Sort this subset of boxes (in descending order) by detection scores\n",
        "4. Iterate until we have items in the subset:\n",
        "    1. Save first most likely box\n",
        "    2. Compute `iou` between the first most likely box and the rest\n",
        "    3. Filter out remaining boxes by `nms_threshold`\n",
        "    4. Remove first most likely box from the subset\n",
        "5. Return selected boxes, their scores, and class indices\n",
        "\n",
        "**Excercise:** implement Non-Max Supression algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh3keArgptBC"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] non_max_supression\n",
        "\n",
        "def non_max_supression(boxes, box_scores, box_classes, nms_thres=Config.nms_thresh):\n",
        "    \"\"\"\n",
        "    Performs Non-Max Suppression Algorithm\n",
        "\n",
        "    Arguments:\n",
        "    boxes - torch.tensor of shape [batch, 4] boxes in the x1y1x2y2 format\n",
        "    box_scores - torch.tensor of shape [batch, 1] joint proboability scores of boxes\n",
        "    box_classes - torch.tensor of shape [batch, 1] the most likely class index for each box\n",
        "\n",
        "    Return:\n",
        "    boxes - torch.tensor of shape [filtered, 4] filtered boxes in the x1y1x2y2 format\n",
        "    box_scores - torch.tensor of shape [filtered, 1] joint proboability scores of boxes\n",
        "    box_classes - torch.tensor of shape [filtered, 1] the most likely class index for each box\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    unique_labels =\n",
        "    for c in unique_labels:\n",
        "        # choose only member of class c\n",
        "        detection_boxes =\n",
        "        detection_scores =\n",
        "        # sort boxes by scores\n",
        "        sort_index =\n",
        "        detection_boxes =\n",
        "        detection_scores =\n",
        "        while detection_boxes.shape[0] > 1:\n",
        "            # save first most-probable box\n",
        "            predictions.append([detection_boxes[0], detection_scores[0], c])\n",
        "            # perform iou\n",
        "            iou_values =\n",
        "            # filter out remaining boxes using threshold\n",
        "            iou_filter =\n",
        "            detection_boxes  = detection_boxes[1:][iou_filter]\n",
        "            detection_scores = detection_scores[1:][iou_filter]\n",
        "        if detection_boxes.shape[0] > 0:\n",
        "            predictions.append([detection_boxes[0], detection_scores[0], c])\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    boxes = torch.stack([x[0] for x in predictions])\n",
        "    box_scores = torch.tensor([x[1] for x in predictions])\n",
        "    box_classes = torch.tensor([x[2] for x in predictions])\n",
        "    return boxes, box_scores, box_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcSIDIQO5aq-"
      },
      "outputs": [],
      "source": [
        "boxes = torch.tensor([[213.1915, 242.9775, 349.4505, 355.3651],\n",
        "                      [216.4065, 241.9519, 363.9741, 354.2513],\n",
        "                      [ 20.6423,   6.6963, 160.5648, 307.5111],\n",
        "                      [ 23.9501,  12.4610, 174.3551, 303.5546],\n",
        "                      [ 20.3151,  19.1298, 161.0345, 307.2794],\n",
        "                      [ 26.9205,  25.4599, 172.5603, 302.3964],\n",
        "                      [182.9737, 129.4792, 312.9923, 303.7195],\n",
        "                      [191.5784, 127.7021, 324.2952, 307.8519],\n",
        "                      [181.7884, 135.8694, 314.2904, 318.8308],\n",
        "                      [204.8123, 234.1404, 355.8813, 365.6572]])\n",
        "box_scores = torch.tensor([0.9979, 0.9601, 0.9871, 0.9520, 0.9966, 0.9800, 0.9936, 0.8047, 0.9679, 0.9608])\n",
        "box_classes = torch.tensor([22, 22, 23, 23, 23, 23, 23, 23, 23, 22])\n",
        "boxes, box_scores, box_classes = non_max_supression(boxes, box_scores, box_classes)\n",
        "print('filtered boxes:', boxes)\n",
        "print('corresponding scores:', box_scores)\n",
        "print('class index:', box_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIK9VmXz5aq-"
      },
      "source": [
        "**Expected Output:**\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>boxes</b></td>\n",
        "        <td>tensor([[213.1915, 242.9775, 349.4505, 355.3651], <br>\n",
        "                    [ 20.3151,  19.1298, 161.0345, 307.2794],<br>\n",
        "                    [182.9737, 129.4792, 312.9923, 303.7195]])</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>scores</b></td>\n",
        "        <td>tensor([0.9979, 0.9966, 0.9936])</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>class index</b></td>\n",
        "        <td>tensor([22, 23, 23])</td>\n",
        "    </tr>               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXjoNcaLIfL"
      },
      "source": [
        "# 5 - Putting everything together\n",
        "\n",
        "Let's put everything together into one algorithm\n",
        "\n",
        "**Excercise** implement predict function:\n",
        "\n",
        "1. Resize input `PIL` image\n",
        "2. Transform image to tensor (hint use `transforms`) and pass it to device\n",
        "3. Get an extra dimension to create a batch\n",
        "4. Push your image through the model\n",
        "5. Split detection vector into `boxes`, `p_c_prob`, and `class_prob`\n",
        "6. Perform confidence thresholding\n",
        "7. Convert boxes to x1y1x2y2 format\n",
        "8. Perform Non-Max Thresholding\n",
        "9. Return boxes, their scores, class id, and resize_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RnhKBVDLHM7"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] predict\n",
        "\n",
        "def predict(img, model, conf_thresh=Config.conf_thresh, nms_thres=Config.nms_thresh, device=Config.device):\n",
        "    \"\"\"\n",
        "    Performs YOLO_v3 prediction on the image\n",
        "\n",
        "    Arguments:\n",
        "    img - PIL.Image.Image, input PIL image\n",
        "    model - nn.Module, torch model for prediction (backbone of YOLO)\n",
        "\n",
        "    Return:\n",
        "    boxes - torch.tensor of shape [filtered, 4] filtered boxes\n",
        "    box_scores - torch.tensor of shape [filtered, 1] joint proboability scores of boxes\n",
        "    box_classes - torch.tensor of shape [filtered, 1] the most likely class index for each box\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈11 lines of code)\n",
        "    # resize image to the target size\n",
        "    img, resize_params =\n",
        "    # convert image to Tensor and pass it to device\n",
        "    image_tensor =\n",
        "    # create a batch of single image [batch_size, chanels, height, width]\n",
        "    image_batch =\n",
        "    # run inference on the model and get detections\n",
        "    with torch.no_grad():\n",
        "        detections =\n",
        "        # split detections\n",
        "        boxes =\n",
        "        p_c_prob =\n",
        "        class_prob =\n",
        "        # filter by confidence\n",
        "        boxes, box_scores, box_classes =\n",
        "        # preprocess boxes to x1y1x2y2 format\n",
        "        boxes =\n",
        "        # filter by non-max-supression\n",
        "        boxes, box_scores, box_classes =\n",
        "    ### END CODE HERE ###\n",
        "    return boxes, box_scores, box_classes, resize_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwoylgtpMJ_k"
      },
      "outputs": [],
      "source": [
        "boxes, box_scores, box_classes, resize_params = predict(img, model)\n",
        "print('boxes:', boxes)\n",
        "print('scores:', box_scores)\n",
        "print('class index:', box_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFOOGECM5aq-"
      },
      "source": [
        "**Expected Output:**\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>boxes</b></td>\n",
        "        <td>tensor([[328.0109, 206.3550, 393.1273, 268.4946], <br>\n",
        "        [146.6449, 214.6219, 217.2606, 272.8861],<br>\n",
        "        [218.7541, 242.6662, 324.1006, 326.5118],<br>\n",
        "        [ 38.9592, 100.0730, 118.3660, 248.0865],<br>\n",
        "        [130.5345, 163.0148, 197.8460, 244.1906]])</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>scores</b></td>\n",
        "        <td>tensor([0.9992, 0.9987, 0.3262, 0.9966, 0.9689])</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>class index</b></td>\n",
        "        <td>tensor([22, 22, 22, 23, 23])</td>\n",
        "    </tr>               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luFCIAO7Pj71"
      },
      "source": [
        "# 6 - Visualizing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdnitlMR5aq_"
      },
      "source": [
        "Next, after we extracted the needed boxes, we will need to see how they would look on the original image. To do so, we can use `parameters` that we saved in `resize_params` to resize the box to the original image scale.\n",
        "\n",
        "**Note:** Here we start with a box in $(x_1, y_1, x_2, y_2)$ format and convert it to $(x_1, y_1, \\text{width}, \\text{height})$ (standard `matplotlib` format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZcUNdUPzRvZ"
      },
      "outputs": [],
      "source": [
        "def resize_box(box, resize_params):\n",
        "    \"\"\"\n",
        "    Resizes box into the sclae of the original image, and converts to x1, y1, width, height format\n",
        "    \"\"\"\n",
        "    # unpacking inputs\n",
        "    x1, y1, x2, y2 = box\n",
        "    h_scale = resize_params['original'][0] / resize_params['resized'][0]\n",
        "    w_scale = resize_params['original'][1] / resize_params['resized'][1]\n",
        "\n",
        "    # resize height and width\n",
        "    box_h = (y2 - y1) * h_scale\n",
        "    box_w = (x2 - x1) * w_scale\n",
        "    y1 = (y1 - resize_params['padding'][1]) * h_scale\n",
        "    x1 = (x1 - resize_params['padding'][0]) * w_scale\n",
        "    return x1, y1, box_w, box_h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVfqbctW_mX-"
      },
      "outputs": [],
      "source": [
        "def visualize(boxes, scores, classes, resize_params, cmap='tab20b'):\n",
        "    fig, ax = plt.subplots(1, figsize=(12,9))\n",
        "    ax.imshow(img)\n",
        "    if boxes is not None:\n",
        "        # generate random colors\n",
        "        unique_labels = classes.unique()\n",
        "        cmap = plt.get_cmap(cmap)\n",
        "        colors = [cmap(i) for i in np.linspace(0, 1, len(boxes))]\n",
        "        bbox_colors = random.sample(colors, len(unique_labels))\n",
        "\n",
        "        # browse detections and draw bounding boxes\n",
        "        for box, conf, cls_pred in zip(boxes, scores, classes):\n",
        "            x1, y1, box_w, box_h = resize_box(box, resize_params)\n",
        "            color = bbox_colors[torch.nonzero(unique_labels == cls_pred, as_tuple=False).item()]\n",
        "            bbox = patches.Rectangle((x1, y1), box_w, box_h,\n",
        "                linewidth=2, edgecolor=color, facecolor='none')\n",
        "            ax.add_patch(bbox)\n",
        "            plt.text(x1, y1,\n",
        "                    s=class_labels[int(cls_pred)] + f' {conf:.3f}',\n",
        "                    color='white', verticalalignment='top',\n",
        "                    bbox={'color': color, 'pad': 0})\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9L07rXNJlvU"
      },
      "outputs": [],
      "source": [
        "boxes, scores, classes, resize_params = predict(img, model)\n",
        "visualize(boxes, scores, classes, resize_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W05oUY9-5aq_"
      },
      "source": [
        "Yay! As you can see, the model successfully identified all objects on the image. The zebra in the foreground has a confidence score of `0.326`, which is not surprising because it's partially covered by a trunk, which makes it harder for a Conv net to find the regular features of a zebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAPJioQFPzrt"
      },
      "source": [
        "# 7 - Try your own"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP574vNIZlKQ"
      },
      "outputs": [],
      "source": [
        "your_path = ''\n",
        "img = Image.open(your_path)\n",
        "boxes, scores, classes, resize_params = predict(img, model)\n",
        "visualize(boxes, scores, classes, resize_params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py397",
      "language": "python",
      "name": "py397"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}