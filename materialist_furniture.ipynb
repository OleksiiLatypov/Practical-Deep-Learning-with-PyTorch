{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8220,
          "databundleVersionId": 44216,
          "sourceType": "competition"
        },
        {
          "sourceId": 4174563,
          "sourceType": "datasetVersion",
          "datasetId": 2463428
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "materialist_furniture",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/materialist_furniture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'imaterialist-challenge-furniture-2018:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F8220%2F44216%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240510%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240510T194834Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5ed9ca77c4bbeee47c09ce6a7f9767d7c60ba18562e12fc4da7fa4aa38a9651874d4e6c172d87ad94761d7b02c7935080ce7115a3a1738653df1282e614328f77a15edc9d350033ed944f524a2ee389236f550322d2b877c4a7fdae3bbef2f6c4699078758d368cab264608880df1f037f0da4cfbca5c262de6acef63ab1da6849d05b602e989794e7d05ce9aa47dea77feaffd5a85b6f348613227a0403bf3e7cd0a17f90f4cc8ea3a51fdc65dae3c6418cba8a4fb12392347dc4ea5fb32bb98ed732e971f57d1cf73638fae7b3e01ecca04e768b9c533be62215396e40cc1b2ac361d5a78e41a7bc4420e35af04ee304fd449bdc205d936c6a5a9b8c98fc39,imaterialist-furniture-fgvc5:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2463428%2F4174563%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240510%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240510T194834Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4019bbdad841314ab2e69a7dca6c879fa5d95e0b41e31ba7a15388603f66a9ca22b7a29e368f22a0724734ac9e6357e878f303344900ff1c729a5c0cec056af85be987a5fb72f05d6714ca238df628dddd2299b3d72b3e0d5695a384b3a5202014a4713ddd30cde1677d1de3286a17d010ebaa42a2cd0f7fd15a445b64f69d4d9105c3caffea75375a3b6f4c3e38d89493d250711d70de8ceef90c61b7fbcd98504548f13e588626a2235aed2d5859eccc6ad3f1a00827ab549e473645aa5feea173e248d437765677a9a8093db3c0f42f5665fff0690e3b938d71929385ec9e1c5c95a0932d454b2deb941b0b4484bd255d02132705d8a9d08c1f771fa63f56'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "R1qTEBIHDOCx"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:38.225029Z",
          "iopub.execute_input": "2024-05-10T14:21:38.226147Z",
          "iopub.status.idle": "2024-05-10T14:21:38.235164Z",
          "shell.execute_reply.started": "2024-05-10T14:21:38.226103Z",
          "shell.execute_reply": "2024-05-10T14:21:38.233985Z"
        },
        "trusted": true,
        "id": "_D6gEigZDOCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "import random\n",
        "import json\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from multiprocessing.pool import ThreadPool as Pool\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import urllib3\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:40.69652Z",
          "iopub.execute_input": "2024-05-10T14:21:40.696923Z",
          "iopub.status.idle": "2024-05-10T14:21:40.709118Z",
          "shell.execute_reply.started": "2024-05-10T14:21:40.696892Z",
          "shell.execute_reply": "2024-05-10T14:21:40.707989Z"
        },
        "trusted": true,
        "id": "hmWUuhQMDOC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Training\n",
        "    epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    num_classes = 129\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # Data preparation\n",
        "    train_data_root = '/kaggle/input/imaterialist-challenge-furniture-2018/train.json'\n",
        "    test_data_root = '/kaggle/input/imaterialist-challenge-furniture-2018/test.json'\n",
        "    validation_data_root = '/kaggle/input/imaterialist-challenge-furniture-2018/validation.json'\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    anotation_file_root = '/kaggle/input/imaterialist-furniture-fgvc5/{}.csv'\n",
        "    images_path = '/kaggle/input/imaterialist-furniture-fgvc5/images/{}'\n",
        "    test_anotation_file_root = '/kaggle/working/test_df.csv'\n",
        "\n",
        "    # Data transformation\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std = (0.229, 0.224, 0.225)\n",
        "    resize_to = 256\n",
        "    img_size = 224"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:43.042432Z",
          "iopub.execute_input": "2024-05-10T14:21:43.042849Z",
          "iopub.status.idle": "2024-05-10T14:21:43.05092Z",
          "shell.execute_reply.started": "2024-05-10T14:21:43.042815Z",
          "shell.execute_reply": "2024-05-10T14:21:43.049776Z"
        },
        "trusted": true,
        "id": "zC8Xl6CoDOC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_json(Config.train_data_root)\n",
        "test_data = pd.read_json(Config.test_data_root)\n",
        "valid_data = pd.read_json(Config.validation_data_root)\n",
        "\n",
        "print(test_data.size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:45.679128Z",
          "iopub.execute_input": "2024-05-10T14:21:45.679978Z",
          "iopub.status.idle": "2024-05-10T14:21:47.285051Z",
          "shell.execute_reply.started": "2024-05-10T14:21:45.679943Z",
          "shell.execute_reply": "2024-05-10T14:21:47.283987Z"
        },
        "trusted": true,
        "id": "FzzwLcycDOC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:49.245231Z",
          "iopub.execute_input": "2024-05-10T14:21:49.245696Z",
          "iopub.status.idle": "2024-05-10T14:21:49.263784Z",
          "shell.execute_reply.started": "2024-05-10T14:21:49.245654Z",
          "shell.execute_reply": "2024-05-10T14:21:49.262595Z"
        },
        "trusted": true,
        "id": "bQargb92DOC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(data, test_data=False):\n",
        "    df = pd.DataFrame()\n",
        "    if test_data:\n",
        "        df['image_id'] = data.images.map(lambda x: x['image_id'])\n",
        "        df['url'] = data.images.map(lambda x: x['url'][0])\n",
        "    else:\n",
        "        df['image_id'] = data.annotations.map(lambda x: x['image_id'])\n",
        "        df['label_id'] = data.annotations.map(lambda x: x['label_id'])\n",
        "        df['url'] = data.images.map(lambda x: x['url'])\n",
        "    return df\n",
        "\n",
        "train_df = format_dataset(train_data)\n",
        "valid_df = format_dataset(valid_data)\n",
        "test_df = format_dataset(test_data, test_data=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:51.068401Z",
          "iopub.execute_input": "2024-05-10T14:21:51.069156Z",
          "iopub.status.idle": "2024-05-10T14:21:51.477983Z",
          "shell.execute_reply.started": "2024-05-10T14:21:51.06912Z",
          "shell.execute_reply": "2024-05-10T14:21:51.476731Z"
        },
        "trusted": true,
        "id": "tvbQJ9zrDOC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:53.571214Z",
          "iopub.execute_input": "2024-05-10T14:21:53.572245Z",
          "iopub.status.idle": "2024-05-10T14:21:53.577655Z",
          "shell.execute_reply.started": "2024-05-10T14:21:53.572207Z",
          "shell.execute_reply": "2024-05-10T14:21:53.57655Z"
        },
        "trusted": true,
        "id": "Xtrp7WDZDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:55.211448Z",
          "iopub.execute_input": "2024-05-10T14:21:55.211874Z",
          "iopub.status.idle": "2024-05-10T14:21:55.217027Z",
          "shell.execute_reply.started": "2024-05-10T14:21:55.211842Z",
          "shell.execute_reply": "2024-05-10T14:21:55.215992Z"
        },
        "trusted": true,
        "id": "aSRnWqrFDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('test_df.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:56.626165Z",
          "iopub.execute_input": "2024-05-10T14:21:56.626527Z",
          "iopub.status.idle": "2024-05-10T14:21:56.702285Z",
          "shell.execute_reply.started": "2024-05-10T14:21:56.626488Z",
          "shell.execute_reply": "2024-05-10T14:21:56.701079Z"
        },
        "trusted": true,
        "id": "rPg6WpOMDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_count = train_df.groupby(['label_id']).size().sort_values(ascending=False)\n",
        "class_count"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:21:58.435069Z",
          "iopub.execute_input": "2024-05-10T14:21:58.435447Z",
          "iopub.status.idle": "2024-05-10T14:21:58.451181Z",
          "shell.execute_reply.started": "2024-05-10T14:21:58.43542Z",
          "shell.execute_reply": "2024-05-10T14:21:58.450055Z"
        },
        "trusted": true,
        "id": "uPHV_n8DDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Grouping the data and getting counts\n",
        "class_count = train_df.groupby(['label_id']).size().reset_index(name='count')\n",
        "\n",
        "# Creating the count plot\n",
        "sns.displot(data=train_df['label_id'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:22:00.211889Z",
          "iopub.execute_input": "2024-05-10T14:22:00.212483Z",
          "iopub.status.idle": "2024-05-10T14:22:00.867747Z",
          "shell.execute_reply.started": "2024-05-10T14:22:00.21245Z",
          "shell.execute_reply": "2024-05-10T14:22:00.866588Z"
        },
        "trusted": true,
        "id": "j302KYU1DOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms(data_subset):\n",
        "    if data_subset == 'train':\n",
        "        transformations = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomResizedCrop((Config.img_size, Config.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(Config.mean, Config.std)\n",
        "        ])\n",
        "    else:\n",
        "        transformations = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((Config.img_size, Config.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(Config.mean, Config.std)\n",
        "        ])\n",
        "    return transformations"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:22:03.242758Z",
          "iopub.execute_input": "2024-05-10T14:22:03.243147Z",
          "iopub.status.idle": "2024-05-10T14:22:03.251549Z",
          "shell.execute_reply.started": "2024-05-10T14:22:03.243118Z",
          "shell.execute_reply": "2024-05-10T14:22:03.250109Z"
        },
        "trusted": true,
        "id": "7gNDjEeADOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(TensorDataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.shuffle()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, f'{self.img_labels.iloc[idx, 0]}.jpg')\n",
        "        image = read_image(img_path)\n",
        "        image = self.transform(image)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        return image, label\n",
        "\n",
        "    def shuffle(self):\n",
        "        self.img_labels = self.img_labels.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:39:54.819756Z",
          "iopub.execute_input": "2024-05-10T14:39:54.820108Z",
          "iopub.status.idle": "2024-05-10T14:39:54.828971Z",
          "shell.execute_reply.started": "2024-05-10T14:39:54.820082Z",
          "shell.execute_reply": "2024-05-10T14:39:54.827734Z"
        },
        "trusted": true,
        "id": "fkW4wPfwDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestImageDataset(TensorDataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        self.img_ids = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.img_ids.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, f'{self.img_ids.iloc[idx, 0]}.jpg')\n",
        "        try:\n",
        "            image = read_image(img_path)\n",
        "            image = self.transform(image)\n",
        "            return image, image_id\n",
        "        except:\n",
        "            return [], image_id"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:22:07.426698Z",
          "iopub.execute_input": "2024-05-10T14:22:07.427073Z",
          "iopub.status.idle": "2024-05-10T14:22:07.435892Z",
          "shell.execute_reply.started": "2024-05-10T14:22:07.427027Z",
          "shell.execute_reply": "2024-05-10T14:22:07.434384Z"
        },
        "trusted": true,
        "id": "Z33_EGnbDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import requests\n",
        "# print(train_df.iloc[4,2][0])\n",
        "# image = requests.get(train_df.iloc[4,2][0],  stream=True)\n",
        "# print(image.raise_for_status())\n",
        "# img_open = Image.open(image.raw).convert('RGB')\n",
        "# img_open"
      ],
      "metadata": {
        "trusted": true,
        "id": "Nucs6IxMDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = {\n",
        "    'test': test_df,\n",
        "    'train': train_df,\n",
        "    'validation': valid_df\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:22:10.485233Z",
          "iopub.execute_input": "2024-05-10T14:22:10.485698Z",
          "iopub.status.idle": "2024-05-10T14:22:10.515866Z",
          "shell.execute_reply.started": "2024-05-10T14:22:10.485662Z",
          "shell.execute_reply": "2024-05-10T14:22:10.514639Z"
        },
        "trusted": true,
        "id": "QCGQaPPfDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {}\n",
        "image_datasets = {}\n",
        "for name in ['train', 'validation', 'test']:\n",
        "    if name == 'test':\n",
        "        image_datasets[name] = TestImageDataset(Config.test_anotation_file_root,\n",
        "                                     Config.images_path.format(name),\n",
        "                                     transform=get_transforms(name))\n",
        "        dataloaders[name] = DataLoader(image_datasets[name],\n",
        "                                       shuffle=False,\n",
        "                                       num_workers=0)\n",
        "    else:\n",
        "        image_datasets[name] = CustomImageDataset(Config.anotation_file_root.format(name),\n",
        "                                     Config.images_path.format(name),\n",
        "                                     transform=get_transforms(name))\n",
        "        dataloaders[name] = DataLoader(image_datasets[name],\n",
        "                                       shuffle=True,\n",
        "                                       batch_size=Config.batch_size,\n",
        "                                       num_workers=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:39:57.961691Z",
          "iopub.execute_input": "2024-05-10T14:39:57.962069Z",
          "iopub.status.idle": "2024-05-10T14:39:58.048406Z",
          "shell.execute_reply.started": "2024-05-10T14:39:57.962041Z",
          "shell.execute_reply": "2024-05-10T14:39:58.047244Z"
        },
        "trusted": true,
        "id": "Gt4zk0D0DOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_array(img, mean = Config.mean, std = Config.std):\n",
        "    img = img.numpy()\n",
        "    np_img = np.transpose(img, (1, 2, 0))\n",
        "    np_img = np.multiply(np_img, Config.std) + Config.mean\n",
        "    np_img = np.clip(np_img, 0, 1)\n",
        "\n",
        "    return np_img"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:00.106265Z",
          "iopub.execute_input": "2024-05-10T14:40:00.107024Z",
          "iopub.status.idle": "2024-05-10T14:40:00.113492Z",
          "shell.execute_reply.started": "2024-05-10T14:40:00.106986Z",
          "shell.execute_reply": "2024-05-10T14:40:00.112403Z"
        },
        "trusted": true,
        "id": "NA0D5MvsDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_grid(grid, title=None):\n",
        "    \"\"\"\n",
        "    displays a grid of images\n",
        "    \"\"\"\n",
        "    np_grid = tensor_to_array(grid)\n",
        "\n",
        "    # display\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np_grid)\n",
        "    if title is not None:\n",
        "        plt.title(title)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:01.626002Z",
          "iopub.execute_input": "2024-05-10T14:40:01.626986Z",
          "iopub.status.idle": "2024-05-10T14:40:01.633063Z",
          "shell.execute_reply.started": "2024-05-10T14:40:01.626946Z",
          "shell.execute_reply": "2024-05-10T14:40:01.631914Z"
        },
        "trusted": true,
        "id": "u1WVreLdDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in ['train', 'validation', 'test']:\n",
        "    for imgs, _ in dataloaders[name]:\n",
        "        if imgs != []:\n",
        "            out = make_grid(imgs)\n",
        "            show_grid(out, f'{name} samples')\n",
        "            break\n",
        "        else:\n",
        "            continue"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:03.347091Z",
          "iopub.execute_input": "2024-05-10T14:40:03.348225Z",
          "iopub.status.idle": "2024-05-10T14:40:05.342599Z",
          "shell.execute_reply.started": "2024-05-10T14:40:03.348181Z",
          "shell.execute_reply": "2024-05-10T14:40:05.341568Z"
        },
        "trusted": true,
        "id": "EvCflMcQDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True).to(Config.device)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "output_features = model.fc.in_features\n",
        "\n",
        "model.fc = nn.Linear(output_features, Config.num_classes).to(Config.device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:08.565441Z",
          "iopub.execute_input": "2024-05-10T14:40:08.565841Z",
          "iopub.status.idle": "2024-05-10T14:40:08.819048Z",
          "shell.execute_reply.started": "2024-05-10T14:40:08.565812Z",
          "shell.execute_reply": "2024-05-10T14:40:08.818043Z"
        },
        "trusted": true,
        "id": "3TK30UhaDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate, weight_decay=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:12.914402Z",
          "iopub.execute_input": "2024-05-10T14:40:12.914778Z",
          "iopub.status.idle": "2024-05-10T14:40:12.922618Z",
          "shell.execute_reply.started": "2024-05-10T14:40:12.914751Z",
          "shell.execute_reply": "2024-05-10T14:40:12.921384Z"
        },
        "trusted": true,
        "id": "mDNUR63wDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=3):\n",
        "    callbackers = {}\n",
        "    callbackers['train_loss'] = []\n",
        "    callbackers['validation_loss'] = []\n",
        "    callbackers['train_accuracy'] = []\n",
        "    callbackers['validation_accuracy'] = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(Config.device)\n",
        "                labels = labels.to(Config.device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(image_datasets[phase])\n",
        "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
        "                                                        epoch_loss,\n",
        "                                                        epoch_acc))\n",
        "            callbackers[f'{phase}_loss'].append(epoch_loss)\n",
        "            callbackers[f'{phase}_accuracy'].append(epoch_acc)\n",
        "\n",
        "    return model, callbackers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:14.442744Z",
          "iopub.execute_input": "2024-05-10T14:40:14.443133Z",
          "iopub.status.idle": "2024-05-10T14:40:14.45645Z",
          "shell.execute_reply.started": "2024-05-10T14:40:14.443104Z",
          "shell.execute_reply": "2024-05-10T14:40:14.455189Z"
        },
        "trusted": true,
        "id": "5uzMOHuLDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained, callbackers = train_model(model, criterion, optimizer, num_epochs=Config.epochs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T14:40:18.164288Z",
          "iopub.execute_input": "2024-05-10T14:40:18.164763Z",
          "iopub.status.idle": "2024-05-10T15:20:35.813963Z",
          "shell.execute_reply.started": "2024-05-10T14:40:18.164718Z",
          "shell.execute_reply": "2024-05-10T15:20:35.812447Z"
        },
        "trusted": true,
        "id": "Zx5L7_LXDOC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}