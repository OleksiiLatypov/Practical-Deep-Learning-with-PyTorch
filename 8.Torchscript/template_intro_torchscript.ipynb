{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8624a0b3c7674b6c93339343238839b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e71caae99f9f4ac5911b34e289e44df1",
              "IPY_MODEL_ce74dfad784e49fe85f4bfa791d24d0d",
              "IPY_MODEL_e06ab1a20aa34844a5ba8cba411a333a"
            ],
            "layout": "IPY_MODEL_15e3af6f55a5497d9b4f52836d92367b"
          }
        },
        "e71caae99f9f4ac5911b34e289e44df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7b77ad556a43eea2f192d6f9f2b277",
            "placeholder": "​",
            "style": "IPY_MODEL_ee081847a20543c7b79466d7e6f51be2",
            "value": "Downloading: 100%"
          }
        },
        "ce74dfad784e49fe85f4bfa791d24d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8751d18ab1ab497db14c5a8799e2781b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a429ee929e214fa19e851ee68f723f49",
            "value": 231508
          }
        },
        "e06ab1a20aa34844a5ba8cba411a333a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9dbedc8b1b471989d3032a8a7c53ef",
            "placeholder": "​",
            "style": "IPY_MODEL_c29e68b9d5bf47a686b0c6086c278e10",
            "value": " 226k/226k [00:00&lt;00:00, 1.03MB/s]"
          }
        },
        "15e3af6f55a5497d9b4f52836d92367b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7b77ad556a43eea2f192d6f9f2b277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee081847a20543c7b79466d7e6f51be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8751d18ab1ab497db14c5a8799e2781b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a429ee929e214fa19e851ee68f723f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9dbedc8b1b471989d3032a8a7c53ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c29e68b9d5bf47a686b0c6086c278e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25427e3c29754528a680580de44fb509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e65ac89765324f3c93eb875779674a00",
              "IPY_MODEL_5ffab477c5694342bf5ba6c625a29269",
              "IPY_MODEL_e01d05eec60747f59dd978143ddd742e"
            ],
            "layout": "IPY_MODEL_2a4ce18e893a4d3cb34320098e51430f"
          }
        },
        "e65ac89765324f3c93eb875779674a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59bd9a2ceb3d4b52878c371a3de240d2",
            "placeholder": "​",
            "style": "IPY_MODEL_95d125740a4e49c4b1d3122d8647e753",
            "value": "Downloading: 100%"
          }
        },
        "5ffab477c5694342bf5ba6c625a29269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70f929b1e5d54d4fb0371ae67d5695f6",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a64c7b46a08f4252a2ea71ab04907259",
            "value": 48
          }
        },
        "e01d05eec60747f59dd978143ddd742e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d3207f23cc04be9b8e991de8c44fc89",
            "placeholder": "​",
            "style": "IPY_MODEL_b3231b6ecdfa4f6aba4096fb1d4c4e1d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.67kB/s]"
          }
        },
        "2a4ce18e893a4d3cb34320098e51430f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59bd9a2ceb3d4b52878c371a3de240d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95d125740a4e49c4b1d3122d8647e753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70f929b1e5d54d4fb0371ae67d5695f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a64c7b46a08f4252a2ea71ab04907259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d3207f23cc04be9b8e991de8c44fc89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3231b6ecdfa4f6aba4096fb1d4c4e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0c2f9e7678243a1916168242eac02fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_286d0effe9bc4bbda563754abdba08f1",
              "IPY_MODEL_e6a93843541d4561ae9f80628fa4cbaf",
              "IPY_MODEL_fa66fb6a898849f0b292191b46668470"
            ],
            "layout": "IPY_MODEL_9a9568a0bfc94d688c63cf85d94bcc3e"
          }
        },
        "286d0effe9bc4bbda563754abdba08f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1e037feb14943208c20d417bd874eb5",
            "placeholder": "​",
            "style": "IPY_MODEL_87d3ba5c20a34af789c609abc8524778",
            "value": "Downloading: 100%"
          }
        },
        "e6a93843541d4561ae9f80628fa4cbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8830ffb69d04d79bad98bebcd7d2aeb",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ce29cdb8eca4ef2b1fb8e3191e38f41",
            "value": 570
          }
        },
        "fa66fb6a898849f0b292191b46668470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_996c93536a1147d6a4cbb7b0b7fcbacd",
            "placeholder": "​",
            "style": "IPY_MODEL_0f5d0bbc2678498d943acfc8286d96a5",
            "value": " 570/570 [00:00&lt;00:00, 30.3kB/s]"
          }
        },
        "9a9568a0bfc94d688c63cf85d94bcc3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e037feb14943208c20d417bd874eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d3ba5c20a34af789c609abc8524778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8830ffb69d04d79bad98bebcd7d2aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ce29cdb8eca4ef2b1fb8e3191e38f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "996c93536a1147d6a4cbb7b0b7fcbacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5d0bbc2678498d943acfc8286d96a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/Torchscript/template_intro_torchscript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TorchScript**\n",
        "\n",
        "In this lab, you will learn how to use TorchScript to optimise inference runtime using examples of Yolov5s and BERT models.\n",
        "\n",
        ">GPU is recomended for this assignment. `Runtime` -> `Change runtime type` -> `GPU`\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- How to use tracing or scripting to convert a model to TorchScript\n",
        "- How to measure Inference time of a model"
      ],
      "metadata": {
        "id": "mfR8P8DEkkTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import packages**"
      ],
      "metadata": {
        "id": "YoylOzlFhmgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yolort==0.6.2\n",
        "!pip install transformers==4.18.0"
      ],
      "metadata": {
        "id": "RsTR4b643m9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf302fe-7220-4dce-d27a-3b6cf1e946f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yolort==0.6.2\n",
            "  Downloading yolort-0.6.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (1.26.4)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (9.4.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (4.66.4)\n",
            "Collecting onnx>=1.8.0 (from yolort==0.6.2)\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (7.34.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from yolort==0.6.2) (2.1.4)\n",
            "Collecting thop (from yolort==0.6.2)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->yolort==0.6.2) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.8.0->yolort==0.6.2) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython->yolort==0.6.2)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->yolort==0.6.2) (4.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->yolort==0.6.2) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->yolort==0.6.2) (2024.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop->yolort==0.6.2) (2.3.1+cu121)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->yolort==0.6.2) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->yolort==0.6.2) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->yolort==0.6.2) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->yolort==0.6.2) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->thop->yolort==0.6.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop->yolort==0.6.2) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->thop->yolort==0.6.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop->yolort==0.6.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop->yolort==0.6.2) (1.3.0)\n",
            "Downloading yolort-0.6.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, yolort\n",
            "Successfully installed jedi-0.19.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 onnx-1.16.2 thop-0.1.1.post2209072238 yolort-0.6.2\n",
            "Collecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.18.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (1.4.2)\n",
            "Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed sacremoses-0.1.1 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "from yolort.models import yolov5s\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "zyHg9h8VklD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35209ae3-7d8a-4148-9309-e82866d85bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Font error: cannot open resource\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[cls] Config\n",
        "\n",
        "class Config:\n",
        "\n",
        "    n_imgs = 32\n",
        "    input_shape = (3, 256, 256)\n",
        "    yolo_nwarmup = 20\n",
        "    yolo_nruns = 100\n",
        "\n",
        "    bert_nwarmup = 50\n",
        "    bert_nruns = 500\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vfKv2QgN5Sz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Runtime optimization with TochScript**\n",
        "\n",
        "## **What is TorchScript ?**\n",
        "[TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript-language) is a high-performance subset of Python language, specialised for ML applications. Using TorchScript, you can easily create serialisable and optimisable models from PyTorch code to load into a process with no dependency on Python, such as in a C++ program. It will run your models faster and independent.\n",
        "\n",
        "## **The Script mode and PyTorch JIT**\n",
        "\n",
        "It is a part of the PyTorch focused on the production use case, which has 2 components PyTorch JIT (an optimised compiler) and TorchScript.\n",
        "\n",
        "Script mode creates an intermediate representation (IR) through [`torch.jit.trace`](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) and [`torch.jit.script`](https://pytorch.org/docs/stable/generated/torch.jit.script.html) to represent computation. The IR is internally optimised and utilises PyTorch JIT compilation at runtime.\n",
        "\n",
        "PyTorch JIT is a compiler for PyTorch programs:\n",
        "- It is a lightweight threadsafe interpreter\n",
        "- Supports easy to write custom transformations\n",
        "- It’s not just for inference as it has auto diff support\n",
        "\n",
        "Pytorch provides two methods for generating TorchScript from your model code:\n",
        "\n",
        "- **Tracing**: [`torch.jit.trace`](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) takes a data instance and trained model as input. It runs the model and then records and traces the executed operations performed on all the tensors. This recording is turned into a TorchScript.\n",
        "\n",
        "- **Scripting**: [`torch.jit.script`](https://pytorch.org/docs/stable/generated/torch.jit.script.html) allows writing code directly into TorchScript, which will be generated from the static inspection of the nn.Module contents. So, in contrast to trace mode, you only need to pass an instance of your model to torch.jit.script and a data sample are not necessary.\n",
        "\n",
        "### When to use:\n",
        "\n",
        "- **Tracing**:\n",
        "    - Use `torch.jit.trace` if you are unable to modify the model code. In this case scripting the model simply will not work, because it uses unsupported Pytorch/Python functionality.\n",
        "    - You may use the logic [freezing](https://pytorch.org/tutorials/prototype/torchscript_freezing.html) behaviour of tracing if you need to gain better performance or to make changes in architectural decisions.\n",
        "\n",
        "- **Scripting**:\n",
        "    - `torch.jit.script` is easy to use because it captures both your model's operations and full conditional logic. An export is likely to either fail for a well-defined reason, which you can solve by implying a clear code modification or even succeed without warnings.\n",
        "\n",
        "Scripted and traced code can be mixed. You can see the existing [documentation](https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting) for details and examples.\n"
      ],
      "metadata": {
        "id": "PpGFg8LhTRBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Yolov5s inference optimising**"
      ],
      "metadata": {
        "id": "-f4zbrPSghkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load yolov5s model and scripting it**\n",
        "\n",
        "We will use yolov5s from the [yolort](https://github.com/zhiqwang/yolov5-rt-stack) library, trained on the COCO dataset, to test speed inference time."
      ],
      "metadata": {
        "id": "qjUFmgevvvjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] native_yolov5s_model\n",
        "\n",
        "native_yolov5s_model = yolov5s(pretrained=True, size=(256, 256))"
      ],
      "metadata": {
        "id": "iDNiHM1AqX3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3727bdbb-b614-4276-adaa-a8466d696c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/zhiqwang/yolov5-rt-stack/releases/download/v0.5.2-alpha/yolov5_darknet_pan_s_r60_coco-9f44bf3f.pt\" to /root/.cache/torch/hub/checkpoints/yolov5_darknet_pan_s_r60_coco-9f44bf3f.pt\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 106MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excercise**: create the script of yolov5s model, passing an instance of the model to `torch.jit.script`:"
      ],
      "metadata": {
        "id": "E1-HAKtQMOl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] scripted_yolov5s_model\n",
        "\n",
        "### START CODE HERE ### (1 line of code)\n",
        "scripted_yolov5s_model = torch.jit.script(native_yolov5s_model)\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "xFKqY0XE5WXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For clarity, let's save the scripted model and load it."
      ],
      "metadata": {
        "id": "IZ2WzYlTN-8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(scripted_yolov5s_model,'scripted_yolov5s.pt')\n",
        "loaded_scripted_yolov5s_model = torch.jit.load('scripted_yolov5s.pt')"
      ],
      "metadata": {
        "id": "5n_8P4jn5WZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benchmarking native and scripted Yolov5s model**"
      ],
      "metadata": {
        "id": "o2_559cbv8rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before making inference time measurements, we need to run some dummy examples through the model to do a GPU warm-up. Warm-up will initialise the GPU and prevent it from going into a power-saving mode.\n",
        "\n",
        "Next, we need to use [`torch.cuda.event`](https://pytorch.org/docs/stable/generated/torch.cuda.Event.html) and init loggers to measure time on the GPU.\n",
        "\n",
        "Finally, after the model is warmed-up, we can measure performance. To do this correctly, we need to use [`torch.cuda.synchronize()`](https://alband.github.io/doc_view/cuda.html#torch.cuda.synchronize). This will perform synchronisation between the host and device (i.e., GPU and CPU), so the time recording takes place only after the process running on the GPU is finished.\n",
        "\n",
        "Below is an implemented function that will perform warming up and the main inference of the model with measuring performance time.\n"
      ],
      "metadata": {
        "id": "n5U3rQ0Iv4re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] run_model\n",
        "\n",
        "def run_model(model, input, nruns, desc=None, unpack=False):\n",
        "    \"\"\"\n",
        "    Runs a model with inputs and measures performance time\n",
        "\n",
        "    Arguments:\n",
        "    model -- a model we will use to test performance\n",
        "    input -- inputs for a model\n",
        "    nruns -- steps to run a model\n",
        "    desc -- description of a progress bar\n",
        "    unpack -- bool indicator. Indicates unpack input or not\n",
        "\n",
        "    Return:\n",
        "    infer_time -- inference time\n",
        "    \"\"\"\n",
        "\n",
        "    pbar = trange(nruns,\n",
        "                  unit=\" runs\",\n",
        "                  desc=desc,\n",
        "                  bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt} run /{total_fmt} runs '\n",
        "                '[{elapsed}<{remaining}, {rate_fmt}{postfix}]')\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # init loggers\n",
        "        star_measure_time, end_measure_time = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        timings=np.zeros((nruns,1))\n",
        "        with torch.no_grad():\n",
        "            for i in pbar:\n",
        "                star_measure_time.record()\n",
        "                if unpack == False:\n",
        "                    _ = model(input)\n",
        "                else:\n",
        "                    _ = model(*input)\n",
        "                end_measure_time.record()\n",
        "                # wait for gpu sync\n",
        "                torch.cuda.synchronize()\n",
        "                curr_time = star_measure_time.elapsed_time(end_measure_time)\n",
        "                timings[i] = curr_time\n",
        "\n",
        "        infer_time = np.sum(timings)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            for _ in pbar:\n",
        "                if unpack == False:\n",
        "                    _ = model(input)\n",
        "                else:\n",
        "                    _ = model(*input)\n",
        "\n",
        "        infer_time = pbar.format_dict[\"elapsed\"] * 1000\n",
        "\n",
        "    return infer_time"
      ],
      "metadata": {
        "id": "i_GePTGYQV33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use `run_model` function prepared above to benchmark the model.\n",
        "\n",
        "**Excercise**: implement `benchmark` function.\n",
        "\n",
        "1. Pass dummy inputs to `device`.\n",
        "2. Pass the model to `device`.\n",
        "3. Switch the model to `eval` mode.\n",
        "4. Warm-up the model using `run_model` with params `nwarmup` and `desc=\"Warm-up\"`.\n",
        "5. Measure inference time of the model using `run_model` with params `ninfer` and `desc=\"Inference timing\"`."
      ],
      "metadata": {
        "id": "uppYUXWo_GgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] benchmark\n",
        "\n",
        "def benchmark(model, dummy_input, nwarmup, ninfer, unpack=False, device=Config.device):\n",
        "\n",
        "    \"\"\"\n",
        "    Benchmarks a model inference\n",
        "\n",
        "    Arguments:\n",
        "    model -- a model we will benchmark\n",
        "    dummy_input -- list of dummy inputs for model inference\n",
        "    nwarmup -- steps for warm-up model\n",
        "    ninfer -- steps for main model inference\n",
        "    unpack -- bool indicator. Indicates unpack input or not\n",
        "    device -- type of device\n",
        "\n",
        "    Return:\n",
        "    infer_time -- inference time\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 5 lines of code)\n",
        "    # pass dummy inputs to device\n",
        "    #print(dummy_input[0])\n",
        "    # if isinstance(dummy_input, list):\n",
        "    #   dummy_input = [x.to(device) for x in dummy_input]\n",
        "    # else:\n",
        "    dummy_input = [x.to(device) for x in list(dummy_input)]\n",
        "\n",
        "\n",
        "    # pass the model to device\n",
        "    model.to(device)\n",
        "\n",
        "    # switch the model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # warm-up the model\n",
        "    _ = run_model(model, dummy_input, unpack=unpack, nruns=nwarmup, desc=\"Warm-up\")\n",
        "\n",
        "    # measure performance of the model\n",
        "    infer_time = run_model(model, dummy_input, unpack=unpack, nruns=ninfer, desc=\"Inference timing\")\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    print(f\"\\nInference time for the model: {infer_time:.2f} ms for {ninfer} runs\")\n",
        "    print(f\"Avg. inference time for the model: {(infer_time)/ ninfer:.2f} ms\\n\")\n",
        "\n",
        "    return infer_time"
      ],
      "metadata": {
        "id": "8VUeqwLN5WcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's benchmark the scripted and native Yolov5s model:"
      ],
      "metadata": {
        "id": "MH1_UCvaW60D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dummy_input for Yolov5s\n",
        "yolo_dummy_input = [torch.rand(Config.input_shape)for i in range(Config.n_imgs)]\n",
        "\n",
        "# benchmark\n",
        "print(\"Native Yolov5s benchmark:\\n\")\n",
        "native_yolo_infer_time = benchmark(native_yolov5s_model, yolo_dummy_input, nwarmup=Config.yolo_nwarmup, ninfer=Config.yolo_nruns)\n",
        "print(\"Scripted Yolov5s benchmark:\\n\")\n",
        "scripted_yolo_infer_time = benchmark(loaded_scripted_yolov5s_model, yolo_dummy_input, nwarmup=Config.yolo_nwarmup, ninfer=Config.yolo_nruns)\n",
        "\n",
        "print(f\"Scripted Yolov5s is {((native_yolo_infer_time - scripted_yolo_infer_time)/ native_yolo_infer_time) * 100 :.2f} percent faster than Native model\")"
      ],
      "metadata": {
        "id": "IFx2XzVI5Weu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e096cfe5-0271-41eb-f123-2731451225a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Native Yolov5s benchmark:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warm-up: 100%|██████████| 20 run /20 runs [00:01<00:00, 14.44 runs/s]\n",
            "Inference timing: 100%|██████████| 100 run /100 runs [00:07<00:00, 14.07 runs/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference time for the model: 6975.76 ms for 100 runs\n",
            "Avg. inference time for the model: 69.76 ms\n",
            "\n",
            "Scripted Yolov5s benchmark:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warm-up: 100%|██████████| 20 run /20 runs [00:01<00:00, 19.54 runs/s]\n",
            "Inference timing: 100%|██████████| 100 run /100 runs [00:05<00:00, 19.59 runs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference time for the model: 5031.56 ms for 100 runs\n",
            "Avg. inference time for the model: 50.32 ms\n",
            "\n",
            "Scripted Yolov5s is 27.87 percent faster than Native model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output**:\n",
        "```\n",
        "Native Yolov5s benchmark:\n",
        "\n",
        "Warm-up: 100%|██████████| 20 run /20 runs [00:03<00:00,  5.37 runs/s]\n",
        "Inference timing: 100%|██████████| 100 run /100 runs [00:18<00:00,  5.42 runs/s]\n",
        "\n",
        "Inference time for the model: 18185.01 ms for 100 runs\n",
        "Avg. inference time for the model: 181.85 ms\n",
        "\n",
        "Scripted Yolov5s benchmark:\n",
        "\n",
        "Warm-up: 100%|██████████| 20 run /20 runs [00:03<00:00,  5.94 runs/s]\n",
        "Inference timing: 100%|██████████| 100 run /100 runs [00:16<00:00,  5.94 runs/s]\n",
        "Inference time for the model: 16608.96 ms for 100 runs\n",
        "Avg. inference time for the model: 166.09 ms\n",
        "\n",
        "Scripted Yolov5s is 8.67 percent faster than Native model\n",
        "```\n",
        ">(Elapsed time and speed may slightly vary)\n",
        "\n",
        ">If you are running on Tesla K80, the scripted Yolov5s model will be **~ 6-10** percent faster than native Yolov5s model. For Telsa T4, the performance difference will be more significant on average **~ 15-20** percent.\n"
      ],
      "metadata": {
        "id": "bgh1Vev3HigM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT inference optimising**\n",
        "\n",
        "Here we will use BERT from the transformer’s library provided by HuggingFace."
      ],
      "metadata": {
        "id": "HMvV2OwqwLL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Creating dummy input for BERT**\n",
        "\n",
        "Firstly let's initialise BERT tokenizer:"
      ],
      "metadata": {
        "id": "vQYGbKsOdK_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', torchscript=True)"
      ],
      "metadata": {
        "id": "bp47y22Llnk-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8624a0b3c7674b6c93339343238839b0",
            "e71caae99f9f4ac5911b34e289e44df1",
            "ce74dfad784e49fe85f4bfa791d24d0d",
            "e06ab1a20aa34844a5ba8cba411a333a",
            "15e3af6f55a5497d9b4f52836d92367b",
            "3c7b77ad556a43eea2f192d6f9f2b277",
            "ee081847a20543c7b79466d7e6f51be2",
            "8751d18ab1ab497db14c5a8799e2781b",
            "a429ee929e214fa19e851ee68f723f49",
            "9e9dbedc8b1b471989d3032a8a7c53ef",
            "c29e68b9d5bf47a686b0c6086c278e10",
            "25427e3c29754528a680580de44fb509",
            "e65ac89765324f3c93eb875779674a00",
            "5ffab477c5694342bf5ba6c625a29269",
            "e01d05eec60747f59dd978143ddd742e",
            "2a4ce18e893a4d3cb34320098e51430f",
            "59bd9a2ceb3d4b52878c371a3de240d2",
            "95d125740a4e49c4b1d3122d8647e753",
            "70f929b1e5d54d4fb0371ae67d5695f6",
            "a64c7b46a08f4252a2ea71ab04907259",
            "5d3207f23cc04be9b8e991de8c44fc89",
            "b3231b6ecdfa4f6aba4096fb1d4c4e1d",
            "b0c2f9e7678243a1916168242eac02fd",
            "286d0effe9bc4bbda563754abdba08f1",
            "e6a93843541d4561ae9f80628fa4cbaf",
            "fa66fb6a898849f0b292191b46668470",
            "9a9568a0bfc94d688c63cf85d94bcc3e",
            "e1e037feb14943208c20d417bd874eb5",
            "87d3ba5c20a34af789c609abc8524778",
            "a8830ffb69d04d79bad98bebcd7d2aeb",
            "8ce29cdb8eca4ef2b1fb8e3191e38f41",
            "996c93536a1147d6a4cbb7b0b7fcbacd",
            "0f5d0bbc2678498d943acfc8286d96a5"
          ]
        },
        "outputId": "bc1fc83a-9578-4720-eb55-a418c855fb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8624a0b3c7674b6c93339343238839b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25427e3c29754528a680580de44fb509"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0c2f9e7678243a1916168242eac02fd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create a sample input data for inference."
      ],
      "metadata": {
        "id": "ljZEmcISO0fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[str]\n",
        "\n",
        "text = \"I live in [MASK], this country is located on the continent of the same name.\""
      ],
      "metadata": {
        "id": "aIy-rOVl0NC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excercise**: implement `create_dummy_input_for_bert` function to create dummy input for BERT model.\n",
        "\n",
        "- tokenize `text_example` using `encode_plus` with parameter `return_tensors=\"pt\"` and return a list with `'input_ids'` and `'attention_mask'`:"
      ],
      "metadata": {
        "id": "OUmMg2ZFidoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] create_dummy_input_for_bert\n",
        "\n",
        "def create_dummy_input_for_bert(text_example):\n",
        "\n",
        "    \"\"\"\n",
        "    Creates dummy input for BERT model\n",
        "\n",
        "    Arguments:\n",
        "    text_example -- string with a sentence example\n",
        "\n",
        "    Return:\n",
        "    dummy_input -- list of 'input_ids' and 'attention_mask'\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 2-4 lines of code)\n",
        "    text_example = tokenizer.encode_plus(text_example, return_tensors='pt')\n",
        "    return [text_example['input_ids'], text_example['attention_mask']]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "dummy_input = create_dummy_input_for_bert(text)\n",
        "print(f\"{dummy_input[0]}\\n{dummy_input[1]}\")"
      ],
      "metadata": {
        "id": "qCdVVAv6bXm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a43e62-ec31-40aa-99c4-b04199173900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 1045, 2444, 1999,  103, 1010, 2023, 2406, 2003, 2284, 2006, 1996, 9983, 1997, 1996, 2168, 2171, 1012,  102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output**:\n",
        "\n",
        "```\n",
        "tensor([[ 101, 1045, 2444, 1999,  103, 1010, 2023, 2406, 2003, 2284, 2006, 1996, 9983, 1997, 1996, 2168, 2171, 1012,  102]])\n",
        "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TXqTNg7CqJW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading native BERT model and tracing it**"
      ],
      "metadata": {
        "id": "r47jFRS2e2q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will load the pretrained BERT model:"
      ],
      "metadata": {
        "id": "YtWicjXvND4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] native_bert_model\n",
        "\n",
        "native_bert_model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)"
      ],
      "metadata": {
        "id": "k8UV9-ZCxBwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3aebdaf-66a1-41f2-9630-8f280412f969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create the trace of BERT model.\n",
        "\n",
        "**Excercise**: create the trace of BERT model, passing an instance of the model and `dummy_input` to `torch.jit.trace`:"
      ],
      "metadata": {
        "id": "W8lhzanJqyKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION_FIELD[func] traced_bert_model\n",
        "\n",
        "### START CODE HERE ### (1 line of code)\n",
        "traced_bert_model = torch.jit.trace(native_bert_model, (dummy_input))\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "PskRGXoX4GTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For clarity, save the trace of BERT model and load it."
      ],
      "metadata": {
        "id": "i9HGjeRfy8dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(traced_bert_model, \"traced_bert.pt\")\n",
        "loaded_traced_bert_model = torch.jit.load(\"traced_bert.pt\")"
      ],
      "metadata": {
        "id": "IzMhAL4i4KF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benchmarking native and traced BERT model**"
      ],
      "metadata": {
        "id": "rBdRV1V7lE-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark\n",
        "print(\"Native BERT benchmark:\\n\")\n",
        "native_bert_infer_time = benchmark(native_bert_model, dummy_input, nwarmup=Config.bert_nwarmup, ninfer=Config.bert_nruns, unpack=True)\n",
        "print(\"Traced BERT benchmark:\\n\")\n",
        "traced_bert_infer_time = benchmark(loaded_traced_bert_model, dummy_input, nwarmup=Config.bert_nwarmup, ninfer=Config.bert_nruns, unpack=True)\n",
        "\n",
        "print(f\"Traced BERT is {((native_bert_infer_time - traced_bert_infer_time)/ native_bert_infer_time) * 100 :.2f} percent faster than Native model\")"
      ],
      "metadata": {
        "id": "wqDlNgCnew-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee0c63e-f755-449e-b3ae-886582063f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Native BERT benchmark:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warm-up: 100%|██████████| 50 run /50 runs [00:00<00:00, 103.50 runs/s]\n",
            "Inference timing: 100%|██████████| 500 run /500 runs [00:04<00:00, 101.80 runs/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference time for the model: 4827.04 ms for 500 runs\n",
            "Avg. inference time for the model: 9.65 ms\n",
            "\n",
            "Traced BERT benchmark:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warm-up: 100%|██████████| 50 run /50 runs [00:02<00:00, 24.23 runs/s]\n",
            "Inference timing: 100%|██████████| 500 run /500 runs [00:02<00:00, 186.02 runs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference time for the model: 2606.74 ms for 500 runs\n",
            "Avg. inference time for the model: 5.21 ms\n",
            "\n",
            "Traced BERT is 46.00 percent faster than Native model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output**:\n",
        "```\n",
        "Native BERT benchmark:\n",
        "\n",
        "Warm-up: 100%|██████████| 50 run /50 runs [00:00<00:00, 58.19 runs/s]\n",
        "Inference timing: 100%|██████████| 500 run /500 runs [00:07<00:00, 63.76 runs/s]\n",
        "\n",
        "Inference time for the model: 7643.96 ms for 500 runs\n",
        "Avg. inference time for the model: 15.29 ms\n",
        "\n",
        "Traced BERT benchmark:\n",
        "\n",
        "Warm-up: 100%|██████████| 50 run /50 runs [00:00<00:00, 75.80 runs/s]\n",
        "Inference timing: 100%|██████████| 500 run /500 runs [00:06<00:00, 76.36 runs/s]\n",
        "Inference time for the model: 6347.65 ms for 500 runs\n",
        "Avg. inference time for the model: 12.70 ms\n",
        "\n",
        "Traced BERT is 16.96 percent faster than Native model\n",
        "```\n",
        ">(Elapsed time and speed may slightly vary)\n",
        "\n",
        ">If you are running on Tesla K80, the traced BERT model will be **~ 10-20** percent faster than native BERT model. For Telsa T4, the performance difference will be more significant on average **~ 40-50** percent."
      ],
      "metadata": {
        "id": "amjnlwkGlM01"
      }
    }
  ]
}