{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksiiLatypov/Practical_Deep_Learning_with_PyTorch/blob/main/week2/template_multi_class_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAEMTesGDH5c"
      },
      "source": [
        "# Multiclass Classification Problem\n",
        "\n",
        "Hey! Welcome to your next lab. Here you will implement a neural network for a multiclass classification problem.\n",
        "\n",
        "**Requirements**\n",
        "- Although it's not mandatory, we highly recommend that you complete Binary Classification Lab before working on this one.\n",
        "\n",
        "**Instructions**\n",
        "- Write code in the space indicated with `### START CODE HERE ###`\n",
        "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
        "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
        "\n",
        "**You will learn**\n",
        "- How to implement a Neural Network for multiclass classification\n",
        "    - How to balance the loss for more optimal training\n",
        "    - How to evaluate the model with imbalanced classes\n",
        "    - How to train and test a multiclass classification Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdKrFQ3vyfQk"
      },
      "source": [
        "# 0 - Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40w55iizyfQk",
        "outputId": "eb89fc89-4c1e-49d2-d1d2-2f2b040815bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=e9c26ecf9faaf481643982860509bdf24ef4ddb687cff1faec89db0b4f388da2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/2_pytorch/multi_class_classification/template.zip')\n",
        "!unzip -q 'template.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73sXz1P8-wz9"
      },
      "source": [
        "# 1 - Import packages and set up the environment\n",
        "\n",
        "Here we will import a regular set of packages with a few additional ones.\n",
        "\n",
        "- [pandas](https://pandas.pydata.org/docs/index.html) is one of the most popular Python packages for tabular data\n",
        "- [seaborn](https://seaborn.pydata.org/) a package built on `matplotlib`, which provides some useful high-level functionality for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N6Q_yNxq2E_F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_PGDegmjeJP"
      },
      "source": [
        "## Configs\n",
        "\n",
        "Here is our regular Config class. We define few constants like `num_classes` and `input_dim` to simplify building the network architecture.<br>\n",
        "**Please note** that `classes` is a `tuple`, not a `list`, because we are not going to modify this variable anywhere in our code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E36O4iLMGbht"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] Config\n",
        "\n",
        "class Config:\n",
        "    # data\n",
        "    test_size = 0.33\n",
        "    classes = ('Mammal', 'Bird', 'Reptile',\n",
        "            'Fish', 'Amphibian', 'Bug', 'Invertebrate')\n",
        "    # model\n",
        "    num_classes = 7\n",
        "    input_dim = 16\n",
        "    hidden_dims = (64, 32)\n",
        "\n",
        "    # training\n",
        "    seed = 21\n",
        "    batch_size = 8\n",
        "    epochs = 10\n",
        "    learning_rate = 0.01\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0A0JjfTIyrMH"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(Config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Lsh8di-0U-"
      },
      "source": [
        "# 2 - Data Preprocessing and visualizations\n",
        "\n",
        "For this lab, we will learn how to classify an animal using 16 categorical features from  [Zoo Animal UCI Machine Learning Dataset](\n",
        "https://www.kaggle.com/uciml/zoo-animal-classification). Here is the description of each feature in the dataset:\n",
        "\n",
        "- animal_name: Unique for each instance\n",
        "- hair Boolean\n",
        "- feathers Boolean\n",
        "- eggs Boolean\n",
        "- milk Boolean\n",
        "- airborne Boolean\n",
        "- aquatic Boolean\n",
        "- predator Boolean\n",
        "- toothed Boolean\n",
        "- backbone Boolean\n",
        "- breathes Boolean\n",
        "- venomous Boolean\n",
        "- fins Boolean\n",
        "- legs Numeric (set of values: {0,2,4,5,6,8})\n",
        "- tail Boolean\n",
        "- domestic Boolean\n",
        "- catsize Boolean\n",
        "- class_type Numeric (integer values in range [1,7])\n",
        "\n",
        "Most of the columns in the dataset are of Boolean type, which means that they contain either 1 (meaning that an animal has the corresponding feature) or 0 (if an animal doesn't have the indicated feature). The exception is `legs,` which is a number from a set `{0,2,4,5,6,8}` representing the number of legs, and `class_type`, which has an integer value from `[1, 7]` range, representing an animal class. <br>\n",
        "\n",
        "We will use all features (except for `animal_name`) to predict the `class_type` of the given animal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pme86ApYm4fr"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "**Important:** by default CrossEntropyLoss for PyTorch expects target labels to be from a range `[0, num_classes-1]`. So, we subtract 1 from `class_type`column below. Also, we use `startify` parameter in `train_test_split` to ensure that `train` and `test` sets have an equal proportion of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qM_WAad15M7i"
      },
      "outputs": [],
      "source": [
        "def load_data(test_size=Config.test_size, seed=Config.seed):\n",
        "    # read dataset from csv file\n",
        "    dataset = pd.read_csv('zoo_dataset.csv')\n",
        "\n",
        "    # drop values that we won't be using fro training\n",
        "    inputs = dataset.drop(['animal_name','class_type'], axis=1).values\n",
        "    # enumerate targets from 0\n",
        "    targets = dataset['class_type'].values - 1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        inputs, targets, stratify=targets,\n",
        "        test_size=test_size, random_state=seed)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E5jj-vRGIWMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f01c2d-6586-4476-e558-00d3765c9885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (143, 16) (143,)\n",
            "Test set: (71, 16) (71,)\n"
          ]
        }
      ],
      "source": [
        "print('Train set:', X_train.shape, y_train.shape)\n",
        "print('Test set:', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsLb1e8sqxH6"
      },
      "source": [
        "**Expected Output:**\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>Train set: </b></td>\n",
        "        <td>(143, 16) (143,)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Test set:</b></td>\n",
        "        <td>(71, 16) (71,)</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhqLgoQN-94e"
      },
      "source": [
        "## Data Visualization\n",
        "\n",
        "Our target labels are the following:\n",
        "* 1 - Mammal\n",
        "* 2 - Bird\n",
        "* 3 - Reptile\n",
        "* 4 - Fish\n",
        "* 5 - Amphibian\n",
        "* 6 - Bug\n",
        "* 7 - Invertebrate\n",
        "\n",
        "Let's visualize how many representatives of each class we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "YLGjHWvK7JxZ",
        "outputId": "958e78e1-8917-41e8-b4aa-dea6da37989f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "class_counts = pd.value_counts(y_train)\n",
        "plt.bar(class_counts.index, class_counts.to_list())\n",
        "plt.xticks(np.arange(Config.num_classes), Config.classes)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Animal class')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E91MG-2KsjCB"
      },
      "source": [
        "**Interpretation:** As we can see from the graph above, our classes are unbalanced, meaning that some classes like **Mammal** have much more examples in the dataset than **Bug** or **Amphibian**. This might lead to poor model performance because the neural network will prioritize correctly predicting classes that are more abundant in the dataset and neglect the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaCXZfSS28KX"
      },
      "source": [
        "# 3 - Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wuwayAJ3D0k"
      },
      "source": [
        "## Loss Scaler\n",
        "\n",
        "In the standard variation of multiclass classification, we use Cross Entropy between network predictions and target values, which is given by the formula below:\n",
        "\n",
        "$$CE(y, \\hat{y}) = - \\sum^{{n}_{c}-1}_{i=0} y_{i}\\log(\\hat{y}_{i})$$\n",
        "\n",
        "where $y\\in\\mathbb{R}^{n_c}$ is a one-hot label vector, $\\hat{y}\\in\\mathbb{R}^{n_c}$ is network predictions, and ${n}_{c}$ is the number of classes. <br>\n",
        "And to find the overall loss over a batch we simply average the cross entropy for each of the network prediction:\n",
        "\n",
        "$$CELoss(Y, \\hat{Y}) = - \\frac{1}{m}\\sum^{{m}-1}_{j=0} \\sum^{{n}_{c}-1}_{i=0} y_{i}^{<j>}\\log(\\hat{y}_{i}^{<j>})$$\n",
        "\n",
        "Note here, $Y\\in\\mathbb{R}^{m\\ x\\ n_c}$ and $\\hat{Y}\\in\\mathbb{R}^{m\\ x\\ n_c}$ are matrices of network predictions and target values respectfully, where $m$ is a number of examples in a batch, and $y_{i}^{<j>}$ is a prediction for the ith class of the jth example in the batch. <br>\n",
        "\n",
        "A simple average over all training examples, in this case, is not the best strategy. Instead of learning distinct features of each class, a neural network will learn how to correctly classify the most popular classes in the dataset, which would minimize the average value, ignoring the classes that rarely occur in the mini-batch. For example,  **Mammal** class, which has 40 examples, would be prioritized over **Amphibian** class, which has only 10 examples in the dataset. <br><br>\n",
        "\n",
        "So, instead of taking a **simple average** over all training examples, we will take a **weighted average**, increasing the importance of rare classes, and penalizing classes that occur much more often. The formulas, for the class weights are given below:\n",
        "\n",
        "\n",
        "$$score_i = \\sqrt{\\frac{\\sum_{i=0}^{n\\_classes-1}m_i}{m_i}}$$ <br>\n",
        "$$w_i = \\frac{score_i}{\\sum_{i=0}^{n\\_classes-1}score_i}$$\n",
        "\n",
        "where $m_i$ is a number of representatives of class $i$ in the dataset, $score_i$ is relative importance of each class, and $w_i$ is a normalized weight that ensures that the sum of our weights adds up to 1. <br>\n",
        "\n",
        "**Excercise:** Your job is to implement `get_loss_scalars` function that calculates weights `w_i` to scale `CrossEntropyLoss`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2HF3LdDPAlWC"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] get_loss_scalars\n",
        "\n",
        "def get_loss_scalars(labels):\n",
        "    \"\"\"\n",
        "    Calculates the coefficients that scale the importance of rare classes\n",
        "    in CrossEntropyLoss\n",
        "\n",
        "    The formula for calculation is:\n",
        "    score_i = (total_number_of_samples / number_samples_of_label_i)^1/2\n",
        "    weight_i = score_i/sum_scores\n",
        "\n",
        "    Arguments:\n",
        "    labels -- 1-D np.array of integers that represents classes in the dataset\n",
        "    Return:\n",
        "    weights -- 1-D torch.Tensor of length num_classes\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    counts = np.bincount(labels)\n",
        "    #print(counts)\n",
        "    scores = np.sqrt(len(labels) / counts)\n",
        "    #print(scores)\n",
        "    weights = torch.tensor(scores / np.sum(scores), dtype=torch.float)\n",
        "    ### END CODE HERE ###\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZE8yL8PCkkI",
        "outputId": "0ea8bb3c-ae3b-4792-e7dd-9285297fe9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights: tensor([0.0953, 0.1160, 0.1462, 0.1383, 0.1907, 0.1462, 0.1672])\n"
          ]
        }
      ],
      "source": [
        "weights = get_loss_scalars(y_train)\n",
        "print('weights:', weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOhCgOu2AFIR"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>weights</b></td>\n",
        "        <td>tensor([0.0953, 0.1160, 0.1462, 0.1383, 0.1907, 0.1462, 0.1672])</td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa3QMkCLAlNk"
      },
      "source": [
        "## F1 score\n",
        "\n",
        "Besides the loss function that we use for training, we need to have a metric to account for class imbalances.\n",
        "Accuracy score would simply tell us what proportion of all examples we classify correctly, but it doesn't tell us how\n",
        "well we perform on underrepresented classes. The following metric, called **F1 score** does just that. For a given\n",
        "class $i$ we calculate **F1 score** using:\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/confusion.png\">\n",
        "\n",
        "* $TP$ (true positive) = number of correct predictions of class $i$\n",
        "* $FP$ (false positive) = number of false predictions of class $i$\n",
        "* $FN$ (false negative) = number of examples of class $i$ not predicted with class $i$\n",
        "\n",
        "$$ Precision = \\frac{TP}{FP + TP} $$ <br>\n",
        "$$ Recall = \\frac{TP}{FN + TP} $$ <br>\n",
        "\n",
        "$$ F1_i = \\frac{ 2 * Precision * Recall}{Precision + Recall} $$\n",
        "\n",
        "And then, to calculate **macro F1 score**, we simply average all $F1_i$\n",
        "\n",
        "$$ F1_{macro} = \\sum^{{n}_{c}-1}_{i=0} F1_i $$\n",
        "\n",
        "**Note:** This function is available in `sklearn.metrics` but we will implement it to understand this evaluation method better.\n",
        "\n",
        "**Excercise:** Implement the F1 scoring function using the formulas above. You will have to use for-loop to iterate over unique target labels.\n",
        "- Hint: if for a given class $TP = 0$, you should assign $F1_i = 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bXrI9ohCBeK1"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] f1_score\n",
        "\n",
        "def f1_score(target, predictions):\n",
        "    \"\"\"\n",
        "    Calculates macro f1 score for target and predictions\n",
        "\n",
        "    Arguments:\n",
        "    target -- 1-D torch.Tensor of target classes\n",
        "    predictions -- 1-D torch.Tensor of integer network predictions\n",
        "\n",
        "    Returns:\n",
        "    F1_macro -- float, macro avergae of F1 scores\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for cls in target.unique():\n",
        "        ### START CODE HERE ### (≈ 9 lines of code)\n",
        "        TP = ((target == cls) & (predictions == cls)).sum().item()\n",
        "        FN = ((target == cls) & (predictions != cls)).sum().item()\n",
        "        FP = ((target != cls) & (predictions == cls)).sum().item()\n",
        "        if TP == 0:\n",
        "            F1 = 0\n",
        "        else:\n",
        "            precision = TP / (TP + FP)\n",
        "            recall = TP / (TP + FN)\n",
        "            F1 = 2 * (precision * recall) / (precision + recall)\n",
        "        ### END CODE HERE ###\n",
        "        scores.append(F1)\n",
        "    F1_macro = sum(scores) / len(scores)\n",
        "    return F1_macro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "sgPMZVaRCAX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93ee182-dcc0-4992-bca9-2103a464305a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_score: 0.4666666666666666\n"
          ]
        }
      ],
      "source": [
        "target = torch.tensor([0, 1, 2, 3, 1, 4])\n",
        "predic = torch.tensor([0, 1, 2, 0, 1, 2])\n",
        "print('f1_score:', f1_score(target, predic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzsdkKfDUHvK"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>f1_score</b></td>\n",
        "        <td>0.4666666666666666</td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc2YlalXGPed"
      },
      "source": [
        "# 3 - Creating the model class\n",
        "\n",
        "In the last lab, we learned how to use `nn.Sequential` to predefine the order of operations for some layers in the network. This time we will go even further and define our sequential blocks iteratively using `nn.ModuleList`<br>\n",
        "\n",
        "Let's start with a simple function that returns our usual Sequential block <br>\n",
        "\n",
        "**Excercise:** Implement function that returns `nn.Sequential` module with the following layers: `nn.Linear` -> `nn.Dropout` -> `nn.ReLU`\n",
        "- Set parameter `p` in Dropout to `0.2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "o5IhPvRwtmFl"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] linear_block\n",
        "\n",
        "def linear_block(input_dim, output_dim):\n",
        "    \"\"\"\n",
        "    Generates a generic sequential block\n",
        "\n",
        "    Arguments:\n",
        "    input_dim -- integer, input dimension to a linear block\n",
        "    output_dim -- integer, output dimension of a linear block\n",
        "\n",
        "    Return:\n",
        "    block -- nn.Sequential, block of a neural network\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    block = nn.Sequential(nn.Linear(input_dim, output_dim),\n",
        "                          nn.Dropout(p=0.2),\n",
        "                          nn.ReLU())\n",
        "    ### END CODE HERE ###\n",
        "    return block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZdlzpNQyrNI"
      },
      "source": [
        "## Model\n",
        "\n",
        "The architecture of this network is very similar to the binary classification network, with a few exceptions:\n",
        "1. The output layer of this network has seven neurons (one for each class)\n",
        "2. The output layer has linear activation (no activation function is applied). This is because we will use `nn.CrossEntropyLoss` as our loss function that expects raw logits as an output from the neural network.\n",
        "\n",
        "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/multiclass_cls_network.png\">\n",
        "\n",
        "**Excercise:** Implement `__init__` and `__forward__` for a Multiclass Classification Neural Network. <br>\n",
        "- In `__init__`, define a `nn.ModuleList` with the name `block_list`, which would contain all but the last layer of the neural network.\n",
        "- **Important:** your implementation of `__init__` should be universal for any number of input `hidden_dims`. In other words, don't use direct indexing `hidden_dims[0]`, `hidden_dims[1]`, because parameter `hidden_dims` might have more than two elements!\n",
        "- Also, define the last linear layer `out`\n",
        "    - Do not use any activation function for the last layer\n",
        "    - Do not wrap Linear layer in `nn.Sequential`\n",
        "- In `forward()`, simply pass your input x through all of the layers defined above.\n",
        "    - Don't forget to pass your input through every block in `block_list`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "V-tjuWTK-ojg"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[cls] NNet\n",
        "\n",
        "class NNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim=Config.input_dim,\n",
        "                 num_classes=Config.num_classes,\n",
        "                 hidden_dims=Config.hidden_dims):\n",
        "        super(NNet, self).__init__()\n",
        "        dims = [input_dim] + list(hidden_dims)\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        self.block_list = nn.ModuleList([linear_block(dims[i], dims[i+1]) for i in range(len(dims) - 1)])\n",
        "        self.out = nn.Linear(hidden_dims[-1], num_classes)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### (≈ 3 lines of code)\n",
        "        for block in self.block_list:\n",
        "            x = block(x)\n",
        "        x = self.out(x)\n",
        "        ### END CODE HERE ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2tXTiXU3yfQp"
      },
      "outputs": [],
      "source": [
        "# test for many hidden dims\n",
        "model = NNet(1, 6, [2, 3, 4, 5])\n",
        "input = torch.ones(4, 1)\n",
        "assert (list(model(input).shape) == [4, 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Jk1JPbn3toSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f61d52-3d7a-498d-de6d-3519eeb36c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNet(\n",
            "  (block_list): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=16, out_features=64, bias=True)\n",
            "      (1): Dropout(p=0.2, inplace=False)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): Dropout(p=0.2, inplace=False)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (out): Linear(in_features=32, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# test the architecture design\n",
        "print(NNet())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSGb3TJS2ifj"
      },
      "source": [
        "**Expected output:**\n",
        "```\n",
        "NNet(\n",
        "  (block_list): ModuleList(\n",
        "    (0): Sequential(\n",
        "      (0): Linear(in_features=16, out_features=64, bias=True)\n",
        "      (1): Dropout(p=0.2, inplace=False)\n",
        "      (2): ReLU()\n",
        "    )\n",
        "    (1): Sequential(\n",
        "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
        "      (1): Dropout(p=0.2, inplace=False)\n",
        "      (2): ReLU()\n",
        "    )\n",
        "  )\n",
        "  (out): Linear(in_features=32, out_features=7, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGtUnUP_GRL1"
      },
      "source": [
        "# 4 - Training the model\n",
        "\n",
        "**Excercise:** implement training loop for the model. <br>\n",
        "**Please note** that in the previous lab where we calculated accuracy for each batch and then averaged the values; here, we will collect all of the network predictions first and then calculate the f1 score. This is done because F1-score is largely influenced by a sample of target labels, so it's much more stable to run evaluation once we know all of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je9eDjAeDFch"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] train\n",
        "\n",
        "def train(model, optimizer, criterion, X_train, y_train,\n",
        "          num_epochs=Config.epochs,\n",
        "          batch_size=Config.batch_size,\n",
        "          device=Config.device):\n",
        "    loss_values, f1_values = [], []\n",
        "    for e in range(num_epochs):\n",
        "        epoch_loss =  0\n",
        "        num_steps = 0\n",
        "        network_predictions = []\n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            ### START CODE HERE ###\n",
        "            # data prep (≈ 4 lines of code)\n",
        "            x_batch =         # Load batch_size of samples from X_train\n",
        "            y_batch =         # Load batch_size of corresponding target values from y_train\n",
        "            x_tensor =        # Create torch.FloatTensor from x_batch and pass it to device\n",
        "            y_tensor =        # Create torch.LongTensor from y_batch and pass it to device\n",
        "            # forward (≈ 2 lines of code)\n",
        "            out =             # Pass x_batch forward through the model\n",
        "            loss =            # Calculate the loss using criterion\n",
        "            # optimization (≈ 3 lines of code)\n",
        "\n",
        "            # calculate network predictions (≈ 1 line of code)\n",
        "            labels =\n",
        "            ### END CODE HERE ###\n",
        "            network_predictions.extend(labels)\n",
        "            epoch_loss += loss.item()\n",
        "            num_steps += 1\n",
        "        targets = torch.tensor(y_train)\n",
        "        preds = torch.tensor(network_predictions)\n",
        "        f1 = f1_score(targets, preds)\n",
        "        epoch_loss /= num_steps\n",
        "        loss_values.append(epoch_loss)\n",
        "        f1_values.append(f1)\n",
        "        print(f'EPOCH: {e+1} \\tF-1: {f1:.3f}\\tLOSS: {epoch_loss:.3f}')\n",
        "    return loss_values, f1_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSou4GN_DOf3"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "net = NNet().to(Config.device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=Config.learning_rate)\n",
        "weighted_criterion = nn.CrossEntropyLoss(weight=get_loss_scalars(y_train)).to(Config.device)\n",
        "loss, f1 = train(net, optimizer, weighted_criterion, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKyXsfra7f6"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "EPOCH: 1 \tF-1: 0.353\tLOSS: 1.729\n",
        "EPOCH: 2 \tF-1: 0.559\tLOSS: 0.920\n",
        "EPOCH: 3 \tF-1: 0.756\tLOSS: 0.549\n",
        "EPOCH: 4 \tF-1: 0.811\tLOSS: 0.419\n",
        "EPOCH: 5 \tF-1: 0.871\tLOSS: 0.258\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmavqG3Oa45y"
      },
      "outputs": [],
      "source": [
        "fig, ax =  plt.subplots(2, 1, figsize=(10, 6))\n",
        "ax[0].plot(loss)\n",
        "ax[0].set_title('Training Loss')\n",
        "ax[1].plot(f1)\n",
        "ax[1].set_title('Training F1-score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xaQn2sZbTpA"
      },
      "source": [
        "As we can see training loss is still decreasing, meaning that we can achieve even better results on the train set if we would continue to train our model. However, there is always a danger of overfitting, so we should check our performance on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfAr86hRMKSX"
      },
      "source": [
        "# 5 - Evaluating the model\n",
        "**Excercise:** Implement evaluation loop for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6f21DVwJKq5"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] evaluate\n",
        "\n",
        "def evaluate(model, criterion, X_test, y_test,\n",
        "             batch_size=Config.batch_size, class_names=Config.classes, device=Config.device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        network_predictions = []\n",
        "        overall_loss = 0\n",
        "        num_steps = 0\n",
        "        for i in range(0, X_test.shape[0], batch_size):\n",
        "            ### START CODE HERE ### (≈7 lines of code)\n",
        "            x_batch =\n",
        "            y_batch =\n",
        "            x_tensor =\n",
        "            y_tensor =\n",
        "            out =\n",
        "            loss =\n",
        "            labels =\n",
        "            ### END CODE HERE ###\n",
        "            num_steps += 1\n",
        "            overall_loss += loss.item()\n",
        "            network_predictions.extend(labels)\n",
        "\n",
        "    # calculate metrics\n",
        "    overall_loss /= num_steps\n",
        "    targets = torch.tensor(y_test)\n",
        "    preds = torch.tensor(network_predictions)\n",
        "    f1 = f1_score(targets, preds)\n",
        "    return {\n",
        "        'targets':targets,\n",
        "        'predictions': preds,\n",
        "        'loss': overall_loss,\n",
        "        'f1_macro': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oPD6fe_LgCI"
      },
      "outputs": [],
      "source": [
        "results = evaluate(net, weighted_criterion, X_test, y_test)\n",
        "print('Test Loss:', results['loss'])\n",
        "print('Test F1 Macro:', results['f1_macro'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RQuGUwgb-pW"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><b>Test Loss</b></td>\n",
        "        <td>0.1952717396327191</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Test F1 Macro</b></td>\n",
        "        <td>0.921401489070662</td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7wqcgnWyrMe"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(results['targets'], results['predictions'])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, cbar=False,\n",
        "            xticklabels=Config.classes,\n",
        "            yticklabels=Config.classes)\n",
        "plt.ylabel('Predicted class')\n",
        "plt.xlabel('Actual class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUn0jH2tPrD"
      },
      "source": [
        "**Interpretation:** As we can see from the confusion matrix, our model performs decently well on most examples. It confuses only some amphibians with reptiles and some bugs with invertebrates, which is justifiable given that those animal classes have similar characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50MwJ542uMlh"
      },
      "source": [
        "# 6 - What to do next\n",
        "\n",
        "After you submit your lab, try playing with model architecture, and hyperparameters. We highly recommend that you try fitting a model without loss scaling and compare the results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py397",
      "language": "python",
      "name": "py397"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}